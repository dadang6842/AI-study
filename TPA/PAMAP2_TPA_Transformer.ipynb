{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyIOgpo8QbM3"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/TPA2\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    d_model: int = 128\n",
        "\n",
        "    # Transformer hyperparameters\n",
        "    num_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    ff_dim: int = 256\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ========================\n",
        "# Dataset Class\n",
        "# ========================\n",
        "class PreloadedDataset(Dataset):\n",
        "    \"\"\"Dataset for pre-loaded numpy arrays\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        super().__init__()\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "\n",
        "        # Label 범위 확인 및 조정 (1-6 -> 0-5)\n",
        "        if y.min() >= 1:\n",
        "            y = y - 1\n",
        "\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ========================\n",
        "# Data Loading Functions\n",
        "# ========================\n",
        "def load_dataset(base_dir: str, dataset_name: str):\n",
        "    \"\"\"\n",
        "    Load pre-augmented dataset\n",
        "    Args:\n",
        "        base_dir: base directory containing all datasets\n",
        "        dataset_name: e.g., \"ORIGINAL\", \"STANDING_TO_SITTING_10pct\", etc.\n",
        "    Returns:\n",
        "        train_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
        "\n",
        "    print(f\"\\nLoading {dataset_name}...\")\n",
        "    print(f\"  Path: {dataset_dir}\")\n",
        "\n",
        "    # Load data\n",
        "    X_train = np.load(os.path.join(dataset_dir, \"X_train.npy\"))\n",
        "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
        "    X_test = np.load(os.path.join(dataset_dir, \"X_test.npy\"))\n",
        "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
        "\n",
        "    print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    train_dataset = PreloadedDataset(X_train, y_train)\n",
        "    test_dataset = PreloadedDataset(X_test, y_test)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# ========================\n",
        "# Transformer Backbone Components\n",
        "# ========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, D]\n",
        "        Returns:\n",
        "            [B, T, D]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight Transformer Encoder Backbone\n",
        "    - 2 layers\n",
        "    - d_model=128\n",
        "    - n_heads=4\n",
        "    - ff_dim=256\n",
        "    - Dropout=0.1\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection: [B, C, T] -> [B, T, D]\n",
        "        self.input_projection = nn.Linear(in_channels, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-LN for better stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Output normalization\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, C, T] - input sensor data\n",
        "        Returns:\n",
        "            [B, T, D] - transformed sequence\n",
        "        \"\"\"\n",
        "        # [B, C, T] -> [B, T, C]\n",
        "        # x = x.transpose(1, 2)\n",
        "\n",
        "        # Project to d_model: [B, T, C] -> [B, T, D]\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding: [B, T, D]\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Transformer encoding: [B, T, D]\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Final normalization: [B, T, D]\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ========================\n",
        "# GAP Model with Transformer\n",
        "# ========================\n",
        "class GAPModel(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling with Transformer Backbone\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        pooled = features.mean(dim=1)  # [B, D]\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Pure-TPA with Transformer\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Pure TPA\"\"\"\n",
        "    def __init__(self, dim, num_prototypes=16, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_norm = self.pre_norm(x)\n",
        "\n",
        "        K = self.k_proj(x_norm)\n",
        "        V = self.v_proj(x_norm)\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        z_tpa = proto_tokens.mean(dim=1)\n",
        "\n",
        "        z = self.fuse(z_tpa)\n",
        "\n",
        "        return z\n",
        "\n",
        "class TPAModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12,\n",
        "                 tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        z = self.tpa(features)  # [B, D]\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Gated-TPA with Transformer\n",
        "# ========================\n",
        "class GatedTPAModel(nn.Module):\n",
        "    \"\"\"Gated-TPA: Hybrid of TPA and GAP\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12,\n",
        "                 tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "\n",
        "        # TPA path\n",
        "        z_tpa = self.tpa(features)  # [B, D]\n",
        "\n",
        "        # GAP path\n",
        "        z_gap = features.mean(dim=1)  # [B, D]\n",
        "\n",
        "        # Gating\n",
        "        gate_input = torch.cat([z_tpa, z_gap], dim=-1)  # [B, 2D]\n",
        "        alpha = self.gate(gate_input)  # [B, D]\n",
        "\n",
        "        # Combine\n",
        "        z = alpha * z_tpa + (1 - alpha) * z_gap  # [B, D]\n",
        "\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Training & Evaluation\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"[Warning] NaN/Inf loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "def train_model(model, train_loader, val_loader, cfg: Config, model_name: str):\n",
        "    \"\"\"Train a single model\"\"\"\n",
        "    print(f\"\\n[Training {model_name}]\")\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    best_acc, best_wts = 0.0, None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "        val_acc, val_f1 = evaluate(model, val_loader, cfg)\n",
        "\n",
        "        if val_acc > best_acc + cfg.min_delta:\n",
        "            best_acc = val_acc\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"  Epoch {epoch:3d}: Train Acc={stats['acc']:.4f}, Val Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
        "\n",
        "        if patience_counter >= cfg.patience:\n",
        "            print(f\"  Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_wts:\n",
        "        model.load_state_dict(best_wts)\n",
        "\n",
        "    print(f\"  Best Val Acc: {best_acc:.4f}\")\n",
        "    return best_acc\n",
        "\n",
        "def create_model(model_name: str, cfg: Config):\n",
        "    \"\"\"Create model by name\"\"\"\n",
        "    tpa_config = {\n",
        "        'num_prototypes': cfg.tpa_num_prototypes,\n",
        "        'heads': cfg.tpa_heads,\n",
        "        'dropout': cfg.tpa_dropout,\n",
        "        'temperature': cfg.tpa_temperature,\n",
        "        'topk_ratio': cfg.tpa_topk_ratio\n",
        "    }\n",
        "\n",
        "    if model_name == \"GAP\":\n",
        "        return GAPModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout\n",
        "        ).to(cfg.device).float()\n",
        "    elif model_name == \"TPA\":\n",
        "        return TPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "    elif model_name == \"Gated-TPA\":\n",
        "        return GatedTPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "# ========================\n",
        "# Main Experiment\n",
        "# ========================\n",
        "def run_experiment(dataset_name: str, cfg: Config):\n",
        "    \"\"\"Run complete experiment for one dataset\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXPERIMENT: {dataset_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load data\n",
        "    train_dataset, test_dataset = load_dataset(cfg.data_dir, dataset_name)\n",
        "\n",
        "    # Split train into train/val using indices\n",
        "    n_total = len(train_dataset)\n",
        "    indices = np.arange(n_total)\n",
        "\n",
        "    # Get labels for stratification\n",
        "    y_labels = train_dataset.y.numpy()\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "\n",
        "    # Create subsets using Subset\n",
        "    from torch.utils.data import Subset\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_subset, cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, generator=g)\n",
        "    val_loader = DataLoader(val_subset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    print(f\"\\nDataset splits:\")\n",
        "    print(f\"  Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Train and evaluate all models\n",
        "    results = []\n",
        "    model_names = [\"GAP\", \"TPA\", \"Gated-TPA\"]\n",
        "\n",
        "    for model_name in model_names:\n",
        "        # Reset seed for each model\n",
        "        random.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "\n",
        "        # Create and train model\n",
        "        model = create_model(model_name, cfg)\n",
        "        best_val_acc = train_model(model, train_loader, val_loader, cfg, model_name)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_acc, test_f1 = evaluate(model, test_loader, cfg)\n",
        "\n",
        "        print(f\"\\n[{model_name} Results]\")\n",
        "        print(f\"  Val Acc: {best_val_acc:.4f}\")\n",
        "        print(f\"  Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Dataset': dataset_name,\n",
        "            'Val_Accuracy': float(best_val_acc),\n",
        "            'Test_Accuracy': float(test_acc),\n",
        "            'Test_F1_Score': float(test_f1)\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# ========================\n",
        "# Run All Experiments\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"UNIFIED MODEL COMPARISON: GAP vs TPA vs Gated-TPA\")\n",
        "    print(\"WITH TRANSFORMER ENCODER BACKBONE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    datasets = []\n",
        "\n",
        "    transitions = [\n",
        "        'Standing_TO_Lying',\n",
        "        'Lying_TO_Standing',\n",
        "        'Standing_TO_Walking',\n",
        "        'Walking_TO_Standing',\n",
        "        'Walking_TO_Running',\n",
        "        'Running_TO_Walking',\n",
        "        'Walking_TO_Ascending_stairs',\n",
        "        'Walking_TO_Descending_stairs',\n",
        "        'Ascending_stairs_TO_Walking',\n",
        "        'Descending_stairs_TO_Walking'\n",
        "    ]\n",
        "\n",
        "    # 모든 전이에 대해 10%, 20%, 30%, 40% 추가\n",
        "    mix_pcts = [10, 20, 30, 40]\n",
        "\n",
        "    for transition in transitions:\n",
        "        for pct in mix_pcts:\n",
        "            datasets.append(f\"{transition}_{pct}PCT\")\n",
        "\n",
        "    print(f\"\\nTotal datasets to test: {len(datasets)}\")\n",
        "    print(f\"  - transitions: {len(transitions) * len(mix_pcts)}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Run experiments\n",
        "    for i, dataset_name in enumerate(datasets, 1):\n",
        "        print(f\"\\n[Progress: {i}/{len(datasets)}]\")\n",
        "        results = run_experiment(dataset_name, cfg)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Save all results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SAVING RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    results_dict = {\n",
        "        'experiment_info': {\n",
        "            'date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'models': ['GAP', 'TPA', 'Gated-TPA'],\n",
        "            'backbone': 'Transformer Encoder',\n",
        "            'total_datasets': len(datasets),\n",
        "            'datasets': datasets,\n",
        "            'config': {\n",
        "                'epochs': cfg.epochs,\n",
        "                'batch_size': cfg.batch_size,\n",
        "                'lr': cfg.lr,\n",
        "                'd_model': cfg.d_model,\n",
        "                'num_layers': cfg.num_layers,\n",
        "                'n_heads': cfg.n_heads,\n",
        "                'ff_dim': cfg.ff_dim,\n",
        "                'dropout': cfg.dropout,\n",
        "                'tpa_num_prototypes': cfg.tpa_num_prototypes,\n",
        "                'tpa_heads': cfg.tpa_heads\n",
        "            }\n",
        "        },\n",
        "        'results': all_results\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    json_path = os.path.join(cfg.save_dir, \"pamap2_tpa_transformer.json\")\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_dict, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved to: {json_path}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total experiments: {len(all_results)}\")\n",
        "    print(f\"Total datasets tested: {len(datasets)}\")\n",
        "    print(f\"Models compared: 3 (GAP, TPA, Gated-TPA)\")\n",
        "\n",
        "    # Calculate average performance per model\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"AVERAGE PERFORMANCE (All Datasets)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for model_name in ['GAP', 'TPA', 'Gated-TPA']:\n",
        "        model_results = [r for r in all_results if r['Model'] == model_name]\n",
        "        avg_acc = np.mean([r['Test_Accuracy'] for r in model_results])\n",
        "        avg_f1 = np.mean([r['Test_F1_Score'] for r in model_results])\n",
        "        print(f\"{model_name:12s}: Acc={avg_acc:.4f}, F1={avg_f1:.4f}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(f\"{'='*80}\")"
      ]
    }
  ]
}