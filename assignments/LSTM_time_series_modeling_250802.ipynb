{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOsXHt/lWemIIjVpXODpfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadang6842/AI-study/blob/main/assignments/LSTM_time_series_modeling_250802.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://velog.io/@lazy_learner/LSTM-%EC%8B%9C%EA%B3%84%EC%97%B4-%EC%98%88%EC%B8%A1-%EB%AA%A8%EB%93%88-%EB%A7%8C%EB%93%A4%EA%B8%B0-1\n",
        "- 위 블로그 참고\n",
        "- 모델만 빌드\n",
        "\n",
        "### 전체 과정 요약\n",
        "1. reshape_dataset\n",
        "- shape 변경(차원의 변화가 없긴 함), numpy 배열로 변경\n",
        "2. split_sequence\n",
        "- seq_x, seq_y(레이블) 생성\n",
        "- single_output=True면 레이블이 하나, False면 여러 개\n",
        "3. split_train_valid_dataset\n",
        "- reshape_dataset, split_sequence 메서드 실행 수 validation data를 나눔\n",
        "4. build_and_compile_lstm_model\n",
        "- 다층 LSTM, 단층 LSTM일 때 / single_output=True, False일 때 모델 구조를 다르게 빌드\n",
        "- return_sequence를 다르게 설정\n",
        "- ModelCheckpoint, ReduceLROnPlateau, EarlyStopping 등의 콜백 사용하여 성능 개선\n",
        "5. forecast_validation_dataset\n",
        "- 검증 데이터셋 예측\n",
        "6. calculate_metrics\n",
        "- 성능 평가"
      ],
      "metadata": {
        "id": "yTXAvGTViwCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CFLHCkHUqvbV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ForecastLSTM:\n",
        "    def __init__(self, random_seed: int = 1234):\n",
        "        self.random_seed = random_seed"
      ],
      "metadata": {
        "id": "SCPGQMkKsa-1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape input dataset\n",
        "def reshape_dataset(self, df: pd.DataFrame) -> np.array:\n",
        "    # y 컬럼을 데이터프레임의 맨 마지막 위치로 이동\n",
        "    if \"y\" in df.columns:\n",
        "        df = df.drop(columns=[\"y\"]).assign(y=df[\"y\"]) # assign(): Dataframe에 새 열을 할당\n",
        "    else:\n",
        "        raise KeyError(\"Not found target column 'y' in dataset.\")\n",
        "\n",
        "    # shape 변경 & numpy array로 변경(sequential dataset 생성을 위해)\n",
        "    dataset = df.values.reshape(df.shape)\n",
        "    return dataset\n",
        "\n",
        "ForecastLSTM.reshape_dataset = reshape_dataset # ForecastLSTM 클래스에 reshape_dataset를 동적으로 붙임(monkey-patching)"
      ],
      "metadata": {
        "id": "D6rKc96Gsisb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequential dataset\n",
        "- single_output=True: seq_y가 한 개인 경우 -> 입력 시퀀스의 끝 인덱스부터 steps만큼 떨어진 값 한 개를 가져옴\n",
        "- single_output=False: seq_y가 여러 개인 경우 -> 입력 시퀀스의 끝 인덱스부터 steps만큼의 값을 가져옴\n",
        "- seq_x = dataset[i:idx_in, :-1] -> 행, 열 선택, 열을 마지막에서 한 칸 전까지 선택(마지막 컬럼은 타깃 y)\n",
        "- seq_y = dataset[idx_out - 1 : idx_out, -1] -> 마지막 열 선택(y)\n"
      ],
      "metadata": {
        "id": "l-YeCZNoqpUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequences(\n",
        "    self, dataset: np.array, seq_len: int, steps: int, single_output: bool\n",
        ") -> tuple:\n",
        "\n",
        "    # feature와 y 각각 sequential dataset을 반환할 리스트 생성\n",
        "    X, y = list(), list()\n",
        "    # sequence length와 step에 따라 sequential dataset 생성\n",
        "    for i, _ in enumerate(dataset):\n",
        "        # 입력 시퀀스의 끝 인덱스\n",
        "        idx_in = i + seq_len\n",
        "        # 출력 시퀀스의 끝 인덱스\n",
        "        idx_out = idx_in + steps\n",
        "\n",
        "        # 남은 데이터가 부족하면 반복 종료\n",
        "        if idx_out > len(dataset):\n",
        "            break\n",
        "\n",
        "        seq_x = dataset[i:idx_in, :-1]\n",
        "        if single_output:\n",
        "            seq_y = dataset[idx_out - 1 : idx_out, -1]\n",
        "        else:\n",
        "            seq_y = dataset[idx_in:idx_out, -1]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "ForecastLSTM.split_sequences = split_sequences"
      ],
      "metadata": {
        "id": "7r0la-7Ji8Lw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "def split_train_valid_dataset(\n",
        "    self,\n",
        "    df: pd.DataFrame,\n",
        "    seq_len: int,\n",
        "    steps: int,\n",
        "    single_output: bool,\n",
        "    validation_split: float = 0.3,\n",
        "    verbose: bool = True, # 데이터 분할 결과를 확인하고 싶을 때 True\n",
        ") -> tuple:\n",
        "    # dataframe을 numpy array로 reshape\n",
        "    dataset = self.reshape_dataset(df=df)\n",
        "\n",
        "    # feature와 y를 sequential dataset으로 분리\n",
        "    X, y = self.split_sequences(\n",
        "        dataset=dataset,\n",
        "        seq_len=seq_len,\n",
        "        steps=steps,\n",
        "        single_output=single_output,\n",
        "    )\n",
        "\n",
        "    # X, y에서 validation dataset 분리\n",
        "    dataset_size = len(X)\n",
        "    train_size = int(dataset_size * (1 - validation_split))\n",
        "    X_train, y_train = X[:train_size, :], y[:train_size, :]\n",
        "    X_val, y_val = X[train_size:, :], y[train_size:, :]\n",
        "    if verbose:\n",
        "        print(f\" >>> X_train: {X_train.shape}\")\n",
        "        print(f\" >>> y_train: {y_train.shape}\")\n",
        "        print(f\" >>> X_val: {X_val.shape}\")\n",
        "        print(f\" >>> y_val: {y_val.shape}\")\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "\n",
        "ForecastLSTM.split_train_valid_dataset = split_train_valid_dataset"
      ],
      "metadata": {
        "id": "uvk7oT4orW21"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Step Forecast(single_output=True)\n",
        "- 한 시점만 예측\n",
        "- 최종 출력이 스칼라여야 하므로 Dense(1)\n",
        "- LSTM도 마지막 시점만 내보내도록 return_sequences=False\n",
        "\n",
        "Multi-Step Forecast\n",
        "- 다중 시점 예측\n",
        "- 모델 최종 출력이 길이 N 벡터여야 하므로 Dense(steps)\n",
        "- Flatten()을 추가하는 이유:\n",
        "- return_sequences=True면 (batch_size, time_step, unit(은닉 상태의 차원))를 출력\n",
        "- Dense layer는 2D 입력 (batch_size, features)를 기대, 따라서 (batch_size, time_step x unit)으로 2차원으로 펴 줌\n"
      ],
      "metadata": {
        "id": "wpALuJv82T4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build LSTM\n",
        "def build_and_compile_lstm_model(\n",
        "    self,\n",
        "    seq_len: int,\n",
        "    n_features: int,\n",
        "    lstm_units: list,\n",
        "    learning_rate: float,\n",
        "    dropout: float,\n",
        "    steps: int,\n",
        "    metrics: str,\n",
        "    single_output: bool,\n",
        "    last_lstm_return_sequences: bool = False,\n",
        "    dense_units: list = None,\n",
        "    activation: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    LSTM 네트워크를 생성한 결과를 반환한다.\n",
        "\n",
        "    :param seq_len: Length of sequences. (Look back window size)\n",
        "    :param n_features: Number of features. It requires for model input shape.\n",
        "    :param lstm_units: Number of cells each LSTM layers.\n",
        "    :param learning_rate: Learning rate.\n",
        "    :param dropout: Dropout rate.\n",
        "    :param steps: Length to predict.\n",
        "    :param metrics: Model loss function metric.\n",
        "    :param single_output: Whether 'yhat' is a multiple value or a single value.\n",
        "    :param last_lstm_return_sequences: Last LSTM's `return_sequences`. Allow when `single_output=False` only.\n",
        "    :param dense_units: Number of cells each Dense layers. It adds after LSTM layers.\n",
        "    :param activation: Activation function of Layers.\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(self.random_seed) # 시드를 고정함으로써 실험을 재현할 수 있음\n",
        "    model = Sequential()\n",
        "\n",
        "    if len(lstm_units) > 1:\n",
        "        # 다층 LSTM\n",
        "        # 첫 번째 LSTM layer\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                units=lstm_units[0],\n",
        "                activation=activation,\n",
        "                return_sequences=True, # 이후 레이어에 타임 스텝 별 은닉 상태 전체를 넘김\n",
        "                input_shape=(seq_len, n_features),\n",
        "            )\n",
        "        )\n",
        "        # 나머지 LSTM layer\n",
        "        lstm_layers = lstm_units[1:]\n",
        "        for i, n_units in enumerate(lstm_layers, start=1):\n",
        "            # 마지막 LSTM layer\n",
        "            if i == len(lstm_layers):\n",
        "                if single_output:\n",
        "                    return_sequences = False # 마지막 시점 은닉 상태 하나만\n",
        "                else:\n",
        "                    return_sequences = last_lstm_return_sequences # 파라미터에 따라\n",
        "                model.add(\n",
        "                    LSTM(\n",
        "                        units=n_units,\n",
        "                        activation=activation,\n",
        "                        return_sequences=return_sequences,\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                model.add(\n",
        "                    LSTM(\n",
        "                        units=n_units,\n",
        "                        activation=activation,\n",
        "                        return_sequences=True,\n",
        "                    )\n",
        "                )\n",
        "    else:\n",
        "        # 단일 LSTM\n",
        "        if single_output:\n",
        "            return_sequences = False\n",
        "        else:\n",
        "            return_sequences = last_lstm_return_sequences\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                units=lstm_units[0],\n",
        "                activation=activation,\n",
        "                return_sequences=return_sequences,\n",
        "                input_shape=(seq_len, n_features),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if single_output: # 한 시점만 예측\n",
        "        if dense_units:\n",
        "            for n_units in dense_units:\n",
        "                model.add(Dense(units=n_units, activation=activation))\n",
        "        if dropout > 0:\n",
        "            model.add(Dropout(rate=dropout))\n",
        "        model.add(Dense(1)) # 출력: 스칼라 값 하나\n",
        "    else:  # Multiple Output Step\n",
        "        if last_lstm_return_sequences:\n",
        "            model.add(Flatten())\n",
        "        if dense_units:\n",
        "            for n_units in dense_units:\n",
        "                model.add(Dense(units=n_units, activation=activation))\n",
        "        if dropout > 0:\n",
        "            model.add(Dropout(rate=dropout))\n",
        "        model.add(Dense(units=steps)) # steps개 값을 반환\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=MSE, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "\n",
        "ForecastLSTM.build_and_compile_lstm_model = build_and_compile_lstm_model"
      ],
      "metadata": {
        "id": "n4pyGOMdwI_p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModelCheckpoint\n",
        "-  Keras 모델을 학습하는 동안 일정한 간격으로 모델의 가중치를 저장하고, 최상의 성능을 보인 모델을 선택하는 기능을 제공\n",
        "- 모델을 재학습할 때는 이전에 저장된 가중치를 불러와서 학습을 시작하면 됨 (load_weights)\n",
        "\n",
        "ReduceLROnPlateau\n",
        "- 검증 손실(validation loss)이 더 이상 개선되지 않을 때 학습률을 동적으로 감소시켜 모델의 학습을 돕는 기법\n",
        "- 학습률을 낮춤으로써 파라미터 업데이트 크기를 줄여 최솟값 근처를 정교하게 탐색\n",
        "- factor: learning rate를 감소시킬 비율\n",
        "- patience: 손실값이 개선되지 않은 상태를 얼마나 허용할 것인지를 설정하는 정수값(얼마의 epoch 동안 기다릴 건지)\n",
        "\n",
        "EarlyStopping(patience=patience)\n",
        "- patience 에포크 동안 모니터링 지표가 개선되지 않으면 훈련을 조기 종료\n",
        "- 디폴트: monitor=\"val_loss\", mode=\"auto\"\n"
      ],
      "metadata": {
        "id": "DGHPZWz0ElaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "def fit_lstm(\n",
        "    self,\n",
        "    df: pd.DataFrame,\n",
        "    steps: int,\n",
        "    lstm_units: list,\n",
        "    activation: str,\n",
        "    dropout: float = 0,\n",
        "    seq_len: int = 16,\n",
        "    single_output: bool = False,\n",
        "    epochs: int = 200,\n",
        "    batch_size: int = None,\n",
        "    steps_per_epoch: int = None,\n",
        "    learning_rate: float = 0.001,\n",
        "    patience: int = 10,\n",
        "    validation_split: float = 0.3,\n",
        "    last_lstm_return_sequences: bool = False,\n",
        "    dense_units: list = None,\n",
        "    metrics: str = \"mse\",\n",
        "    check_point_path: str = None,\n",
        "    verbose: bool = False,\n",
        "    plot: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    LSTM 기반 모델 훈련을 진행한다.\n",
        "\n",
        "    :param df: DataFrame for model train.\n",
        "    :param steps: Length to predict.\n",
        "    :param lstm_units: LSTM, Dense Layers\n",
        "    :param activation: Activation function for LSTM, Dense Layers.\n",
        "    :param dropout: Dropout ratio between Layers.\n",
        "    :param seq_len: Length of sequences. (Look back window size)\n",
        "    :param single_output: Select whether 'y' is a continuous value or a single value.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(self.random_seed)\n",
        "    tf.random.set_seed(self.random_seed)\n",
        "\n",
        "    # 훈련, 검증 데이터셋 생성\n",
        "    (\n",
        "        self.X_train,\n",
        "        self.y_train,\n",
        "        self.X_val,\n",
        "        self.y_val,\n",
        "    ) = self.split_train_valid_dataset(\n",
        "        df=df,\n",
        "        seq_len=seq_len,\n",
        "        steps=steps,\n",
        "        validation_split=validation_split,\n",
        "        single_output=single_output,\n",
        "        verbose=verbose,\n",
        "    )\n",
        "\n",
        "    # LSTM 모델 생성\n",
        "    n_features = df.shape[1] - 1 # 라벨 제외\n",
        "    self.model = self.build_and_compile_lstm_model(\n",
        "        seq_len=seq_len,\n",
        "        n_features=n_features,\n",
        "        lstm_units=lstm_units,\n",
        "        activation=activation,\n",
        "        learning_rate=learning_rate,\n",
        "        dropout=dropout,\n",
        "        steps=steps,\n",
        "        last_lstm_return_sequences=last_lstm_return_sequences,\n",
        "        dense_units=dense_units,\n",
        "        metrics=metrics,\n",
        "        single_output=single_output,\n",
        "    )\n",
        "\n",
        "    # 콜백 함수 정의\n",
        "    if check_point_path is not None:\n",
        "        # create checkpoint\n",
        "        checkpoint_path = f\"checkpoint/lstm_{check_point_path}.h5\"\n",
        "        # 모델 적합 과정에서 best model 저장\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            filepath=checkpoint_path,\n",
        "            save_weights_only=False, # 전체 모델 구조 + 가중치를 함께 저장 (True면 가중치만 저장)\n",
        "            save_best_only=True, # 지정한 모니터링 지표(monitor)가 개선되었을 때만 새 파일로 덮어씀\n",
        "            monitor=\"val_loss\",\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        rlr = ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
        "        )\n",
        "        callbacks = [checkpoint, EarlyStopping(patience=patience), rlr]\n",
        "    else:\n",
        "        rlr = ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
        "        )\n",
        "        callbacks = [EarlyStopping(patience=patience), rlr]\n",
        "\n",
        "    # 모델 훈련\n",
        "    self.history = self.model.fit(\n",
        "        self.X_train,\n",
        "        self.y_train,\n",
        "        batch_size=batch_size,\n",
        "        steps_per_epoch=steps_per_epoch, # 한 에포크(epoch)가 몇 스텝인지 (numpy 배열을 넘길 때는 내부 계산, 제너레이터를 쓸 경우 명시적 지정)\n",
        "        validation_data=(self.X_val, self.y_val),\n",
        "        epochs=epochs,\n",
        "        use_multiprocessing=True, # 데이터 로딩을 다중 프로세스로 병렬 수행\n",
        "        workers=8, # 몇 개의 워커 프로세스를 띄울지\n",
        "        verbose=verbose,\n",
        "        callbacks=callbacks, # 훈련 과정 중 특정 이벤트(에포크 종료, 성능 개선 등)가 발생할 때 추가 작업을 실행하는 함수들의 리스트\n",
        "        shuffle=False, # 에포크 시작 전 데이터를 섞을지 여부, 시계열 데이터는 False로 두어 순서를 유지\n",
        "    )\n",
        "\n",
        "    # 훈련 종료 후 best model 로드\n",
        "    if check_point_path is not None:\n",
        "        self.model.load_weights(f\"checkpoint/lstm_{check_point_path}.h5\")\n",
        "\n",
        "    # 모델링 과정 시각화\n",
        "    if plot:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.history.history[f\"{metrics}\"])\n",
        "        plt.plot(self.history.history[f\"val_{metrics}\"])\n",
        "        plt.title(\"Performance Metric\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(f\"{metrics}\")\n",
        "        if metrics == \"mape\": # MAPE: 평균 절대 백분율 오차\n",
        "            plt.axhline(y=10, xmin=0, xmax=1, color=\"grey\", ls=\"--\", alpha=0.5) # 수평 기준선을 그림 (예측 오차가 10% 이하인지)\n",
        "        plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "ForecastLSTM.fit_lstm = fit_lstm"
      ],
      "metadata": {
        "id": "OgwZY0k15BEx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "np.expand_dims(x_val, axis=0)\n",
        "- X_val -> (seq_len, n_features)\n",
        "- predict 메서드는 (batch_size, seq_len, n_features) 형태를 기대\n",
        "- expand_dims로 맨 앞에(axis=0) 차원을 하나 추가\n",
        "\n",
        "self.model.predict(x_val)[0]\n",
        "- predict(x_val) 은 (1, steps) 또는 (1, …) 형태의 배열을 반환\n",
        "- [0] 으로 첫(유일한) 배치의 결과만 꺼내어 1차원 배열로 만듦"
      ],
      "metadata": {
        "id": "iG3XSwMpff2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_validation_dataset(self) -> pd.DataFrame:\n",
        "    # 검증 데이터셋의 실제 값(y)과, 예측 값(yhat)을 저장할 리스트 생성\n",
        "    y_pred_list, y_val_list = list(), list()\n",
        "\n",
        "    # 훈련된 모델로 validation dataset에 대한 예측값 생성\n",
        "    for x_val, y_val in zip(self.X_val, self.y_val): # zip(): X_val과 y_val을 쌍으로 묶어줌\n",
        "        x_val = np.expand_dims(\n",
        "            x_val, axis=0\n",
        "        )  # (seq_len, n_features) -> (1, seq_len, n_features)\n",
        "        y_pred = self.model.predict(x_val)[0]\n",
        "        y_pred_list.extend(y_pred.tolist()) # append() 쓰면 2차원 배열 구조가 됨 (x)\n",
        "        y_val_list.extend(y_val.tolist())\n",
        "    return pd.DataFrame({\"y\": y_val_list, \"yhat\": y_pred_list}) # 실제값, 예측값 리스트로 dataframe 생\n",
        "\n",
        "\n",
        "ForecastLSTM.forecast_validation_dataset = forecast_validation_dataset"
      ],
      "metadata": {
        "id": "4iRdp2DMKATa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance metric\n",
        "def calculate_metrics(df_fcst: pd.DataFrame) -> dict:\n",
        "    true = df_fcst[\"y\"]\n",
        "    pred = df_fcst[\"yhat\"]\n",
        "\n",
        "    mae = (true - pred).abs().mean()\n",
        "    mape = (true - pred).abs().div(true).mean() * 100\n",
        "    mse = ((true - pred) ** 2).mean()\n",
        "    return {\n",
        "        \"mae\": mae,\n",
        "        \"mape\": mape,\n",
        "        \"mse\": mse,\n",
        "    }"
      ],
      "metadata": {
        "id": "NqncjpCzgt3p"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}