{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Transition Augmentation (행동 전이 증강)\n",
        "- 행동 전이 구간을 인위적으로 만들어서 모델이 구간 경계에 강건하도록 학습 시킴\n",
        "\n",
        "**4개의 모델**\n",
        "1. GAP + NO mask\n",
        "2. GAP + mask\n",
        "3. TPA + NO mask\n",
        "4. TPA + mask\n",
        "\n",
        "- mask를 적용할 경우 전이 구간을 무시하는 법을 학습함 -> 유효한 activity만 사용\n",
        "- mask를 적용하지 않을 경우 모델 자체의 강건성(robustness)를 측정하는 것\n",
        "\n",
        "**모델별 출력값**\n",
        "- original test: 데이터 변형을 하지 않은 테스트셋에 대한 정확도\n",
        "- avg transition: 변형 테스트셋의 정확도에 대한 평균값\n",
        "- avg drop: original test - avg transition\n",
        "- retention: avg drop이 original test 정확도에서 차지하는 비율, 강건성이 얼마나 잘 유지되었는지 백분율로 표시\n",
        "\n",
        "**effective analysis 항목 해석**\n",
        "1. mask effect (in GAP)\n",
        "- 마스크 사용이 GAP 모델에서 단독으로 가져오는 효과를 측정\n",
        "- mask_effect_gap: 마스크를 사용함으로써 감소된 드롭폭의 절대값\n",
        "- mask_imporve_gap: 마스크 사용이 GAP_NoMask의 drop을 몇%나 감소시켰는지\n",
        "\n",
        "2. TPA effect (without mask)\n",
        "- tpa_effect_nomask: tpa를 사용함으로써 감소된 드롭 폭의 절대값, TPA가 GAP보다 전이 노이즈를 더 잘 무시했는지 측정\n",
        "- tpa_improve_nomask: tpa가 GAP_NoMask의 drop을 몇%나 감소시켰는지\n",
        "\n",
        "3. combined effect (tpa + mask)\n",
        "- tpa + mask와 gap_nomask를 비교\n",
        "- combined_effect: tpa_withmask가 gap_nomask보다 drop을 얼마나 감소시켰는지\n",
        "- combined_imporve: tpa_withmask가 gap_nomask의 drop을 몇%나 개선했는지\n",
        "\n",
        "4. synergy analysis\n",
        "- expective_addtive: 예상 avg drop값 (단순 합산)\n",
        "- actual: 실제 avg drop값\n",
        "- synergy: 예상되는 드롭 - 실제 드롭\n",
        "- synergy가 양수면 두 요소가 함께 사용되었을 때 시너지, 음수면 성능 더 하락, 0에 가까우면 두 요소의 효과가 단순히 합산된 것이고 시너지는 없음"
      ],
      "metadata": {
        "id": "rHcb0f8ZB-Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hj9K7uZU7k3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c76a1f-37ab-4d9c-d847-d71040f622f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "UCI-HAR Complete Ablation Study: 4-way comparison\n",
        "GAP/TPA × NoMask/WithMask to isolate the effect of each component\n",
        "\"\"\"\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str = \"ablation\"\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/ablation_4way\"\n",
        "\n",
        "    epochs: int = 25\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2  # 20% of train for validation\n",
        "\n",
        "    train_augment_prob: float = 0.25\n",
        "    train_augment_mix: float = 0.35\n",
        "\n",
        "    d_model: int = 128\n",
        "    use_tpa: bool = False\n",
        "    use_mask: bool = False\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_seg_kernel: int = 9\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"), (\"Inertial Signals/total_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_x_\", \"txt\"), (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"), (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "_LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None] for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None, mask: np.ndarray | None = None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        # Apply indices if provided (for train/val split)\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "            if mask is not None:\n",
        "                mask = mask[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "        T = self.X.shape[2]\n",
        "        if mask is None:\n",
        "            self.mask = np.ones((self.X.shape[0], T), dtype=bool)\n",
        "        else:\n",
        "            assert mask.shape == (self.X.shape[0], T)\n",
        "            self.mask = mask.astype(bool)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            torch.from_numpy(self.mask[idx]).float()\n",
        "        )\n",
        "\n",
        "# ========================\n",
        "# 2) Online Transition Augmentation\n",
        "# ========================\n",
        "def apply_transition_augmentation(x: torch.Tensor, y: torch.Tensor, mix_ratio: float = 0.25) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    B, C, T = x.shape\n",
        "    device = x.device\n",
        "\n",
        "    mask = torch.ones(B, T, dtype=torch.bool, device=device)\n",
        "    mix_pts = int(T * mix_ratio)\n",
        "\n",
        "    for i in range(B):\n",
        "        if random.random() < 0.5:\n",
        "            other_class_indices = (y != y[i]).nonzero(as_tuple=True)[0]\n",
        "            if len(other_class_indices) > 0:\n",
        "                j = other_class_indices[random.randint(0, len(other_class_indices)-1)]\n",
        "                x[i, :, -mix_pts:] = x[j, :, :mix_pts].clone()\n",
        "                mask[i, -mix_pts:] = False\n",
        "\n",
        "    return x, mask\n",
        "\n",
        "# ========================\n",
        "# 3) TPA Module\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Temporal Prototype Attention with optional mask support\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        pad = (seg_kernel - 1) // 2\n",
        "        self.lowpass = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim, bias=False)\n",
        "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
        "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, return_confidence=False, use_mask=True):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        mask: [B, T] - True for valid frames, False for transition frames\n",
        "        use_mask: whether to use mask (for ablation study)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_filtered = self.lowpass(x.transpose(1, 2)).transpose(1, 2)\n",
        "        xloc = self.pw(self.dw(x_filtered.transpose(1, 2))).transpose(1, 2)\n",
        "        xloc = self.pre_norm(xloc) + x\n",
        "\n",
        "        # Apply mask only if use_mask=True\n",
        "        if mask is not None and use_mask:\n",
        "            # [FIXED] 특징 필터링을 위해 Float 타입 마스크를 사용합니다.\n",
        "            float_mask = mask.float()\n",
        "            float_mask_expanded = float_mask.unsqueeze(-1)\n",
        "            xloc = xloc * float_mask_expanded\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        # Apply mask to attention only if use_mask=True\n",
        "        if mask is not None and use_mask:\n",
        "            # [FIXED] ~ 연산자를 사용하기 위해 Boolean 타입 마스크를 생성하여 사용합니다.\n",
        "            boolean_mask = mask.bool()\n",
        "            mask_attn = boolean_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(~mask_attn, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        # Compute GAP for fallback (with or without mask based on use_mask)\n",
        "        if mask is not None and use_mask:\n",
        "            # Float 타입 마스크를 사용하여 정확한 합계/평균을 계산합니다.\n",
        "            mask_expanded = float_mask.unsqueeze(-1).float()\n",
        "            z_gap = (x * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 4) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1)) for k in branches])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling (with optional mask support)\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap, mask: torch.BoolTensor | None = None, use_mask: bool = False):\n",
        "        # [B, D, T] -> [B, T, D]\n",
        "        features = Fmap.transpose(1, 2)\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            # Masked average pooling\n",
        "            mask_expanded = mask.unsqueeze(-1).float()  # [B, T, 1]\n",
        "            pooled = (features * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            # Standard global average pooling\n",
        "            pooled = features.mean(dim=1)\n",
        "\n",
        "        logits = self.fc(pooled)\n",
        "        aux = {\"confidence\": None}\n",
        "        return logits, aux\n",
        "\n",
        "class TPAHead(nn.Module):\n",
        "    \"\"\"TPA: Temporal Prototype Attention (with optional mask support)\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, seg_kernel: int = 9,\n",
        "                 heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=num_prototypes,\n",
        "            seg_kernel=seg_kernel,\n",
        "            heads=heads,\n",
        "            dropout=dropout,\n",
        "            temperature=temperature,\n",
        "            topk_ratio=topk_ratio\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap, mask: torch.BoolTensor | None = None, use_mask: bool = False):\n",
        "        \"\"\"\n",
        "        Fmap: [B, D, T]\n",
        "        mask: [B, T]\n",
        "        use_mask: whether to use mask (for ablation)\n",
        "        \"\"\"\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        z, confidence = self.tpa(features, mask=mask, return_confidence=True, use_mask=use_mask)\n",
        "        logits = self.classifier(z)\n",
        "        aux = {\"confidence\": confidence.mean().item()}\n",
        "        return logits, aux\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, use_tpa=False, use_mask=False, tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.use_tpa = use_tpa\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        if use_tpa:\n",
        "            self.head = TPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                seg_kernel=tpa_config.get('seg_kernel', 9),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25)\n",
        "            )\n",
        "        else:\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, mask: torch.BoolTensor | None = None):\n",
        "        fmap = self.backbone(x)\n",
        "\n",
        "        if mask is not None:\n",
        "            stride = self.backbone.stride\n",
        "            mask_float = mask.float().unsqueeze(1)\n",
        "            mask_down = (F.avg_pool1d(mask_float, kernel_size=stride, stride=stride) == 1.0).squeeze(1)\n",
        "        else:\n",
        "            mask_down = None\n",
        "\n",
        "        return self.head(fmap, mask_down, use_mask=self.use_mask)\n",
        "\n",
        "# ========================\n",
        "# 5) Train / Eval\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    aug_count = 0\n",
        "    confidence_vals = []\n",
        "\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device).float(), y.to(cfg.device), m.to(cfg.device).float()\n",
        "\n",
        "        if random.random() < cfg.train_augment_prob:\n",
        "            x, m = apply_transition_augmentation(x, y, cfg.train_augment_mix)\n",
        "            aug_count += 1\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, aux = model(x, mask=m)\n",
        "\n",
        "        cls_loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "        loss = cls_loss\n",
        "        if torch.isnan(loss):\n",
        "            print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            if aux[\"confidence\"] is not None:\n",
        "                confidence_vals.append(aux[\"confidence\"])\n",
        "\n",
        "    stats = {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "        \"aug_count\": aug_count,\n",
        "        \"avg_confidence\": np.mean(confidence_vals) if confidence_vals else None\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config, classes=6):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device), y.to(cfg.device), m.to(cfg.device)\n",
        "        logits, _ = model(x, mask=m)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(classes)))\n",
        "    report = classification_report(y_true, y_pred, target_names=[_CODE_TO_LABEL_NAME[i] for i in range(classes)], digits=4)\n",
        "    return acc, f1, cm, report\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_mask_analysis(model, loader, cfg: Config):\n",
        "    \"\"\"Evaluate and analyze how mask affects predictions\"\"\"\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device), y.to(cfg.device), m.to(cfg.device)\n",
        "        logits, _ = model(x, mask=m)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "\n",
        "        ys.append(y.cpu().numpy())\n",
        "        ps.append(pred.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ========================\n",
        "# 6) Extreme Transition Test Set\n",
        "# ========================\n",
        "def create_transitional_test_set(orig_dataset: UCIHARInertial, class_A: str, class_B: str,\n",
        "                                 p: float=0.05, mix: float=0.25, profile: str='abrupt',\n",
        "                                 pos: str='tail', segments: int=1) -> Tuple[UCIHARInertial, dict]:\n",
        "    \"\"\"Create transitional test set\"\"\"\n",
        "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
        "    N, C, T = X.shape\n",
        "    mask = np.ones((N, T), dtype=bool)\n",
        "\n",
        "    code_A, code_B = _LABEL_NAME_TO_CODE[class_A], _LABEL_NAME_TO_CODE[class_B]\n",
        "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
        "    mix_pts = int(T * mix)\n",
        "\n",
        "    modified_indices = []\n",
        "\n",
        "    if segments > 1:\n",
        "        seg_length = mix_pts // segments\n",
        "        remaining = mix_pts % segments\n",
        "    else:\n",
        "        seg_length = mix_pts\n",
        "        remaining = 0\n",
        "\n",
        "    def get_transition_positions(T, mix_pts, pos, segments):\n",
        "        positions = []\n",
        "\n",
        "        if segments == 1:\n",
        "            if pos == 'tail':\n",
        "                positions = [T - mix_pts]\n",
        "            elif pos == 'middle':\n",
        "                positions = [(T - mix_pts) // 2]\n",
        "            elif pos == 'random':\n",
        "                positions = [random.randint(0, max(0, T - mix_pts))]\n",
        "        else:\n",
        "            seg_len = mix_pts // segments\n",
        "            if pos == 'tail':\n",
        "                start = T - mix_pts\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * seg_len)\n",
        "            elif pos == 'middle':\n",
        "                center = T // 2\n",
        "                total_span = mix_pts + (segments - 1) * seg_len\n",
        "                start = center - total_span // 2\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * (seg_len * 2))\n",
        "            elif pos == 'random':\n",
        "                available_positions = list(range(0, T - seg_len))\n",
        "                random.shuffle(available_positions)\n",
        "                positions = sorted(available_positions[:segments])\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def apply_transition(target_data, source_data, start_pos, length, profile):\n",
        "        end_pos = start_pos + length\n",
        "\n",
        "        if profile == 'abrupt':\n",
        "            target_data[:, start_pos:end_pos] = source_data[:, start_pos:end_pos].copy()\n",
        "        elif profile == 'fade':\n",
        "            alpha = np.linspace(0, 1, length).reshape(1, -1)\n",
        "            target_segment = target_data[:, start_pos:end_pos]\n",
        "            source_segment = source_data[:, start_pos:end_pos]\n",
        "            target_data[:, start_pos:end_pos] = (\n",
        "                target_segment * (1 - alpha) + source_segment * alpha\n",
        "            )\n",
        "\n",
        "    # Apply transitions for class A\n",
        "    n_targets_A = max(1, int(len(idx_A) * p))\n",
        "    targets_A = np.random.choice(idx_A, n_targets_A, replace=False)\n",
        "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_A, sources_B):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "                mask[t, start:start+curr_len] = False\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    # Apply transitions for class B\n",
        "    n_targets_B = max(1, int(len(idx_B) * p))\n",
        "    targets_B = np.random.choice(idx_B, n_targets_B, replace=False)\n",
        "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_B, sources_A):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "                mask[t, start:start+curr_len] = False\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    if p > 0.5:\n",
        "        mid_start = T // 3\n",
        "        mid_end = 2 * T // 3\n",
        "        mid_length = mid_end - mid_start\n",
        "\n",
        "        extra_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p * 0.3)), replace=False)\n",
        "        extra_B_src = np.random.choice(idx_B, len(extra_A), replace=True)\n",
        "\n",
        "        for t, s in zip(extra_A, extra_B_src):\n",
        "            if t not in modified_indices:\n",
        "                apply_transition(X[t], orig_dataset.X[s], mid_start, mid_length, profile)\n",
        "                mask[t, mid_start:mid_end] = False\n",
        "                modified_indices.append(t)\n",
        "\n",
        "    mod_dataset = UCIHARInertial(\n",
        "        root=\"\", split=\"test\",\n",
        "        mean=orig_dataset.mean, std=orig_dataset.std,\n",
        "        mask=mask, preloaded_data=(X, y)\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'total_samples': N,\n",
        "        'modified_samples': len(modified_indices),\n",
        "        'modified_ratio': len(modified_indices) / N,\n",
        "        'mix_frames': mix_pts,\n",
        "        'primary_class_ratio': 1 - mix,\n",
        "        'class_A_modified': len(targets_A),\n",
        "        'class_B_modified': len(targets_B),\n",
        "        'profile': profile,\n",
        "        'position': pos,\n",
        "        'segments': segments\n",
        "    }\n",
        "\n",
        "    return mod_dataset, info\n",
        "\n",
        "def get_transition_scenarios():\n",
        "    \"\"\"Return all transitional test scenarios\"\"\"\n",
        "    scenarios_core = [\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_stress = [\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_ctrl = [\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"middle\",2),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"middle\",2),\n",
        "    ]\n",
        "\n",
        "    return scenarios_core + scenarios_stress + scenarios_ctrl\n",
        "\n",
        "# ========================\n",
        "# 7) 4-Way Ablation Study\n",
        "# ========================\n",
        "def run_ablation_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # Load full train set\n",
        "    train_set_full = UCIHARInertial(cfg.data_dir, \"train\")\n",
        "\n",
        "    # Split train into train + validation (stratified by class)\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    n_samples = len(train_set_full)\n",
        "    indices = np.arange(n_samples)\n",
        "    labels = train_set_full.y\n",
        "\n",
        "    # Stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Total train samples: {n_samples}\")\n",
        "    print(f\"  → Train split: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val split:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    # Verify class distribution\n",
        "    train_labels = labels[train_indices]\n",
        "    val_labels = labels[val_indices]\n",
        "    print(f\"\\nClass distribution check:\")\n",
        "    for cls in range(6):\n",
        "        train_count = (train_labels == cls).sum()\n",
        "        val_count = (val_labels == cls).sum()\n",
        "        print(f\"  Class {cls} ({_CODE_TO_LABEL_NAME[cls]}): Train={train_count}, Val={val_count}\")\n",
        "\n",
        "    # Create datasets using indices\n",
        "    # We need to load raw data and pass with indices\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)  # Ensure float32\n",
        "\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader_orig = DataLoader(test_set_orig, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    scenarios = get_transition_scenarios()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    4-WAY ABLATION STUDY: GAP/TPA × NoMask/WithMask\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Goal: Isolate the effect of:\")\n",
        "    print(f\"     1) Mask usage (NoMask vs WithMask)\")\n",
        "    print(f\"     2) Pooling method (GAP vs TPA)\")\n",
        "    print(f\"     3) Their interaction\")\n",
        "    print(f\"\\n   Training: All 4 configs use SAME augmentation\")\n",
        "    print(f\"   Testing: Differ in mask usage and pooling method\")\n",
        "    print(f\"   Scenarios: {len(scenarios)} extreme transitions (p=0.70-0.75, mix=0.55-0.58)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    # Create transitional test sets\n",
        "    transition_sets, transition_infos = [], []\n",
        "    for i, scenario in enumerate(scenarios):\n",
        "        clsA, clsB, p, mix, profile, pos, segments = scenario\n",
        "        print(f\"  [{i+1}/{len(scenarios)}] {clsA} ↔ {clsB} (p={p:.2f}, mix={mix:.2f}, {profile}, {pos}, seg={segments})\")\n",
        "        print(f\"      Primary class: {(1-mix)*100:.0f}% | Transition: {mix*100:.0f}%\")\n",
        "        test_set_mod, info = create_transitional_test_set(\n",
        "            test_set_orig, clsA, clsB, p=p, mix=mix, profile=profile, pos=pos, segments=segments\n",
        "        )\n",
        "        transition_sets.append(test_set_mod)\n",
        "        transition_infos.append(info)\n",
        "        print(f\"      Modified: {info['modified_samples']}/{info['total_samples']} ({info['modified_ratio']*100:.1f}%)\")\n",
        "\n",
        "    transition_loaders = [DataLoader(ts, cfg.batch_size, num_workers=cfg.num_workers) for ts in transition_sets]\n",
        "    print(f\"\\n✓ {len(transition_loaders)} transitional test sets created.\\n\")\n",
        "\n",
        "    # 4-way ablation configurations\n",
        "    ablation_configs = [\n",
        "        {\"name\": \"GAP_NoMask\", \"use_tpa\": False, \"use_mask\": False},\n",
        "        {\"name\": \"GAP_WithMask\", \"use_tpa\": False, \"use_mask\": True},\n",
        "        {\"name\": \"TPA_NoMask\", \"use_tpa\": True, \"use_mask\": False},\n",
        "        {\"name\": \"TPA_WithMask\", \"use_tpa\": True, \"use_mask\": True},\n",
        "    ]\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    # Train and evaluate each configuration\n",
        "    for ab_cfg in ablation_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   CONFIG: {ab_cfg['name']}\")\n",
        "        print(f\"   Pooling: {'TPA' if ab_cfg['use_tpa'] else 'GAP'} | Mask: {'Yes' if ab_cfg['use_mask'] else 'No'}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        cfg.use_tpa = ab_cfg[\"use_tpa\"]\n",
        "        cfg.use_mask = ab_cfg[\"use_mask\"]\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        model_path = os.path.join(cfg.save_dir, f\"model_{ab_cfg['name']}.pth\")\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'seg_kernel': cfg.tpa_seg_kernel,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            use_tpa=cfg.use_tpa,\n",
        "            use_mask=cfg.use_mask,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {ab_cfg['name']} for up to {cfg.epochs} epochs (patience={cfg.patience})...\")\n",
        "        if cfg.use_tpa:\n",
        "            print(f\"TPA: prototypes={cfg.tpa_num_prototypes}, heads={cfg.tpa_heads}, temp={cfg.tpa_temperature}\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "\n",
        "            # Evaluate on VALIDATION set for early stopping\n",
        "            val_acc, val_f1, _, _ = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            # Early stopping logic based on VALIDATION accuracy\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "\n",
        "            log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "            log_str += f\" Aug:{stats['aug_count']}\"\n",
        "            log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "            if stats['avg_confidence'] is not None:\n",
        "                log_str += f\" | Conf:{stats['avg_confidence']:.3f}\"\n",
        "            if improved:\n",
        "                log_str += \" ✓\"\n",
        "            print(log_str)\n",
        "\n",
        "            # Early stopping check\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  No improvement for {cfg.patience} epochs on validation set\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Final evaluation on TEST set (only once!)\n",
        "        acc_orig, f1_orig, _, _ = evaluate(model, test_loader_orig, cfg)\n",
        "        print(f\"  Final Test Acc: {acc_orig:.4f}, F1: {f1_orig:.4f}\")\n",
        "\n",
        "        print(f\"\\n   Evaluating on {len(transition_loaders)} transitional test sets...\")\n",
        "        transition_accs = []\n",
        "        scenario_details = []\n",
        "\n",
        "        for i, loader in enumerate(transition_loaders):\n",
        "            result = evaluate_with_mask_analysis(model, loader, cfg)\n",
        "            acc_mod = result['accuracy']\n",
        "            transition_accs.append(acc_mod)\n",
        "\n",
        "            scenario = scenarios[i]\n",
        "            clsA, clsB, p, mix = scenario[0], scenario[1], scenario[2], scenario[3]\n",
        "            primary_ratio = (1 - mix) * 100\n",
        "            drop_from_orig = acc_orig - acc_mod\n",
        "\n",
        "            if drop_from_orig < 0.02:\n",
        "                grade = \"Very Robust\"\n",
        "            elif drop_from_orig < 0.05:\n",
        "                grade = \"Slightly Vulnerable\"\n",
        "            else:\n",
        "                grade = \"Vulnerable\"\n",
        "\n",
        "            scenario_details.append({\n",
        "                'scenario': f\"{clsA}↔{clsB}\",\n",
        "                'primary_ratio': primary_ratio,\n",
        "                'acc': acc_mod,\n",
        "                'drop': drop_from_orig,\n",
        "                'grade': grade\n",
        "            })\n",
        "\n",
        "            print(f\"    Scenario {i+1}: Acc={acc_mod:.4f} Drop={drop_from_orig:.4f} [{grade}]\")\n",
        "\n",
        "        avg_trans_acc = np.mean(transition_accs)\n",
        "        avg_drop = acc_orig - avg_trans_acc\n",
        "        retention = (1 - avg_drop/acc_orig) * 100 if acc_orig > 0 else 0\n",
        "\n",
        "        result = {\n",
        "            \"config\": ab_cfg[\"name\"],\n",
        "            \"use_tpa\": ab_cfg[\"use_tpa\"],\n",
        "            \"use_mask\": ab_cfg[\"use_mask\"],\n",
        "            \"orig_acc\": acc_orig,\n",
        "            \"avg_trans_acc\": avg_trans_acc,\n",
        "            \"avg_drop\": avg_drop,\n",
        "            \"retention\": retention,\n",
        "            \"scenario_details\": scenario_details\n",
        "        }\n",
        "        results_table.append(result)\n",
        "\n",
        "        print(f\"\\n {ab_cfg['name']} Summary:\")\n",
        "        print(f\"   Original Test:      {acc_orig:.4f}\")\n",
        "        print(f\"   Avg Transition:     {avg_trans_acc:.4f}\")\n",
        "        print(f\"   Avg Drop:           {avg_drop:.4f}\")\n",
        "        print(f\"   Retention:          {retention:.2f}%\")\n",
        "\n",
        "    # ========================\n",
        "    # Analysis and Comparison\n",
        "    # ========================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   4-WAY ABLATION RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"{'Config':<20} {'Pooling':<8} {'Mask':<6} {'Orig':<8} {'Trans':<8} {'Drop':<8} {'Retention':<10}\")\n",
        "    print(\"-\" * 85)\n",
        "\n",
        "    for r in results_table:\n",
        "        pooling = \"TPA\" if r['use_tpa'] else \"GAP\"\n",
        "        mask = \"Yes\" if r['use_mask'] else \"No\"\n",
        "        print(f\"{r['config']:<20} {pooling:<8} {mask:<6} {r['orig_acc']:<8.4f} {r['avg_trans_acc']:<8.4f} {r['avg_drop']:<8.4f} {r['retention']:<10.2f}%\")\n",
        "\n",
        "    # Compute effects\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   EFFECT ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Find each config\n",
        "    gap_nomask = next(r for r in results_table if not r['use_tpa'] and not r['use_mask'])\n",
        "    gap_mask = next(r for r in results_table if not r['use_tpa'] and r['use_mask'])\n",
        "    tpa_nomask = next(r for r in results_table if r['use_tpa'] and not r['use_mask'])\n",
        "    tpa_mask = next(r for r in results_table if r['use_tpa'] and r['use_mask'])\n",
        "\n",
        "    # 1) Effect of Mask (in GAP)\n",
        "    mask_effect_gap = gap_nomask['avg_drop'] - gap_mask['avg_drop']\n",
        "    mask_improve_gap = (mask_effect_gap / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"1️⃣  MASK EFFECT (in GAP):\")\n",
        "    print(f\"   GAP_NoMask  → GAP_WithMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {gap_mask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {mask_improve_gap:+.2f}% drop reduction\")\n",
        "    print(f\"   Conclusion: Mask {'helps' if mask_effect_gap > 0 else 'does not help'} GAP by filtering transitions\\n\")\n",
        "\n",
        "    # 2) Effect of TPA (without mask)\n",
        "    tpa_effect_nomask = gap_nomask['avg_drop'] - tpa_nomask['avg_drop']\n",
        "    tpa_improve_nomask = (tpa_effect_nomask / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"2️⃣  TPA EFFECT (without mask):\")\n",
        "    print(f\"   GAP_NoMask  → TPA_NoMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {tpa_nomask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {tpa_improve_nomask:+.2f}% drop reduction\")\n",
        "    print(f\"   Conclusion: TPA alone {'helps' if tpa_effect_nomask > 0 else 'does not help'} via prototype attention\\n\")\n",
        "\n",
        "    # 3) Combined effect\n",
        "    combined_effect = gap_nomask['avg_drop'] - tpa_mask['avg_drop']\n",
        "    combined_improve = (combined_effect / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"3️⃣  COMBINED EFFECT (TPA + Mask):\")\n",
        "    print(f\"   GAP_NoMask  → TPA_WithMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {tpa_mask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {combined_improve:+.2f}% drop reduction\")\n",
        "    print(f\"   Retention gain: {tpa_mask['retention'] - gap_nomask['retention']:+.2f}pp\\n\")\n",
        "\n",
        "    # 4) Synergy analysis\n",
        "    expected_additive = gap_nomask['avg_drop'] - mask_effect_gap - tpa_effect_nomask\n",
        "    actual = tpa_mask['avg_drop']\n",
        "    synergy = expected_additive - actual\n",
        "    synergy_pct = (synergy / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"4️⃣  SYNERGY ANALYSIS:\")\n",
        "    print(f\"   Expected (additive): {expected_additive:.4f}\")\n",
        "    print(f\"   Actual (TPA+Mask):   {actual:.4f}\")\n",
        "    print(f\"   Synergy: {synergy:+.4f} ({synergy_pct:+.2f}%)\")\n",
        "    if synergy > 0.005:\n",
        "        print(f\"   ✓ Positive synergy: TPA and Mask work better together!\")\n",
        "    elif synergy < -0.005:\n",
        "        print(f\"   ✗ Negative synergy: Interference between TPA and Mask\")\n",
        "    else:\n",
        "        print(f\"   = No synergy: Effects are simply additive\")\n",
        "\n",
        "    # Best configuration\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    best_config = min(results_table, key=lambda x: x['avg_drop'])\n",
        "    print(f\"   BEST CONFIGURATION: {best_config['config']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Pooling: {'TPA' if best_config['use_tpa'] else 'GAP'}\")\n",
        "    print(f\"   Mask: {'Yes' if best_config['use_mask'] else 'No'}\")\n",
        "    print(f\"   Original Acc: {best_config['orig_acc']:.4f}\")\n",
        "    print(f\"   Transition Acc: {best_config['avg_trans_acc']:.4f}\")\n",
        "    print(f\"   Drop: {best_config['avg_drop']:.4f}\")\n",
        "    print(f\"   Retention: {best_config['retention']:.2f}%\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(cfg.save_dir, \"ablation_4way_results.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            'results': results_table,\n",
        "            'analysis': {\n",
        "                'mask_effect_gap': float(mask_effect_gap),\n",
        "                'tpa_effect_nomask': float(tpa_effect_nomask),\n",
        "                'combined_effect': float(combined_effect),\n",
        "                'synergy': float(synergy)\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Results saved to '{cfg.save_dir}/ablation_4way_results.json'\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ========================\n",
        "# 8) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.mode = \"ablation\"\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    config.train_augment_prob = 0.25\n",
        "    config.train_augment_mix = 0.35\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_seg_kernel = 9\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    UCI-HAR 4-WAY ABLATION STUDY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"Train Augmentation: prob={config.train_augment_prob}, mix={config.train_augment_mix}\")\n",
        "    print(f\"\\n4 Configurations to Compare:\")\n",
        "    print(f\"  1) GAP_NoMask:    Standard GAP (baseline)\")\n",
        "    print(f\"  2) GAP_WithMask:  GAP + mask filtering\")\n",
        "    print(f\"  3) TPA_NoMask:    Prototype Attention only\")\n",
        "    print(f\"  4) TPA_WithMask:  Prototype Attention + mask filtering\")\n",
        "    print(f\"\\nThis allows us to measure:\")\n",
        "    print(f\"  • Effect of mask alone\")\n",
        "    print(f\"  • Effect of TPA alone\")\n",
        "    print(f\"  • Effect of TPA + mask combined\")\n",
        "    print(f\"  • Whether there's synergy between them\")\n",
        "    print(f\"\\nTPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    if config.mode == \"ablation\":\n",
        "        run_ablation_study(config)\n",
        "    else:\n",
        "        print(\"✗ Invalid mode. Set config.mode = 'ablation'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSK_vcXE7lBQ",
        "outputId": "2e5aac9c-bb8d-4150-fd89-143be9f04097"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "    UCI-HAR 4-WAY ABLATION STUDY\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "Train Augmentation: prob=0.25, mix=0.35\n",
            "\n",
            "4 Configurations to Compare:\n",
            "  1) GAP_NoMask:    Standard GAP (baseline)\n",
            "  2) GAP_WithMask:  GAP + mask filtering\n",
            "  3) TPA_NoMask:    Prototype Attention only\n",
            "  4) TPA_WithMask:  Prototype Attention + mask filtering\n",
            "\n",
            "This allows us to measure:\n",
            "  • Effect of mask alone\n",
            "  • Effect of TPA alone\n",
            "  • Effect of TPA + mask combined\n",
            "  • Whether there's synergy between them\n",
            "\n",
            "TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total train samples: 7352\n",
            "  → Train split: 5881 (80%)\n",
            "  → Val split:   1471 (20%)\n",
            "\n",
            "Class distribution check:\n",
            "  Class 0 (WALKING): Train=981, Val=245\n",
            "  Class 1 (WALKING_UPSTAIRS): Train=858, Val=215\n",
            "  Class 2 (WALKING_DOWNSTAIRS): Train=789, Val=197\n",
            "  Class 3 (SITTING): Train=1029, Val=257\n",
            "  Class 4 (STANDING): Train=1099, Val=275\n",
            "  Class 5 (LAYING): Train=1125, Val=282\n",
            "\n",
            "======================================================================\n",
            "    4-WAY ABLATION STUDY: GAP/TPA × NoMask/WithMask\n",
            "======================================================================\n",
            "   Goal: Isolate the effect of:\n",
            "     1) Mask usage (NoMask vs WithMask)\n",
            "     2) Pooling method (GAP vs TPA)\n",
            "     3) Their interaction\n",
            "\n",
            "   Training: All 4 configs use SAME augmentation\n",
            "   Testing: Differ in mask usage and pooling method\n",
            "   Scenarios: 17 extreme transitions (p=0.70-0.75, mix=0.55-0.58)\n",
            "======================================================================\n",
            "\n",
            "  [1/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 743/2947 (25.2%)\n",
            "  [2/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 749/2947 (25.4%)\n",
            "  [3/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 708/2947 (24.0%)\n",
            "  [4/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 713/2947 (24.2%)\n",
            "  [5/17] SITTING ↔ LAYING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [6/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 747/2947 (25.3%)\n",
            "  [7/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 674/2947 (22.9%)\n",
            "  [8/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 670/2947 (22.7%)\n",
            "  [9/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [10/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 752/2947 (25.5%)\n",
            "  [11/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 795/2947 (27.0%)\n",
            "  [12/17] WALKING ↔ WALKING_UPSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 758/2947 (25.7%)\n",
            "  [13/17] SITTING ↔ LAYING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 800/2947 (27.1%)\n",
            "  [14/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 718/2947 (24.4%)\n",
            "  [15/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 803/2947 (27.2%)\n",
            "  [16/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 672/2947 (22.8%)\n",
            "  [17/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 745/2947 (25.3%)\n",
            "\n",
            "✓ 17 transitional test sets created.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP_NoMask\n",
            "   Pooling: GAP | Mask: No\n",
            "======================================================================\n",
            "\n",
            "Training GAP_NoMask for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4672 A:0.5246 Aug:16 | Val A:0.8226 F1:0.8098 ✓\n",
            "[02/100] Train L:0.9890 A:0.8427 Aug:10 | Val A:0.8987 F1:0.8980 ✓\n",
            "[03/100] Train L:0.8015 A:0.8878 Aug:14 | Val A:0.9286 F1:0.9298 ✓\n",
            "[04/100] Train L:0.6738 A:0.9111 Aug:12 | Val A:0.9320 F1:0.9348 ✓\n",
            "[05/100] Train L:0.5957 A:0.9135 Aug:13 | Val A:0.9361 F1:0.9388 ✓\n",
            "[06/100] Train L:0.5532 A:0.9167 Aug:16 | Val A:0.9415 F1:0.9449 ✓\n",
            "[07/100] Train L:0.4940 A:0.9299 Aug:12 | Val A:0.9409 F1:0.9444\n",
            "[08/100] Train L:0.4590 A:0.9311 Aug:9 | Val A:0.9415 F1:0.9450\n",
            "[09/100] Train L:0.4555 A:0.9308 Aug:13 | Val A:0.9395 F1:0.9432\n",
            "[10/100] Train L:0.4497 A:0.9342 Aug:16 | Val A:0.9381 F1:0.9419\n",
            "[11/100] Train L:0.4351 A:0.9354 Aug:13 | Val A:0.9415 F1:0.9450\n",
            "[12/100] Train L:0.4219 A:0.9345 Aug:13 | Val A:0.9436 F1:0.9470 ✓\n",
            "[13/100] Train L:0.4055 A:0.9396 Aug:9 | Val A:0.9436 F1:0.9473\n",
            "[14/100] Train L:0.3932 A:0.9417 Aug:7 | Val A:0.9531 F1:0.9564 ✓\n",
            "[15/100] Train L:0.3980 A:0.9408 Aug:10 | Val A:0.9490 F1:0.9527\n",
            "[16/100] Train L:0.4088 A:0.9408 Aug:14 | Val A:0.9497 F1:0.9534\n",
            "[17/100] Train L:0.3917 A:0.9444 Aug:10 | Val A:0.9504 F1:0.9539\n",
            "[18/100] Train L:0.3940 A:0.9480 Aug:11 | Val A:0.9511 F1:0.9547\n",
            "[19/100] Train L:0.3881 A:0.9427 Aug:9 | Val A:0.9599 F1:0.9630 ✓\n",
            "[20/100] Train L:0.3894 A:0.9442 Aug:10 | Val A:0.9565 F1:0.9598\n",
            "[21/100] Train L:0.3914 A:0.9439 Aug:13 | Val A:0.9497 F1:0.9534\n",
            "[22/100] Train L:0.4011 A:0.9418 Aug:16 | Val A:0.9531 F1:0.9566\n",
            "[23/100] Train L:0.4055 A:0.9410 Aug:19 | Val A:0.9504 F1:0.9540\n",
            "[24/100] Train L:0.3786 A:0.9444 Aug:9 | Val A:0.9517 F1:0.9553\n",
            "[25/100] Train L:0.3899 A:0.9447 Aug:14 | Val A:0.9599 F1:0.9630\n",
            "[26/100] Train L:0.3786 A:0.9493 Aug:10 | Val A:0.9538 F1:0.9573\n",
            "[27/100] Train L:0.3894 A:0.9480 Aug:16 | Val A:0.9626 F1:0.9655 ✓\n",
            "[28/100] Train L:0.3746 A:0.9478 Aug:9 | Val A:0.9558 F1:0.9592\n",
            "[29/100] Train L:0.3747 A:0.9486 Aug:11 | Val A:0.9524 F1:0.9559\n",
            "[30/100] Train L:0.3731 A:0.9497 Aug:9 | Val A:0.9517 F1:0.9553\n",
            "[31/100] Train L:0.3914 A:0.9442 Aug:17 | Val A:0.9531 F1:0.9565\n",
            "[32/100] Train L:0.3834 A:0.9444 Aug:13 | Val A:0.9538 F1:0.9572\n",
            "[33/100] Train L:0.3834 A:0.9481 Aug:14 | Val A:0.9551 F1:0.9585\n",
            "[34/100] Train L:0.3714 A:0.9488 Aug:8 | Val A:0.9613 F1:0.9643\n",
            "[35/100] Train L:0.3747 A:0.9473 Aug:11 | Val A:0.9531 F1:0.9566\n",
            "[36/100] Train L:0.3704 A:0.9502 Aug:11 | Val A:0.9599 F1:0.9630\n",
            "[37/100] Train L:0.3836 A:0.9492 Aug:15 | Val A:0.9551 F1:0.9585\n",
            "[38/100] Train L:0.3742 A:0.9485 Aug:11 | Val A:0.9538 F1:0.9570\n",
            "[39/100] Train L:0.3686 A:0.9481 Aug:8 | Val A:0.9579 F1:0.9610\n",
            "[40/100] Train L:0.3755 A:0.9510 Aug:13 | Val A:0.9545 F1:0.9579\n",
            "[41/100] Train L:0.3645 A:0.9517 Aug:9 | Val A:0.9565 F1:0.9598\n",
            "[42/100] Train L:0.3640 A:0.9507 Aug:7 | Val A:0.9558 F1:0.9591\n",
            "[43/100] Train L:0.3622 A:0.9503 Aug:8 | Val A:0.9585 F1:0.9617\n",
            "[44/100] Train L:0.3750 A:0.9464 Aug:13 | Val A:0.9592 F1:0.9623\n",
            "[45/100] Train L:0.3703 A:0.9509 Aug:10 | Val A:0.9579 F1:0.9609\n",
            "[46/100] Train L:0.3698 A:0.9466 Aug:11 | Val A:0.9565 F1:0.9598\n",
            "[47/100] Train L:0.3688 A:0.9490 Aug:12 | Val A:0.9585 F1:0.9617\n",
            "\n",
            "⚠ Early stopping triggered at epoch 47\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9626 (epoch 27)\n",
            "\n",
            "✓ Best Val Acc: 0.9626 (epoch 27)\n",
            "  Final Test Acc: 0.9084, F1: 0.9097\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8242 Drop=0.0842 [Vulnerable]\n",
            "    Scenario 2: Acc=0.8812 Drop=0.0271 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.7696 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9016 Drop=0.0068 [Very Robust]\n",
            "    Scenario 5: Acc=0.7645 Drop=0.1439 [Vulnerable]\n",
            "    Scenario 6: Acc=0.9209 Drop=-0.0126 [Very Robust]\n",
            "    Scenario 7: Acc=0.7665 Drop=0.1418 [Vulnerable]\n",
            "    Scenario 8: Acc=0.8955 Drop=0.0129 [Very Robust]\n",
            "    Scenario 9: Acc=0.8300 Drop=0.0784 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8836 Drop=0.0248 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.8083 Drop=0.1001 [Vulnerable]\n",
            "    Scenario 12: Acc=0.7340 Drop=0.1744 [Vulnerable]\n",
            "    Scenario 13: Acc=0.7133 Drop=0.1951 [Vulnerable]\n",
            "    Scenario 14: Acc=0.7329 Drop=0.1754 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8073 Drop=0.1011 [Vulnerable]\n",
            "    Scenario 16: Acc=0.7720 Drop=0.1364 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9199 Drop=-0.0115 [Very Robust]\n",
            "\n",
            " GAP_NoMask Summary:\n",
            "   Original Test:      0.9084\n",
            "   Avg Transition:     0.8191\n",
            "   Avg Drop:           0.0892\n",
            "   Retention:          90.18%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP_WithMask\n",
            "   Pooling: GAP | Mask: Yes\n",
            "======================================================================\n",
            "\n",
            "Training GAP_WithMask for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4422 A:0.5482 Aug:16 | Val A:0.8287 F1:0.8162 ✓\n",
            "[02/100] Train L:0.9528 A:0.8623 Aug:10 | Val A:0.9001 F1:0.8995 ✓\n",
            "[03/100] Train L:0.7478 A:0.9063 Aug:14 | Val A:0.9279 F1:0.9291 ✓\n",
            "[04/100] Train L:0.6237 A:0.9243 Aug:12 | Val A:0.9320 F1:0.9348 ✓\n",
            "[05/100] Train L:0.5457 A:0.9259 Aug:13 | Val A:0.9381 F1:0.9408 ✓\n",
            "[06/100] Train L:0.4920 A:0.9345 Aug:16 | Val A:0.9409 F1:0.9443 ✓\n",
            "[07/100] Train L:0.4535 A:0.9376 Aug:12 | Val A:0.9409 F1:0.9444\n",
            "[08/100] Train L:0.4310 A:0.9366 Aug:9 | Val A:0.9422 F1:0.9457 ✓\n",
            "[09/100] Train L:0.4136 A:0.9403 Aug:13 | Val A:0.9395 F1:0.9432\n",
            "[10/100] Train L:0.4009 A:0.9444 Aug:16 | Val A:0.9381 F1:0.9419\n",
            "[11/100] Train L:0.3950 A:0.9432 Aug:13 | Val A:0.9422 F1:0.9457\n",
            "[12/100] Train L:0.3867 A:0.9434 Aug:13 | Val A:0.9443 F1:0.9477 ✓\n",
            "[13/100] Train L:0.3826 A:0.9441 Aug:9 | Val A:0.9449 F1:0.9485 ✓\n",
            "[14/100] Train L:0.3756 A:0.9456 Aug:7 | Val A:0.9545 F1:0.9578 ✓\n",
            "[15/100] Train L:0.3733 A:0.9466 Aug:10 | Val A:0.9490 F1:0.9526\n",
            "[16/100] Train L:0.3734 A:0.9486 Aug:14 | Val A:0.9504 F1:0.9541\n",
            "[17/100] Train L:0.3687 A:0.9483 Aug:10 | Val A:0.9511 F1:0.9545\n",
            "[18/100] Train L:0.3686 A:0.9509 Aug:11 | Val A:0.9524 F1:0.9560\n",
            "[19/100] Train L:0.3658 A:0.9468 Aug:9 | Val A:0.9592 F1:0.9623 ✓\n",
            "[20/100] Train L:0.3653 A:0.9493 Aug:10 | Val A:0.9565 F1:0.9598\n",
            "[21/100] Train L:0.3606 A:0.9488 Aug:13 | Val A:0.9504 F1:0.9540\n",
            "[22/100] Train L:0.3638 A:0.9478 Aug:16 | Val A:0.9545 F1:0.9579\n",
            "[23/100] Train L:0.3614 A:0.9476 Aug:19 | Val A:0.9511 F1:0.9547\n",
            "[24/100] Train L:0.3577 A:0.9468 Aug:9 | Val A:0.9517 F1:0.9553\n",
            "[25/100] Train L:0.3589 A:0.9486 Aug:14 | Val A:0.9592 F1:0.9623\n",
            "[26/100] Train L:0.3558 A:0.9520 Aug:10 | Val A:0.9538 F1:0.9573\n",
            "[27/100] Train L:0.3564 A:0.9519 Aug:16 | Val A:0.9619 F1:0.9649 ✓\n",
            "[28/100] Train L:0.3541 A:0.9492 Aug:9 | Val A:0.9572 F1:0.9604\n",
            "[29/100] Train L:0.3514 A:0.9515 Aug:11 | Val A:0.9524 F1:0.9559\n",
            "[30/100] Train L:0.3523 A:0.9517 Aug:9 | Val A:0.9531 F1:0.9565\n",
            "[31/100] Train L:0.3526 A:0.9498 Aug:17 | Val A:0.9558 F1:0.9591\n",
            "[32/100] Train L:0.3552 A:0.9483 Aug:13 | Val A:0.9565 F1:0.9597\n",
            "[33/100] Train L:0.3507 A:0.9519 Aug:14 | Val A:0.9538 F1:0.9573\n",
            "[34/100] Train L:0.3523 A:0.9520 Aug:8 | Val A:0.9592 F1:0.9624\n",
            "[35/100] Train L:0.3508 A:0.9512 Aug:11 | Val A:0.9531 F1:0.9566\n",
            "[36/100] Train L:0.3475 A:0.9524 Aug:11 | Val A:0.9592 F1:0.9624\n",
            "[37/100] Train L:0.3512 A:0.9526 Aug:15 | Val A:0.9558 F1:0.9591\n",
            "[38/100] Train L:0.3504 A:0.9517 Aug:11 | Val A:0.9545 F1:0.9577\n",
            "[39/100] Train L:0.3509 A:0.9514 Aug:8 | Val A:0.9579 F1:0.9610\n",
            "[40/100] Train L:0.3492 A:0.9532 Aug:13 | Val A:0.9551 F1:0.9585\n",
            "[41/100] Train L:0.3457 A:0.9519 Aug:9 | Val A:0.9572 F1:0.9604\n",
            "[42/100] Train L:0.3470 A:0.9520 Aug:7 | Val A:0.9565 F1:0.9597\n",
            "[43/100] Train L:0.3443 A:0.9534 Aug:8 | Val A:0.9606 F1:0.9636\n",
            "[44/100] Train L:0.3461 A:0.9502 Aug:13 | Val A:0.9599 F1:0.9630\n",
            "[45/100] Train L:0.3446 A:0.9527 Aug:10 | Val A:0.9572 F1:0.9603\n",
            "[46/100] Train L:0.3442 A:0.9500 Aug:11 | Val A:0.9565 F1:0.9598\n",
            "[47/100] Train L:0.3440 A:0.9514 Aug:12 | Val A:0.9585 F1:0.9617\n",
            "\n",
            "⚠ Early stopping triggered at epoch 47\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9619 (epoch 27)\n",
            "\n",
            "✓ Best Val Acc: 0.9619 (epoch 27)\n",
            "  Final Test Acc: 0.9111, F1: 0.9125\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9125 Drop=-0.0014 [Very Robust]\n",
            "    Scenario 2: Acc=0.9097 Drop=0.0014 [Very Robust]\n",
            "    Scenario 3: Acc=0.9101 Drop=0.0010 [Very Robust]\n",
            "    Scenario 4: Acc=0.9094 Drop=0.0017 [Very Robust]\n",
            "    Scenario 5: Acc=0.9111 Drop=0.0000 [Very Robust]\n",
            "    Scenario 6: Acc=0.9111 Drop=0.0000 [Very Robust]\n",
            "    Scenario 7: Acc=0.9094 Drop=0.0017 [Very Robust]\n",
            "    Scenario 8: Acc=0.9084 Drop=0.0027 [Very Robust]\n",
            "    Scenario 9: Acc=0.9118 Drop=-0.0007 [Very Robust]\n",
            "    Scenario 10: Acc=0.9101 Drop=0.0010 [Very Robust]\n",
            "    Scenario 11: Acc=0.9111 Drop=0.0000 [Very Robust]\n",
            "    Scenario 12: Acc=0.9091 Drop=0.0020 [Very Robust]\n",
            "    Scenario 13: Acc=0.9104 Drop=0.0007 [Very Robust]\n",
            "    Scenario 14: Acc=0.9097 Drop=0.0014 [Very Robust]\n",
            "    Scenario 15: Acc=0.9121 Drop=-0.0010 [Very Robust]\n",
            "    Scenario 16: Acc=0.9077 Drop=0.0034 [Very Robust]\n",
            "    Scenario 17: Acc=0.9114 Drop=-0.0003 [Very Robust]\n",
            "\n",
            " GAP_WithMask Summary:\n",
            "   Original Test:      0.9111\n",
            "   Avg Transition:     0.9103\n",
            "   Avg Drop:           0.0008\n",
            "   Retention:          99.91%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: TPA_NoMask\n",
            "   Pooling: TPA | Mask: No\n",
            "======================================================================\n",
            "\n",
            "Training TPA_NoMask for up to 100 epochs (patience=20)...\n",
            "TPA: prototypes=16, heads=4, temp=0.07\n",
            "[01/100] Train L:1.4688 A:0.5402 Aug:16 | Val A:0.6941 F1:0.6292 | Conf:0.484 ✓\n",
            "[02/100] Train L:0.9344 A:0.7982 Aug:10 | Val A:0.8899 F1:0.8884 | Conf:0.384 ✓\n",
            "[03/100] Train L:0.7234 A:0.8677 Aug:14 | Val A:0.8967 F1:0.8957 | Conf:0.349 ✓\n",
            "[04/100] Train L:0.5684 A:0.8932 Aug:12 | Val A:0.9531 F1:0.9557 | Conf:0.362 ✓\n",
            "[05/100] Train L:0.5017 A:0.9158 Aug:13 | Val A:0.9477 F1:0.9510 | Conf:0.375\n",
            "[06/100] Train L:0.4717 A:0.9272 Aug:16 | Val A:0.9483 F1:0.9520 | Conf:0.388\n",
            "[07/100] Train L:0.4250 A:0.9388 Aug:12 | Val A:0.9483 F1:0.9521 | Conf:0.391\n",
            "[08/100] Train L:0.4073 A:0.9395 Aug:9 | Val A:0.9565 F1:0.9597 | Conf:0.390 ✓\n",
            "[09/100] Train L:0.4023 A:0.9390 Aug:13 | Val A:0.9511 F1:0.9546 | Conf:0.391\n",
            "[10/100] Train L:0.3974 A:0.9435 Aug:16 | Val A:0.9524 F1:0.9557 | Conf:0.391\n",
            "[11/100] Train L:0.3893 A:0.9418 Aug:13 | Val A:0.9524 F1:0.9560 | Conf:0.389\n",
            "[12/100] Train L:0.3832 A:0.9429 Aug:13 | Val A:0.9572 F1:0.9604 | Conf:0.386 ✓\n",
            "[13/100] Train L:0.3740 A:0.9463 Aug:9 | Val A:0.9565 F1:0.9597 | Conf:0.383\n",
            "[14/100] Train L:0.3652 A:0.9476 Aug:7 | Val A:0.9558 F1:0.9592 | Conf:0.384\n",
            "[15/100] Train L:0.3658 A:0.9500 Aug:10 | Val A:0.9572 F1:0.9604 | Conf:0.381\n",
            "[16/100] Train L:0.3701 A:0.9471 Aug:14 | Val A:0.9633 F1:0.9662 | Conf:0.385 ✓\n",
            "[17/100] Train L:0.3616 A:0.9500 Aug:10 | Val A:0.9585 F1:0.9617 | Conf:0.383\n",
            "[18/100] Train L:0.3608 A:0.9495 Aug:11 | Val A:0.9592 F1:0.9623 | Conf:0.384\n",
            "[19/100] Train L:0.3565 A:0.9510 Aug:9 | Val A:0.9599 F1:0.9630 | Conf:0.384\n",
            "[20/100] Train L:0.3535 A:0.9520 Aug:10 | Val A:0.9565 F1:0.9598 | Conf:0.386\n",
            "[21/100] Train L:0.3524 A:0.9534 Aug:13 | Val A:0.9599 F1:0.9629 | Conf:0.397\n",
            "[22/100] Train L:0.3587 A:0.9478 Aug:16 | Val A:0.9585 F1:0.9615 | Conf:0.406\n",
            "[23/100] Train L:0.3516 A:0.9531 Aug:19 | Val A:0.9599 F1:0.9629 | Conf:0.407\n",
            "[24/100] Train L:0.3462 A:0.9549 Aug:9 | Val A:0.9653 F1:0.9680 | Conf:0.400 ✓\n",
            "[25/100] Train L:0.3497 A:0.9537 Aug:14 | Val A:0.9640 F1:0.9668 | Conf:0.409\n",
            "[26/100] Train L:0.3401 A:0.9587 Aug:10 | Val A:0.9626 F1:0.9655 | Conf:0.406\n",
            "[27/100] Train L:0.3453 A:0.9546 Aug:16 | Val A:0.9687 F1:0.9712 | Conf:0.413 ✓\n",
            "[28/100] Train L:0.3323 A:0.9595 Aug:9 | Val A:0.9667 F1:0.9692 | Conf:0.410\n",
            "[29/100] Train L:0.3297 A:0.9614 Aug:11 | Val A:0.9680 F1:0.9705 | Conf:0.412\n",
            "[30/100] Train L:0.3313 A:0.9594 Aug:9 | Val A:0.9680 F1:0.9705 | Conf:0.413\n",
            "[31/100] Train L:0.3347 A:0.9587 Aug:17 | Val A:0.9646 F1:0.9674 | Conf:0.418\n",
            "[32/100] Train L:0.3319 A:0.9587 Aug:13 | Val A:0.9653 F1:0.9679 | Conf:0.421\n",
            "[33/100] Train L:0.3334 A:0.9585 Aug:14 | Val A:0.9714 F1:0.9736 | Conf:0.418 ✓\n",
            "[34/100] Train L:0.3254 A:0.9628 Aug:8 | Val A:0.9599 F1:0.9628 | Conf:0.414\n",
            "[35/100] Train L:0.3235 A:0.9633 Aug:11 | Val A:0.9674 F1:0.9699 | Conf:0.420\n",
            "[36/100] Train L:0.3197 A:0.9634 Aug:11 | Val A:0.9701 F1:0.9724 | Conf:0.421\n",
            "[37/100] Train L:0.3245 A:0.9646 Aug:15 | Val A:0.9619 F1:0.9647 | Conf:0.422\n",
            "[38/100] Train L:0.3205 A:0.9641 Aug:11 | Val A:0.9714 F1:0.9737 | Conf:0.420\n",
            "[39/100] Train L:0.3218 A:0.9650 Aug:8 | Val A:0.9646 F1:0.9674 | Conf:0.416\n",
            "[40/100] Train L:0.3194 A:0.9645 Aug:13 | Val A:0.9701 F1:0.9724 | Conf:0.421\n",
            "[41/100] Train L:0.3083 A:0.9701 Aug:9 | Val A:0.9748 F1:0.9768 | Conf:0.421 ✓\n",
            "[42/100] Train L:0.3084 A:0.9708 Aug:7 | Val A:0.9694 F1:0.9718 | Conf:0.418\n",
            "[43/100] Train L:0.3081 A:0.9691 Aug:8 | Val A:0.9708 F1:0.9730 | Conf:0.421\n",
            "[44/100] Train L:0.3214 A:0.9645 Aug:13 | Val A:0.9735 F1:0.9755 | Conf:0.427\n",
            "[45/100] Train L:0.3097 A:0.9691 Aug:10 | Val A:0.9755 F1:0.9774 | Conf:0.424 ✓\n",
            "[46/100] Train L:0.3083 A:0.9723 Aug:11 | Val A:0.9728 F1:0.9749 | Conf:0.425\n",
            "[47/100] Train L:0.3054 A:0.9721 Aug:12 | Val A:0.9714 F1:0.9736 | Conf:0.428\n",
            "[48/100] Train L:0.3092 A:0.9713 Aug:13 | Val A:0.9742 F1:0.9762 | Conf:0.429\n",
            "[49/100] Train L:0.3044 A:0.9745 Aug:14 | Val A:0.9721 F1:0.9742 | Conf:0.426\n",
            "[50/100] Train L:0.3053 A:0.9708 Aug:5 | Val A:0.9755 F1:0.9774 | Conf:0.422\n",
            "[51/100] Train L:0.3065 A:0.9721 Aug:19 | Val A:0.9776 F1:0.9792 | Conf:0.438 ✓\n",
            "[52/100] Train L:0.3003 A:0.9748 Aug:11 | Val A:0.9789 F1:0.9805 | Conf:0.431 ✓\n",
            "[53/100] Train L:0.3035 A:0.9743 Aug:11 | Val A:0.9748 F1:0.9768 | Conf:0.432\n",
            "[54/100] Train L:0.3031 A:0.9753 Aug:17 | Val A:0.9769 F1:0.9786 | Conf:0.440\n",
            "[55/100] Train L:0.3066 A:0.9728 Aug:14 | Val A:0.9755 F1:0.9773 | Conf:0.437\n",
            "[56/100] Train L:0.2924 A:0.9787 Aug:10 | Val A:0.9769 F1:0.9786 | Conf:0.433\n",
            "[57/100] Train L:0.2952 A:0.9769 Aug:10 | Val A:0.9762 F1:0.9780 | Conf:0.430\n",
            "[58/100] Train L:0.2984 A:0.9760 Aug:12 | Val A:0.9782 F1:0.9799 | Conf:0.434\n",
            "[59/100] Train L:0.2956 A:0.9762 Aug:8 | Val A:0.9755 F1:0.9773 | Conf:0.432\n",
            "[60/100] Train L:0.2952 A:0.9794 Aug:13 | Val A:0.9762 F1:0.9780 | Conf:0.440\n",
            "[61/100] Train L:0.2972 A:0.9784 Aug:14 | Val A:0.9782 F1:0.9798 | Conf:0.432\n",
            "[62/100] Train L:0.2926 A:0.9777 Aug:10 | Val A:0.9796 F1:0.9812 | Conf:0.433 ✓\n",
            "[63/100] Train L:0.2884 A:0.9815 Aug:7 | Val A:0.9810 F1:0.9824 | Conf:0.430 ✓\n",
            "[64/100] Train L:0.2816 A:0.9847 Aug:12 | Val A:0.9837 F1:0.9849 | Conf:0.436 ✓\n",
            "[65/100] Train L:0.2824 A:0.9840 Aug:15 | Val A:0.9803 F1:0.9818 | Conf:0.435\n",
            "[66/100] Train L:0.2867 A:0.9804 Aug:15 | Val A:0.9742 F1:0.9760 | Conf:0.436\n",
            "[67/100] Train L:0.2821 A:0.9833 Aug:8 | Val A:0.9816 F1:0.9830 | Conf:0.427\n",
            "[68/100] Train L:0.2895 A:0.9799 Aug:10 | Val A:0.9844 F1:0.9856 | Conf:0.427 ✓\n",
            "[69/100] Train L:0.2761 A:0.9862 Aug:3 | Val A:0.9823 F1:0.9836 | Conf:0.425\n",
            "[70/100] Train L:0.2864 A:0.9818 Aug:17 | Val A:0.9830 F1:0.9843 | Conf:0.441\n",
            "[71/100] Train L:0.2801 A:0.9842 Aug:12 | Val A:0.9864 F1:0.9874 | Conf:0.434 ✓\n",
            "[72/100] Train L:0.2759 A:0.9879 Aug:11 | Val A:0.9816 F1:0.9829 | Conf:0.434\n",
            "[73/100] Train L:0.2824 A:0.9835 Aug:13 | Val A:0.9844 F1:0.9856 | Conf:0.435\n",
            "[74/100] Train L:0.2728 A:0.9886 Aug:9 | Val A:0.9830 F1:0.9843 | Conf:0.432\n",
            "[75/100] Train L:0.2795 A:0.9832 Aug:13 | Val A:0.9837 F1:0.9849 | Conf:0.437\n",
            "[76/100] Train L:0.2756 A:0.9859 Aug:8 | Val A:0.9857 F1:0.9868 | Conf:0.432\n",
            "[77/100] Train L:0.2721 A:0.9879 Aug:8 | Val A:0.9864 F1:0.9874 | Conf:0.427\n",
            "[78/100] Train L:0.2694 A:0.9898 Aug:7 | Val A:0.9823 F1:0.9836 | Conf:0.431\n",
            "[79/100] Train L:0.2761 A:0.9866 Aug:9 | Val A:0.9884 F1:0.9893 | Conf:0.431 ✓\n",
            "[80/100] Train L:0.2725 A:0.9886 Aug:14 | Val A:0.9884 F1:0.9893 | Conf:0.438\n",
            "[81/100] Train L:0.2730 A:0.9891 Aug:13 | Val A:0.9864 F1:0.9874 | Conf:0.430\n",
            "[82/100] Train L:0.2665 A:0.9908 Aug:8 | Val A:0.9884 F1:0.9893 | Conf:0.437\n",
            "[83/100] Train L:0.2650 A:0.9913 Aug:10 | Val A:0.9864 F1:0.9874 | Conf:0.432\n",
            "[84/100] Train L:0.2676 A:0.9908 Aug:10 | Val A:0.9878 F1:0.9887 | Conf:0.436\n",
            "[85/100] Train L:0.2699 A:0.9898 Aug:14 | Val A:0.9884 F1:0.9893 | Conf:0.441\n",
            "[86/100] Train L:0.2663 A:0.9900 Aug:7 | Val A:0.9884 F1:0.9893 | Conf:0.436\n",
            "[87/100] Train L:0.2664 A:0.9908 Aug:13 | Val A:0.9803 F1:0.9818 | Conf:0.438\n",
            "[88/100] Train L:0.2675 A:0.9906 Aug:11 | Val A:0.9898 F1:0.9906 | Conf:0.436 ✓\n",
            "[89/100] Train L:0.2635 A:0.9918 Aug:12 | Val A:0.9891 F1:0.9900 | Conf:0.439\n",
            "[90/100] Train L:0.2640 A:0.9918 Aug:7 | Val A:0.9912 F1:0.9918 | Conf:0.430 ✓\n",
            "[91/100] Train L:0.2621 A:0.9930 Aug:6 | Val A:0.9871 F1:0.9880 | Conf:0.430\n",
            "[92/100] Train L:0.2626 A:0.9932 Aug:10 | Val A:0.9871 F1:0.9881 | Conf:0.433\n",
            "[93/100] Train L:0.2572 A:0.9959 Aug:10 | Val A:0.9878 F1:0.9887 | Conf:0.436\n",
            "[94/100] Train L:0.2682 A:0.9898 Aug:14 | Val A:0.9905 F1:0.9912 | Conf:0.442\n",
            "[95/100] Train L:0.2584 A:0.9946 Aug:9 | Val A:0.9884 F1:0.9893 | Conf:0.440\n",
            "[96/100] Train L:0.2582 A:0.9947 Aug:9 | Val A:0.9905 F1:0.9912 | Conf:0.432\n",
            "[97/100] Train L:0.2572 A:0.9956 Aug:13 | Val A:0.9898 F1:0.9906 | Conf:0.441\n",
            "[98/100] Train L:0.2576 A:0.9949 Aug:10 | Val A:0.9884 F1:0.9893 | Conf:0.438\n",
            "[99/100] Train L:0.2606 A:0.9937 Aug:12 | Val A:0.9878 F1:0.9887 | Conf:0.440\n",
            "[100/100] Train L:0.2609 A:0.9937 Aug:12 | Val A:0.9912 F1:0.9918 | Conf:0.436\n",
            "\n",
            "✓ Best Val Acc: 0.9912 (epoch 90)\n",
            "  Final Test Acc: 0.9515, F1: 0.9511\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9318 Drop=0.0197 [Very Robust]\n",
            "    Scenario 2: Acc=0.9294 Drop=0.0221 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.8395 Drop=0.1120 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9281 Drop=0.0234 [Slightly Vulnerable]\n",
            "    Scenario 5: Acc=0.9481 Drop=0.0034 [Very Robust]\n",
            "    Scenario 6: Acc=0.9430 Drop=0.0085 [Very Robust]\n",
            "    Scenario 7: Acc=0.8456 Drop=0.1059 [Vulnerable]\n",
            "    Scenario 8: Acc=0.9253 Drop=0.0261 [Slightly Vulnerable]\n",
            "    Scenario 9: Acc=0.9270 Drop=0.0244 [Slightly Vulnerable]\n",
            "    Scenario 10: Acc=0.9311 Drop=0.0204 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.9199 Drop=0.0316 [Slightly Vulnerable]\n",
            "    Scenario 12: Acc=0.8127 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 13: Acc=0.9522 Drop=-0.0007 [Very Robust]\n",
            "    Scenario 14: Acc=0.8171 Drop=0.1344 [Vulnerable]\n",
            "    Scenario 15: Acc=0.9233 Drop=0.0282 [Slightly Vulnerable]\n",
            "    Scenario 16: Acc=0.8195 Drop=0.1320 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9440 Drop=0.0075 [Very Robust]\n",
            "\n",
            " TPA_NoMask Summary:\n",
            "   Original Test:      0.9515\n",
            "   Avg Transition:     0.9022\n",
            "   Avg Drop:           0.0493\n",
            "   Retention:          94.82%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: TPA_WithMask\n",
            "   Pooling: TPA | Mask: Yes\n",
            "======================================================================\n",
            "\n",
            "Training TPA_WithMask for up to 100 epochs (patience=20)...\n",
            "TPA: prototypes=16, heads=4, temp=0.07\n",
            "[01/100] Train L:1.4377 A:0.5552 Aug:16 | Val A:0.6825 F1:0.6110 | Conf:0.483 ✓\n",
            "[02/100] Train L:0.8830 A:0.8131 Aug:10 | Val A:0.8872 F1:0.8854 | Conf:0.381 ✓\n",
            "[03/100] Train L:0.6339 A:0.8890 Aug:14 | Val A:0.9014 F1:0.9008 | Conf:0.369 ✓\n",
            "[04/100] Train L:0.4875 A:0.9260 Aug:12 | Val A:0.9565 F1:0.9591 | Conf:0.386 ✓\n",
            "[05/100] Train L:0.4366 A:0.9386 Aug:13 | Val A:0.9463 F1:0.9496 | Conf:0.412\n",
            "[06/100] Train L:0.4026 A:0.9468 Aug:16 | Val A:0.9524 F1:0.9560 | Conf:0.423\n",
            "[07/100] Train L:0.3823 A:0.9485 Aug:12 | Val A:0.9497 F1:0.9534 | Conf:0.427\n",
            "[08/100] Train L:0.3768 A:0.9452 Aug:9 | Val A:0.9599 F1:0.9630 | Conf:0.426 ✓\n",
            "[09/100] Train L:0.3664 A:0.9483 Aug:13 | Val A:0.9531 F1:0.9564 | Conf:0.424\n",
            "[10/100] Train L:0.3620 A:0.9524 Aug:16 | Val A:0.9538 F1:0.9570 | Conf:0.425\n",
            "[11/100] Train L:0.3621 A:0.9471 Aug:13 | Val A:0.9545 F1:0.9579 | Conf:0.421\n",
            "[12/100] Train L:0.3542 A:0.9515 Aug:13 | Val A:0.9633 F1:0.9661 | Conf:0.423 ✓\n",
            "[13/100] Train L:0.3538 A:0.9507 Aug:9 | Val A:0.9551 F1:0.9585 | Conf:0.423\n",
            "[14/100] Train L:0.3479 A:0.9517 Aug:7 | Val A:0.9633 F1:0.9661 | Conf:0.427\n",
            "[15/100] Train L:0.3463 A:0.9529 Aug:10 | Val A:0.9613 F1:0.9642 | Conf:0.426\n",
            "[16/100] Train L:0.3467 A:0.9553 Aug:14 | Val A:0.9619 F1:0.9649 | Conf:0.434\n",
            "[17/100] Train L:0.3429 A:0.9536 Aug:10 | Val A:0.9667 F1:0.9693 | Conf:0.433 ✓\n",
            "[18/100] Train L:0.3414 A:0.9536 Aug:11 | Val A:0.9646 F1:0.9674 | Conf:0.435\n",
            "[19/100] Train L:0.3391 A:0.9548 Aug:9 | Val A:0.9626 F1:0.9655 | Conf:0.440\n",
            "[20/100] Train L:0.3380 A:0.9548 Aug:10 | Val A:0.9626 F1:0.9655 | Conf:0.440\n",
            "[21/100] Train L:0.3334 A:0.9568 Aug:13 | Val A:0.9640 F1:0.9667 | Conf:0.444\n",
            "[22/100] Train L:0.3373 A:0.9548 Aug:16 | Val A:0.9572 F1:0.9603 | Conf:0.444\n",
            "[23/100] Train L:0.3315 A:0.9560 Aug:19 | Val A:0.9640 F1:0.9668 | Conf:0.440\n",
            "[24/100] Train L:0.3311 A:0.9587 Aug:9 | Val A:0.9613 F1:0.9643 | Conf:0.444\n",
            "[25/100] Train L:0.3283 A:0.9607 Aug:14 | Val A:0.9606 F1:0.9636 | Conf:0.442\n",
            "[26/100] Train L:0.3231 A:0.9633 Aug:10 | Val A:0.9613 F1:0.9641 | Conf:0.445\n",
            "[27/100] Train L:0.3309 A:0.9573 Aug:16 | Val A:0.9674 F1:0.9699 | Conf:0.443 ✓\n",
            "[28/100] Train L:0.3228 A:0.9611 Aug:9 | Val A:0.9646 F1:0.9674 | Conf:0.442\n",
            "[29/100] Train L:0.3155 A:0.9651 Aug:11 | Val A:0.9714 F1:0.9736 | Conf:0.446 ✓\n",
            "[30/100] Train L:0.3188 A:0.9658 Aug:9 | Val A:0.9674 F1:0.9699 | Conf:0.443\n",
            "[31/100] Train L:0.3163 A:0.9658 Aug:17 | Val A:0.9680 F1:0.9705 | Conf:0.443\n",
            "[32/100] Train L:0.3138 A:0.9658 Aug:13 | Val A:0.9680 F1:0.9704 | Conf:0.446\n",
            "[33/100] Train L:0.3117 A:0.9684 Aug:14 | Val A:0.9701 F1:0.9724 | Conf:0.445\n",
            "[34/100] Train L:0.3126 A:0.9685 Aug:8 | Val A:0.9694 F1:0.9715 | Conf:0.443\n",
            "[35/100] Train L:0.3088 A:0.9708 Aug:11 | Val A:0.9694 F1:0.9718 | Conf:0.442\n",
            "[36/100] Train L:0.3048 A:0.9704 Aug:11 | Val A:0.9742 F1:0.9761 | Conf:0.443 ✓\n",
            "[37/100] Train L:0.3050 A:0.9723 Aug:15 | Val A:0.9708 F1:0.9730 | Conf:0.442\n",
            "[38/100] Train L:0.3042 A:0.9694 Aug:11 | Val A:0.9694 F1:0.9718 | Conf:0.443\n",
            "[39/100] Train L:0.3114 A:0.9685 Aug:8 | Val A:0.9687 F1:0.9712 | Conf:0.437\n",
            "[40/100] Train L:0.3045 A:0.9709 Aug:13 | Val A:0.9742 F1:0.9761 | Conf:0.438\n",
            "[41/100] Train L:0.2984 A:0.9742 Aug:9 | Val A:0.9708 F1:0.9730 | Conf:0.440\n",
            "[42/100] Train L:0.2949 A:0.9759 Aug:7 | Val A:0.9721 F1:0.9743 | Conf:0.438\n",
            "[43/100] Train L:0.3003 A:0.9764 Aug:8 | Val A:0.9748 F1:0.9767 | Conf:0.437 ✓\n",
            "[44/100] Train L:0.3026 A:0.9725 Aug:13 | Val A:0.9755 F1:0.9774 | Conf:0.440 ✓\n",
            "[45/100] Train L:0.2965 A:0.9752 Aug:10 | Val A:0.9762 F1:0.9780 | Conf:0.437 ✓\n",
            "[46/100] Train L:0.2941 A:0.9770 Aug:11 | Val A:0.9748 F1:0.9768 | Conf:0.437\n",
            "[47/100] Train L:0.2902 A:0.9786 Aug:12 | Val A:0.9776 F1:0.9793 | Conf:0.437 ✓\n",
            "[48/100] Train L:0.2900 A:0.9793 Aug:13 | Val A:0.9803 F1:0.9818 | Conf:0.441 ✓\n",
            "[49/100] Train L:0.2918 A:0.9777 Aug:14 | Val A:0.9755 F1:0.9774 | Conf:0.436\n",
            "[50/100] Train L:0.2896 A:0.9772 Aug:5 | Val A:0.9796 F1:0.9812 | Conf:0.437\n",
            "[51/100] Train L:0.2889 A:0.9796 Aug:19 | Val A:0.9769 F1:0.9787 | Conf:0.439\n",
            "[52/100] Train L:0.2886 A:0.9798 Aug:11 | Val A:0.9789 F1:0.9805 | Conf:0.440\n",
            "[53/100] Train L:0.2886 A:0.9808 Aug:11 | Val A:0.9776 F1:0.9793 | Conf:0.437\n",
            "[54/100] Train L:0.2836 A:0.9837 Aug:17 | Val A:0.9850 F1:0.9862 | Conf:0.433 ✓\n",
            "[55/100] Train L:0.2855 A:0.9801 Aug:14 | Val A:0.9816 F1:0.9830 | Conf:0.432\n",
            "[56/100] Train L:0.2769 A:0.9859 Aug:10 | Val A:0.9837 F1:0.9849 | Conf:0.432\n",
            "[57/100] Train L:0.2771 A:0.9859 Aug:10 | Val A:0.9816 F1:0.9831 | Conf:0.426\n",
            "[58/100] Train L:0.2815 A:0.9832 Aug:12 | Val A:0.9796 F1:0.9812 | Conf:0.431\n",
            "[59/100] Train L:0.2783 A:0.9849 Aug:8 | Val A:0.9810 F1:0.9824 | Conf:0.430\n",
            "[60/100] Train L:0.2786 A:0.9850 Aug:13 | Val A:0.9837 F1:0.9850 | Conf:0.432\n",
            "[61/100] Train L:0.2768 A:0.9861 Aug:14 | Val A:0.9857 F1:0.9868 | Conf:0.427 ✓\n",
            "[62/100] Train L:0.2756 A:0.9864 Aug:10 | Val A:0.9864 F1:0.9875 | Conf:0.426 ✓\n",
            "[63/100] Train L:0.2736 A:0.9876 Aug:7 | Val A:0.9857 F1:0.9868 | Conf:0.425\n",
            "[64/100] Train L:0.2685 A:0.9901 Aug:12 | Val A:0.9918 F1:0.9925 | Conf:0.428 ✓\n",
            "[65/100] Train L:0.2674 A:0.9910 Aug:15 | Val A:0.9850 F1:0.9862 | Conf:0.425\n",
            "[66/100] Train L:0.2684 A:0.9895 Aug:15 | Val A:0.9816 F1:0.9830 | Conf:0.421\n",
            "[67/100] Train L:0.2691 A:0.9888 Aug:8 | Val A:0.9891 F1:0.9900 | Conf:0.418\n",
            "[68/100] Train L:0.2671 A:0.9900 Aug:10 | Val A:0.9898 F1:0.9906 | Conf:0.421\n",
            "[69/100] Train L:0.2616 A:0.9932 Aug:3 | Val A:0.9898 F1:0.9906 | Conf:0.418\n",
            "[70/100] Train L:0.2675 A:0.9908 Aug:17 | Val A:0.9884 F1:0.9893 | Conf:0.426\n",
            "[71/100] Train L:0.2659 A:0.9910 Aug:12 | Val A:0.9884 F1:0.9893 | Conf:0.416\n",
            "[72/100] Train L:0.2600 A:0.9942 Aug:11 | Val A:0.9878 F1:0.9887 | Conf:0.421\n",
            "[73/100] Train L:0.2654 A:0.9893 Aug:13 | Val A:0.9823 F1:0.9837 | Conf:0.418\n",
            "[74/100] Train L:0.2617 A:0.9929 Aug:9 | Val A:0.9864 F1:0.9875 | Conf:0.420\n",
            "[75/100] Train L:0.2667 A:0.9905 Aug:13 | Val A:0.9878 F1:0.9887 | Conf:0.418\n",
            "[76/100] Train L:0.2604 A:0.9930 Aug:8 | Val A:0.9884 F1:0.9893 | Conf:0.419\n",
            "[77/100] Train L:0.2578 A:0.9952 Aug:8 | Val A:0.9891 F1:0.9900 | Conf:0.418\n",
            "[78/100] Train L:0.2584 A:0.9940 Aug:7 | Val A:0.9912 F1:0.9918 | Conf:0.421\n",
            "[79/100] Train L:0.2616 A:0.9927 Aug:9 | Val A:0.9884 F1:0.9893 | Conf:0.419\n",
            "[80/100] Train L:0.2588 A:0.9939 Aug:14 | Val A:0.9884 F1:0.9893 | Conf:0.420\n",
            "[81/100] Train L:0.2584 A:0.9932 Aug:13 | Val A:0.9898 F1:0.9906 | Conf:0.419\n",
            "[82/100] Train L:0.2546 A:0.9957 Aug:8 | Val A:0.9898 F1:0.9906 | Conf:0.421\n",
            "[83/100] Train L:0.2548 A:0.9952 Aug:10 | Val A:0.9891 F1:0.9900 | Conf:0.423\n",
            "[84/100] Train L:0.2596 A:0.9937 Aug:10 | Val A:0.9857 F1:0.9868 | Conf:0.426\n",
            "\n",
            "⚠ Early stopping triggered at epoch 84\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9918 (epoch 64)\n",
            "\n",
            "✓ Best Val Acc: 0.9918 (epoch 64)\n",
            "  Final Test Acc: 0.9301, F1: 0.9304\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9230 Drop=0.0071 [Very Robust]\n",
            "    Scenario 2: Acc=0.9118 Drop=0.0183 [Very Robust]\n",
            "    Scenario 3: Acc=0.9257 Drop=0.0044 [Very Robust]\n",
            "    Scenario 4: Acc=0.9230 Drop=0.0071 [Very Robust]\n",
            "    Scenario 5: Acc=0.9250 Drop=0.0051 [Very Robust]\n",
            "    Scenario 6: Acc=0.9233 Drop=0.0068 [Very Robust]\n",
            "    Scenario 7: Acc=0.9257 Drop=0.0044 [Very Robust]\n",
            "    Scenario 8: Acc=0.9257 Drop=0.0044 [Very Robust]\n",
            "    Scenario 9: Acc=0.9199 Drop=0.0102 [Very Robust]\n",
            "    Scenario 10: Acc=0.9206 Drop=0.0095 [Very Robust]\n",
            "    Scenario 11: Acc=0.9216 Drop=0.0085 [Very Robust]\n",
            "    Scenario 12: Acc=0.9216 Drop=0.0085 [Very Robust]\n",
            "    Scenario 13: Acc=0.9250 Drop=0.0051 [Very Robust]\n",
            "    Scenario 14: Acc=0.9253 Drop=0.0048 [Very Robust]\n",
            "    Scenario 15: Acc=0.9196 Drop=0.0105 [Very Robust]\n",
            "    Scenario 16: Acc=0.9162 Drop=0.0139 [Very Robust]\n",
            "    Scenario 17: Acc=0.9199 Drop=0.0102 [Very Robust]\n",
            "\n",
            " TPA_WithMask Summary:\n",
            "   Original Test:      0.9301\n",
            "   Avg Transition:     0.9219\n",
            "   Avg Drop:           0.0082\n",
            "   Retention:          99.12%\n",
            "\n",
            "======================================================================\n",
            "   4-WAY ABLATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Config               Pooling  Mask   Orig     Trans    Drop     Retention \n",
            "-------------------------------------------------------------------------------------\n",
            "GAP_NoMask           GAP      No     0.9084   0.8191   0.0892   90.18     %\n",
            "GAP_WithMask         GAP      Yes    0.9111   0.9103   0.0008   99.91     %\n",
            "TPA_NoMask           TPA      No     0.9515   0.9022   0.0493   94.82     %\n",
            "TPA_WithMask         TPA      Yes    0.9301   0.9219   0.0082   99.12     %\n",
            "\n",
            "======================================================================\n",
            "   EFFECT ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "1️⃣  MASK EFFECT (in GAP):\n",
            "   GAP_NoMask  → GAP_WithMask\n",
            "   Drop: 0.0892 → 0.0008\n",
            "   Improvement: +99.11% drop reduction\n",
            "   Conclusion: Mask helps GAP by filtering transitions\n",
            "\n",
            "2️⃣  TPA EFFECT (without mask):\n",
            "   GAP_NoMask  → TPA_NoMask\n",
            "   Drop: 0.0892 → 0.0493\n",
            "   Improvement: +44.80% drop reduction\n",
            "   Conclusion: TPA alone helps via prototype attention\n",
            "\n",
            "3️⃣  COMBINED EFFECT (TPA + Mask):\n",
            "   GAP_NoMask  → TPA_WithMask\n",
            "   Drop: 0.0892 → 0.0082\n",
            "   Improvement: +90.85% drop reduction\n",
            "   Retention gain: +8.95pp\n",
            "\n",
            "4️⃣  SYNERGY ANALYSIS:\n",
            "   Expected (additive): -0.0392\n",
            "   Actual (TPA+Mask):   0.0082\n",
            "   Synergy: -0.0473 (-53.05%)\n",
            "   ✗ Negative synergy: Interference between TPA and Mask\n",
            "\n",
            "======================================================================\n",
            "   BEST CONFIGURATION: GAP_WithMask\n",
            "======================================================================\n",
            "   Pooling: GAP\n",
            "   Mask: Yes\n",
            "   Original Acc: 0.9111\n",
            "   Transition Acc: 0.9103\n",
            "   Drop: 0.0008\n",
            "   Retention: 99.91%\n",
            "\n",
            "✓ Results saved to '/content/drive/MyDrive/AI_data/ablation_4way/ablation_4way_results.json'\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}