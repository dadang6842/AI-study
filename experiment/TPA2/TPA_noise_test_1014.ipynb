{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r9TipbUZnJS",
        "outputId": "3a14cf6c-f535-4dd3-f6a9-9ec83d18640f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSydsSkhXMHw",
        "outputId": "e8096875-aefb-4f6f-9f6e-fb60295314af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "    NOISE ROBUSTNESS STUDY\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "Training Augmentation: prob=0.7, max_noise=0.4\n",
            "\n",
            "3 Models to Compare:\n",
            "  1) GAP_Baseline:  Standard GAP (baseline)\n",
            "  2) TPA:           Temporal Prototype Attention\n",
            "  3) TPA_WithMask:  TPA + mask filtering\n",
            "\n",
            "7 Noise Scenarios:\n",
            "  - Clean (no noise)\n",
            "  - Gaussian noise (20%, 40%)\n",
            "  - Temporal mask (30%)\n",
            "  - Channel drift (20%)\n",
            "  - Channel drop (20%)\n",
            "  - Spike noise (5%)\n",
            "\n",
            "TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total samples: 7352\n",
            "  → Train: 5881 (80%)\n",
            "  → Val:   1471 (20%)\n",
            "\n",
            "======================================================================\n",
            "    NOISE ROBUSTNESS STUDY: 3-Model Comparison\n",
            "======================================================================\n",
            "   Models: GAP (Baseline), TPA, TPA+Mask\n",
            "   Training: Mixed noise augmentation (prob=0.7, max=0.4)\n",
            "   Testing: 7 noise scenarios\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   MODEL: GAP_Baseline\n",
            "======================================================================\n",
            "\n",
            "Training GAP_Baseline for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4717 A:0.5215 | Val A:0.8192 F1:0.8030 ✓\n",
            "[10/100] Train L:0.4458 A:0.9299 | Val A:0.9409 F1:0.9444\n",
            "[20/100] Train L:0.3956 A:0.9366 | Val A:0.9483 F1:0.9521\n",
            "[30/100] Train L:0.3797 A:0.9424 | Val A:0.9545 F1:0.9577\n",
            "[40/100] Train L:0.3738 A:0.9413 | Val A:0.9545 F1:0.9577\n",
            "[50/100] Train L:0.3677 A:0.9459 | Val A:0.9599 F1:0.9630\n",
            "\n",
            "⚠ Early stopping triggered at epoch 54\n",
            "  Best validation acc: 0.9613 (epoch 34)\n",
            "\n",
            "✓ Best Val Acc: 0.9613 (epoch 34)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "Noise Type                | Accuracy |     F1\n",
            "--------------------------------------------------\n",
            "Clean                     |   91.79% | 0.919\n",
            "20% Gaussian              |   91.79% | 0.919\n",
            "40% Gaussian              |   91.69% | 0.918\n",
            "30% Temporal Mask         |   91.82% | 0.920\n",
            "20% Channel Drift         |   91.55% | 0.917\n",
            "20% Channel Drop          |   87.24% | 0.875\n",
            "5% Spike Noise            |   91.58% | 0.917\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   MODEL: TPA\n",
            "======================================================================\n",
            "\n",
            "Training TPA for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4678 A:0.5372 | Val A:0.6798 F1:0.6075 ✓\n",
            "[10/100] Train L:0.3904 A:0.9388 | Val A:0.9463 F1:0.9497\n",
            "[20/100] Train L:0.3653 A:0.9437 | Val A:0.9558 F1:0.9590\n",
            "[30/100] Train L:0.3510 A:0.9481 | Val A:0.9613 F1:0.9642\n",
            "[40/100] Train L:0.3453 A:0.9502 | Val A:0.9626 F1:0.9655\n",
            "[50/100] Train L:0.3372 A:0.9553 | Val A:0.9701 F1:0.9724\n",
            "[60/100] Train L:0.3328 A:0.9549 | Val A:0.9694 F1:0.9718\n",
            "[70/100] Train L:0.3246 A:0.9614 | Val A:0.9755 F1:0.9774\n",
            "[80/100] Train L:0.3237 A:0.9629 | Val A:0.9646 F1:0.9674\n",
            "[90/100] Train L:0.3272 A:0.9599 | Val A:0.9789 F1:0.9806\n",
            "\n",
            "⚠ Early stopping triggered at epoch 97\n",
            "  Best validation acc: 0.9823 (epoch 77)\n",
            "\n",
            "✓ Best Val Acc: 0.9823 (epoch 77)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "Noise Type                | Accuracy |     F1\n",
            "--------------------------------------------------\n",
            "Clean                     |   92.91% | 0.928\n",
            "20% Gaussian              |   91.75% | 0.916\n",
            "40% Gaussian              |   91.55% | 0.915\n",
            "30% Temporal Mask         |   92.57% | 0.925\n",
            "20% Channel Drift         |   92.67% | 0.926\n",
            "20% Channel Drop          |   89.96% | 0.900\n",
            "5% Spike Noise            |   91.52% | 0.915\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   MODEL: TPA_WithMask\n",
            "======================================================================\n",
            "\n",
            "Training TPA_WithMask for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4618 A:0.5421 | Val A:0.6832 F1:0.6134 ✓\n",
            "[10/100] Train L:0.3937 A:0.9384 | Val A:0.9436 F1:0.9471\n",
            "[20/100] Train L:0.3689 A:0.9424 | Val A:0.9551 F1:0.9583\n",
            "[30/100] Train L:0.3542 A:0.9492 | Val A:0.9626 F1:0.9654\n",
            "[40/100] Train L:0.3481 A:0.9485 | Val A:0.9633 F1:0.9661\n",
            "[50/100] Train L:0.3398 A:0.9556 | Val A:0.9708 F1:0.9730 ✓\n",
            "[60/100] Train L:0.3333 A:0.9563 | Val A:0.9735 F1:0.9756\n",
            "[70/100] Train L:0.3274 A:0.9597 | Val A:0.9782 F1:0.9798 ✓\n",
            "[80/100] Train L:0.3286 A:0.9592 | Val A:0.9633 F1:0.9661\n",
            "[90/100] Train L:0.3252 A:0.9611 | Val A:0.9769 F1:0.9787\n",
            "[100/100] Train L:0.3155 A:0.9624 | Val A:0.9823 F1:0.9837\n",
            "\n",
            "✓ Best Val Acc: 0.9844 (epoch 97)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "Noise Type                | Accuracy |     F1\n",
            "--------------------------------------------------\n",
            "Clean                     |   94.47% | 0.944\n",
            "20% Gaussian              |   92.74% | 0.927\n",
            "40% Gaussian              |   91.89% | 0.919\n",
            "30% Temporal Mask         |   93.96% | 0.939\n",
            "20% Channel Drift         |   93.35% | 0.933\n",
            "20% Channel Drop          |   91.18% | 0.912\n",
            "5% Spike Noise            |   92.06% | 0.921\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   COMPREHENSIVE ROBUSTNESS ANALYSIS\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   ROBUSTNESS ANALYSIS: Performance Drop from Clean Baseline\n",
            "====================================================================================================\n",
            "\n",
            "📊 ROBUSTNESS RANKING (Lower drop = Better robustness)\n",
            "\n",
            "Rank   | Model                |    Clean |   Avg Drop |   Max Drop |  Retention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1      | GAP_Baseline         |   91.79% |      0.84% |      4.55% |     99.08%\n",
            "2      | TPA                  |   92.91% |      1.24% |      2.95% |     98.67%\n",
            "3      | TPA_WithMask         |   94.47% |      1.94% |      3.29% |     97.95%\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   DETAILED DROP ANALYSIS BY NOISE TYPE\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "🔍 20% Gaussian\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     91.79% |      0.00% |        0.00% |      100.00% | 🟢 Excellent\n",
            "TPA                  |     91.75% |      1.15% |        1.24% |       98.76% | 🟢 Excellent\n",
            "TPA_WithMask         |     92.74% |      1.73% |        1.83% |       98.17% | 🟢 Excellent\n",
            "\n",
            "🔍 40% Gaussian\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     91.69% |      0.10% |        0.11% |       99.89% | 🟢 Excellent\n",
            "TPA                  |     91.55% |      1.36% |        1.46% |       98.54% | 🟢 Excellent\n",
            "TPA_WithMask         |     91.89% |      2.58% |        2.73% |       97.27% | 🟢 Excellent\n",
            "\n",
            "🔍 30% Temporal Mask\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     91.82% |     -0.03% |       -0.04% |      100.04% | 🟢 Excellent\n",
            "TPA                  |     92.57% |      0.34% |        0.37% |       99.63% | 🟢 Excellent\n",
            "TPA_WithMask         |     93.96% |      0.51% |        0.54% |       99.46% | 🟢 Excellent\n",
            "\n",
            "🔍 20% Channel Drift\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     91.55% |      0.24% |        0.26% |       99.74% | 🟢 Excellent\n",
            "TPA                  |     92.67% |      0.24% |        0.26% |       99.74% | 🟢 Excellent\n",
            "TPA_WithMask         |     93.35% |      1.12% |        1.19% |       98.81% | 🟢 Excellent\n",
            "\n",
            "🔍 20% Channel Drop\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     87.24% |      4.55% |        4.95% |       95.05% | 🟢 Excellent\n",
            "TPA                  |     89.96% |      2.95% |        3.18% |       96.82% | 🟢 Excellent\n",
            "TPA_WithMask         |     91.18% |      3.29% |        3.48% |       96.52% | 🟢 Excellent\n",
            "\n",
            "🔍 5% Spike Noise\n",
            "Model                |   Accuracy |       Drop |   Drop Ratio |    Retention | Grade          \n",
            "----------------------------------------------------------------------------------------------------\n",
            "GAP_Baseline         |     91.58% |      0.20% |        0.22% |       99.78% | 🟢 Excellent\n",
            "TPA                  |     91.52% |      1.39% |        1.50% |       98.50% | 🟢 Excellent\n",
            "TPA_WithMask         |     92.06% |      2.41% |        2.55% |       97.45% | 🟢 Excellent\n",
            "\n",
            "====================================================================================================\n",
            "   PERFORMANCE DROP (%) MATRIX - Lower is Better\n",
            "====================================================================================================\n",
            "\n",
            "              20% Gaussian  40% Gaussian  30% Temporal Mask  20% Channel Drift  20% Channel Drop  5% Spike Noise\n",
            "Model                                                                                                           \n",
            "GAP_Baseline          0.00          0.10              -0.03               0.24              4.55            0.20\n",
            "TPA                   1.15          1.36               0.34               0.24              2.95            1.39\n",
            "TPA_WithMask          1.73          2.58               0.51               1.12              3.29            2.41\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   COMPARATIVE ROBUSTNESS IMPROVEMENT\n",
            "====================================================================================================\n",
            "\n",
            "Baseline Model: GAP_Baseline\n",
            "  - Clean Accuracy: 91.79%\n",
            "  - Average Drop: 0.84%\n",
            "  - Average Retention: 99.08%\n",
            "\n",
            "Improvements over Baseline:\n",
            "\n",
            "Model                |  Drop Reduction |  Retention Gain |   Robustness Score\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TPA                  |          -0.40% |          -0.42% |             96.94\n",
            "TPA_WithMask         |          -1.10% |          -1.14% |             96.90\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   NOISE TYPE VULNERABILITY ANALYSIS\n",
            "====================================================================================================\n",
            "\n",
            "Most Challenging Noise Types (Highest Average Drop):\n",
            "\n",
            "Rank   | Noise Type                |   Avg Drop |   Max Drop |   Min Drop\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1      | 20% Channel Drop          |      3.60% |      4.55% |      2.95%\n",
            "2      | 40% Gaussian              |      1.35% |      2.58% |      0.10%\n",
            "3      | 5% Spike Noise            |      1.33% |      2.41% |      0.20%\n",
            "4      | 20% Gaussian              |      0.96% |      1.73% |      0.00%\n",
            "5      | 20% Channel Drift         |      0.53% |      1.12% |      0.24%\n",
            "6      | 30% Temporal Mask         |      0.27% |      0.51% |     -0.03%\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   MODEL-SPECIFIC STRENGTHS & WEAKNESSES\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "📌 GAP_Baseline\n",
            "  Overall:\n",
            "    - Clean Accuracy: 91.79%\n",
            "    - Average Drop: 0.84%\n",
            "    - Average Retention: 99.08%\n",
            "  Strongest Against: 30% Temporal Mask (Drop: -0.03%)\n",
            "  Weakest Against: 20% Channel Drop (Drop: 4.55%)\n",
            "\n",
            "📌 TPA\n",
            "  Overall:\n",
            "    - Clean Accuracy: 92.91%\n",
            "    - Average Drop: 1.24%\n",
            "    - Average Retention: 98.67%\n",
            "  Strongest Against: 20% Channel Drift (Drop: 0.24%)\n",
            "  Weakest Against: 20% Channel Drop (Drop: 2.95%)\n",
            "\n",
            "📌 TPA_WithMask\n",
            "  Overall:\n",
            "    - Clean Accuracy: 94.47%\n",
            "    - Average Drop: 1.94%\n",
            "    - Average Retention: 97.95%\n",
            "  Strongest Against: 30% Temporal Mask (Drop: 0.51%)\n",
            "  Weakest Against: 20% Channel Drop (Drop: 3.29%)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   DROP PERCENTAGE HEATMAP\n",
            "====================================================================================================\n",
            "\n",
            "Noise Type                |         GAP_Baseline |                  TPA |         TPA_WithMask | \n",
            "----------------------------------------------------------------------------------------------------\n",
            "20% Gaussian              |   0.00% (  0.0%) 🟢 |   1.15% (  1.2%) 🟡 |   1.73% (  1.8%) 🟡 | \n",
            "40% Gaussian              |   0.10% (  0.1%) 🟢 |   1.36% (  1.5%) 🟡 |   2.58% (  2.7%) 🟡 | \n",
            "30% Temporal Mask         |  -0.03% ( -0.0%) 🟢 |   0.34% (  0.4%) 🟢 |   0.51% (  0.5%) 🟢 | \n",
            "20% Channel Drift         |   0.24% (  0.3%) 🟢 |   0.24% (  0.3%) 🟢 |   1.12% (  1.2%) 🟡 | \n",
            "20% Channel Drop          |   4.55% (  5.0%) 🟠 |   2.95% (  3.2%) 🟡 |   3.29% (  3.5%) 🟠 | \n",
            "5% Spike Noise            |   0.20% (  0.2%) 🟢 |   1.39% (  1.5%) 🟡 |   2.41% (  2.6%) 🟡 | \n",
            "\n",
            "Legend: 🟢 <1% | 🟡 1-3% | 🟠 3-5% | 🔴 >5%\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   ACCURACY (%) REFERENCE TABLE\n",
            "====================================================================================================\n",
            "\n",
            "Model              GAP_Baseline   TPA  TPA_WithMask\n",
            "Noise Type                                         \n",
            "20% Channel Drift         91.55 92.67         93.35\n",
            "20% Channel Drop          87.24 89.96         91.18\n",
            "20% Gaussian              91.79 91.75         92.74\n",
            "30% Temporal Mask         91.82 92.57         93.96\n",
            "40% Gaussian              91.69 91.55         91.89\n",
            "5% Spike Noise            91.58 91.52         92.06\n",
            "Clean                     91.79 92.91         94.47\n",
            "\n",
            "✓ Robustness reports saved to:\n",
            "  - /content/drive/MyDrive/AI_data/noise_robustness_study/robustness_detailed.csv\n",
            "  - /content/drive/MyDrive/AI_data/noise_robustness_study/robustness_summary.json\n",
            "\n",
            "====================================================================================================\n",
            "   KEY FINDINGS SUMMARY\n",
            "====================================================================================================\n",
            "\n",
            "🏆 Most Robust Model: GAP_Baseline\n",
            "   - Average Drop: 0.84%\n",
            "   - Average Retention: 99.08%\n",
            "   - Clean Accuracy: 91.79%\n",
            "\n",
            "⚠️  Least Robust Model: TPA_WithMask\n",
            "   - Average Drop: 1.94%\n",
            "   - Average Retention: 97.95%\n",
            "   - Clean Accuracy: 94.47%\n",
            "\n",
            "📈 Improvements over Baseline:\n",
            "   TPA: -0.40% drop reduction\n",
            "   TPA_WithMask: -1.10% drop reduction\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "✓ Visualization saved to: /content/drive/MyDrive/AI_data/noise_robustness_study/noise_robustness_comparison.png\n",
            "\n",
            "======================================================================\n",
            "STUDY COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "Generated files:\n",
            "  1. /content/drive/MyDrive/AI_data/noise_robustness_study/noise_robustness_results.csv\n",
            "  2. /content/drive/MyDrive/AI_data/noise_robustness_study/robustness_detailed.csv\n",
            "  3. /content/drive/MyDrive/AI_data/noise_robustness_study/robustness_summary.json\n",
            "  4. /content/drive/MyDrive/AI_data/noise_robustness_study/noise_robustness_comparison.png\n",
            "  5. model_*.pth files for all 3 models\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TPA Noise Robustness Study: 3-Model Comparison with Drop-based Analysis\n",
        "Models: GAP (Baseline), TPA, TPA+Mask\n",
        "Noise Types: Gaussian, Temporal Mask, Channel Drift, Channel Drop, Spike\n",
        "\"\"\"\n",
        "\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/noise_robustness_study\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    # Training augmentation (mixed noise)\n",
        "    train_augment_prob: float = 0.7\n",
        "    max_train_noise: float = 0.4\n",
        "\n",
        "    d_model: int = 128\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_seg_kernel: int = 9\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) Noise Augmentation Functions with Masks\n",
        "# ========================\n",
        "\n",
        "def add_gaussian_noise(X, noise_level):\n",
        "    \"\"\"Add Gaussian noise - mask all valid (noise affects entire signal)\"\"\"\n",
        "    if noise_level == 0:\n",
        "        return X, np.ones((X.shape[0], X.shape[2]), dtype=bool)\n",
        "\n",
        "    noise = np.random.normal(0, noise_level, X.shape).astype(np.float32)\n",
        "    mask = np.ones((X.shape[0], X.shape[2]), dtype=bool)  # [B, T]\n",
        "    return X + noise, mask\n",
        "\n",
        "def add_temporal_mask(X, mask_ratio=0.3):\n",
        "    \"\"\"Add temporal masking - masked parts are False\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    mask = np.ones((B, T), dtype=bool)  # [B, T]\n",
        "\n",
        "    for i in range(B):\n",
        "        if np.random.rand() < 0.5:\n",
        "            mask_len = int(T * mask_ratio)\n",
        "            start = np.random.randint(0, T - mask_len)\n",
        "            X_aug[i, :, start:start+mask_len] = 0\n",
        "            mask[i, start:start+mask_len] = False  # Mark corrupted region\n",
        "\n",
        "    return X_aug, mask\n",
        "\n",
        "def add_channel_drift(X, drift_std=0.2):\n",
        "    \"\"\"Channel drift - mask all valid\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    mask = np.ones((B, T), dtype=bool)\n",
        "\n",
        "    for i in range(B):\n",
        "        for c in range(C):\n",
        "            if np.random.rand() < 0.3:\n",
        "                drift = np.random.normal(0, drift_std)\n",
        "                X_aug[i, c, :] += drift\n",
        "\n",
        "    return X_aug, mask\n",
        "\n",
        "def add_channel_drop(X, drop_prob=0.2):\n",
        "    \"\"\"Channel drop - time axis mask remains valid\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    mask = np.ones((B, T), dtype=bool)\n",
        "\n",
        "    for i in range(B):\n",
        "        drop_mask = np.random.rand(C) < drop_prob\n",
        "        X_aug[i, drop_mask, :] = 0\n",
        "\n",
        "    return X_aug, mask\n",
        "\n",
        "def add_spike_noise(X, spike_prob=0.05, spike_magnitude=2.0):\n",
        "    \"\"\"Spike noise - mark spike locations in mask\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "\n",
        "    spike_mask_3d = np.random.rand(*X.shape) < spike_prob  # [B, C, T]\n",
        "    spikes = np.random.randn(*X.shape) * spike_magnitude\n",
        "    X_aug[spike_mask_3d] += spikes[spike_mask_3d]\n",
        "\n",
        "    # Reduce to time axis: if any channel has spike, mark as False\n",
        "    mask = ~(spike_mask_3d.any(axis=1))  # [B, T]\n",
        "\n",
        "    return X_aug, mask\n",
        "\n",
        "def apply_mixed_augmentation(X, max_noise=0.4):\n",
        "    \"\"\"Apply random combination of augmentations with mask tracking\"\"\"\n",
        "    aug_type = np.random.choice([\n",
        "        'gaussian', 'temporal', 'drift', 'drop', 'spike', 'mixed'\n",
        "    ])\n",
        "\n",
        "    if aug_type == 'gaussian':\n",
        "        noise_level = np.random.uniform(0, max_noise)\n",
        "        return add_gaussian_noise(X, noise_level)\n",
        "\n",
        "    elif aug_type == 'temporal':\n",
        "        mask_ratio = np.random.uniform(0.1, 0.4)\n",
        "        return add_temporal_mask(X, mask_ratio)\n",
        "\n",
        "    elif aug_type == 'drift':\n",
        "        drift_std = np.random.uniform(0.1, 0.3)\n",
        "        return add_channel_drift(X, drift_std)\n",
        "\n",
        "    elif aug_type == 'drop':\n",
        "        drop_prob = np.random.uniform(0.1, 0.3)\n",
        "        return add_channel_drop(X, drop_prob)\n",
        "\n",
        "    elif aug_type == 'spike':\n",
        "        spike_prob = np.random.uniform(0.02, 0.08)\n",
        "        return add_spike_noise(X, spike_prob)\n",
        "\n",
        "    else:  # mixed\n",
        "        X_aug = X.copy()\n",
        "        B, T = X.shape[0], X.shape[2]\n",
        "        mask = np.ones((B, T), dtype=bool)\n",
        "\n",
        "        num_augs = np.random.randint(2, 4)\n",
        "        augs = np.random.choice(\n",
        "            ['gaussian', 'temporal', 'drift', 'spike'],\n",
        "            size=num_augs, replace=False\n",
        "        )\n",
        "\n",
        "        for aug in augs:\n",
        "            if aug == 'gaussian':\n",
        "                X_aug, m = add_gaussian_noise(X_aug, np.random.uniform(0, max_noise*0.5))\n",
        "            elif aug == 'temporal':\n",
        "                X_aug, m = add_temporal_mask(X_aug, np.random.uniform(0.1, 0.2))\n",
        "            elif aug == 'drift':\n",
        "                X_aug, m = add_channel_drift(X_aug, np.random.uniform(0.05, 0.15))\n",
        "            elif aug == 'spike':\n",
        "                X_aug, m = add_spike_noise(X_aug, np.random.uniform(0.02, 0.05))\n",
        "\n",
        "            mask = mask & m  # AND operation: only valid in all augmentations\n",
        "\n",
        "        return X_aug, mask\n",
        "\n",
        "# ========================\n",
        "# 2) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/total_acc_z_\", \"txt\"), (\"Inertial Signals/body_acc_x_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None] for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None,\n",
        "                 augment: bool = False, max_noise: float = 0.4, augment_prob: float = 0.7):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "        self.augment = augment\n",
        "        self.max_noise = max_noise\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].copy()\n",
        "        y = self.y[idx]\n",
        "\n",
        "        # Generate mask\n",
        "        T = x.shape[1]  # Time dimension\n",
        "        mask = np.ones(T, dtype=bool)  # Default: all valid\n",
        "\n",
        "        if self.augment and np.random.rand() < self.augment_prob:\n",
        "            # Apply augmentation and get mask\n",
        "            x_aug, mask_aug = apply_mixed_augmentation(\n",
        "                x[np.newaxis], self.max_noise\n",
        "            )\n",
        "            x = x_aug[0]\n",
        "            mask = mask_aug[0]  # [T]\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(x).float(),\n",
        "            torch.tensor(y, dtype=torch.long),\n",
        "            torch.from_numpy(mask).bool()\n",
        "        )\n",
        "\n",
        "class UCIHARInertialWithMask(Dataset):\n",
        "    \"\"\"Dataset with pre-computed masks for testing\"\"\"\n",
        "    def __init__(self, X, y, masks):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "        self.masks = torch.BoolTensor(masks)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.masks[idx]\n",
        "\n",
        "# ========================\n",
        "# 3) TPA Module\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Temporal Prototype Attention with optional mask support\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        pad = (seg_kernel - 1) // 2\n",
        "        self.lowpass = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim, bias=False)\n",
        "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
        "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, return_confidence=False, use_mask=True):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        mask: [B, T] - True for valid frames\n",
        "        use_mask: whether to use mask (for ablation study)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_filtered = self.lowpass(x.transpose(1, 2)).transpose(1, 2)\n",
        "        xloc = self.pw(self.dw(x_filtered.transpose(1, 2))).transpose(1, 2)\n",
        "        xloc = self.pre_norm(xloc) + x\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            float_mask = mask.float()\n",
        "            float_mask_expanded = float_mask.unsqueeze(-1)\n",
        "            xloc = xloc * float_mask_expanded\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            boolean_mask = mask.bool()\n",
        "            mask_attn = boolean_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(~mask_attn, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            mask_expanded = float_mask.unsqueeze(-1).float()\n",
        "            z_gap = (x * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 4) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1)) for k in branches])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        pooled = features.mean(dim=1)\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "class TPAHead(nn.Module):\n",
        "    \"\"\"TPA: Temporal Prototype Attention (with optional mask support)\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, seg_kernel: int = 9,\n",
        "                 heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25,\n",
        "                 use_mask: bool = False):\n",
        "        super().__init__()\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=num_prototypes,\n",
        "            seg_kernel=seg_kernel,\n",
        "            heads=heads,\n",
        "            dropout=dropout,\n",
        "            temperature=temperature,\n",
        "            topk_ratio=topk_ratio\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap, mask: torch.BoolTensor | None = None):\n",
        "        \"\"\"\n",
        "        Fmap: [B, D, T]\n",
        "        mask: [B, T]\n",
        "        \"\"\"\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        z = self.tpa(features, mask=mask, return_confidence=False, use_mask=self.use_mask)\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, model_type='gap', use_mask=False, tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.model_type = model_type\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        if model_type == 'gap':\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "        else:  # tpa\n",
        "            self.head = TPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                seg_kernel=tpa_config.get('seg_kernel', 9),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25),\n",
        "                use_mask=use_mask\n",
        "            )\n",
        "\n",
        "    def forward(self, x, mask: torch.BoolTensor | None = None):\n",
        "        fmap = self.backbone(x)\n",
        "\n",
        "        if self.model_type == 'tpa' and mask is not None:\n",
        "            stride = self.backbone.stride\n",
        "            mask_float = mask.float().unsqueeze(1)\n",
        "            mask_down = (F.avg_pool1d(mask_float, kernel_size=stride, stride=stride) == 1.0).squeeze(1)\n",
        "            return self.head(fmap, mask_down)\n",
        "        else:\n",
        "            return self.head(fmap)\n",
        "\n",
        "# ========================\n",
        "# 5) Train / Eval\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for x, y, mask in loader:\n",
        "        x = x.to(cfg.device).float()\n",
        "        y = y.to(cfg.device)\n",
        "        mask = mask.to(cfg.device).bool()\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(x, mask=mask)\n",
        "\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config, classes=6):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y, mask in loader:\n",
        "        x = x.to(cfg.device)\n",
        "        y = y.to(cfg.device)\n",
        "        mask = mask.to(cfg.device).bool()\n",
        "\n",
        "        logits = model(x, mask=mask)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "# ========================\n",
        "# 6) Robustness Analysis Functions\n",
        "# ========================\n",
        "\n",
        "def calculate_robustness_metrics(df_results):\n",
        "    \"\"\"Calculate robustness metrics based on performance drop\"\"\"\n",
        "    robustness_metrics = []\n",
        "\n",
        "    for model in df_results['Model'].unique():\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "\n",
        "        # Clean accuracy (baseline)\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "\n",
        "        # Calculate drops for each noise type\n",
        "        noise_drops = []\n",
        "        for _, row in model_data.iterrows():\n",
        "            if row['Noise Type'] != 'Clean':\n",
        "                drop = clean_acc - row['Accuracy']\n",
        "                drop_ratio = (drop / clean_acc) * 100  # Drop percentage\n",
        "                noise_drops.append({\n",
        "                    'Noise Type': row['Noise Type'],\n",
        "                    'Accuracy': row['Accuracy'],\n",
        "                    'Drop (%)': drop,\n",
        "                    'Drop Ratio (%)': drop_ratio,\n",
        "                    'Retention (%)': 100 - drop_ratio\n",
        "                })\n",
        "\n",
        "        noise_drops_df = pd.DataFrame(noise_drops)\n",
        "\n",
        "        robustness_metrics.append({\n",
        "            'Model': model,\n",
        "            'Clean Acc': clean_acc,\n",
        "            'Avg Drop (%)': noise_drops_df['Drop (%)'].mean(),\n",
        "            'Max Drop (%)': noise_drops_df['Drop (%)'].max(),\n",
        "            'Min Drop (%)': noise_drops_df['Drop (%)'].min(),\n",
        "            'Std Drop (%)': noise_drops_df['Drop (%)'].std(),\n",
        "            'Avg Drop Ratio (%)': noise_drops_df['Drop Ratio (%)'].mean(),\n",
        "            'Avg Retention (%)': noise_drops_df['Retention (%)'].mean(),\n",
        "            'Worst Retention (%)': noise_drops_df['Retention (%)'].min(),\n",
        "            'noise_details': noise_drops\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(robustness_metrics)\n",
        "\n",
        "def print_robustness_comparison(df_results):\n",
        "    \"\"\"Print comprehensive robustness comparison based on drop metrics\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   ROBUSTNESS ANALYSIS: Performance Drop from Clean Baseline\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    robustness_df = calculate_robustness_metrics(df_results)\n",
        "\n",
        "    # 1. Overall Robustness Ranking\n",
        "    print(\"📊 ROBUSTNESS RANKING (Lower drop = Better robustness)\\n\")\n",
        "    print(f\"{'Rank':<6} | {'Model':<20} | {'Clean':>8} | {'Avg Drop':>10} | {'Max Drop':>10} | {'Retention':>10}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    ranked = robustness_df.sort_values('Avg Drop (%)')\n",
        "    for idx, row in ranked.iterrows():\n",
        "        rank = ranked.index.get_loc(idx) + 1\n",
        "        print(f\"{rank:<6} | {row['Model']:<20} | {row['Clean Acc']:7.2f}% | \"\n",
        "              f\"{row['Avg Drop (%)']:9.2f}% | {row['Max Drop (%)']:9.2f}% | {row['Avg Retention (%)']:9.2f}%\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 2. Detailed Drop by Noise Type\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   DETAILED DROP ANALYSIS BY NOISE TYPE\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    models = df_results['Model'].unique()\n",
        "    noise_types = [nt for nt in df_results['Noise Type'].unique() if nt != 'Clean']\n",
        "\n",
        "    for noise_type in noise_types:\n",
        "        print(f\"\\n🔍 {noise_type}\")\n",
        "        print(f\"{'Model':<20} | {'Accuracy':>10} | {'Drop':>10} | {'Drop Ratio':>12} | {'Retention':>12} | {'Grade':<15}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        noise_data = df_results[df_results['Noise Type'] == noise_type].copy()\n",
        "\n",
        "        for model in models:\n",
        "            model_row = df_results[df_results['Model'] == model]\n",
        "            clean_acc = model_row[model_row['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "            noise_row = noise_data[noise_data['Model'] == model]\n",
        "\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                drop = clean_acc - acc\n",
        "                drop_ratio = (drop / clean_acc) * 100\n",
        "                retention = 100 - drop_ratio\n",
        "\n",
        "                # Grade based on retention\n",
        "                if retention >= 95:\n",
        "                    grade = \"🟢 Excellent\"\n",
        "                elif retention >= 90:\n",
        "                    grade = \"🟡 Good\"\n",
        "                elif retention >= 85:\n",
        "                    grade = \"🟠 Fair\"\n",
        "                elif retention >= 80:\n",
        "                    grade = \"🔴 Poor\"\n",
        "                else:\n",
        "                    grade = \"⚫ Very Poor\"\n",
        "\n",
        "                print(f\"{model:<20} | {acc:9.2f}% | {drop:9.2f}% | {drop_ratio:11.2f}% | \"\n",
        "                      f\"{retention:11.2f}% | {grade}\")\n",
        "\n",
        "    # 3. Robustness Comparison Matrix\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   PERFORMANCE DROP (%) MATRIX - Lower is Better\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    drop_matrix = []\n",
        "    for model in models:\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "\n",
        "        row_data = {'Model': model}\n",
        "        for noise_type in noise_types:\n",
        "            noise_row = model_data[model_data['Noise Type'] == noise_type]\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                drop = clean_acc - acc\n",
        "                row_data[noise_type] = drop\n",
        "\n",
        "        drop_matrix.append(row_data)\n",
        "\n",
        "    drop_df = pd.DataFrame(drop_matrix).set_index('Model')\n",
        "    print(drop_df.to_string(float_format=\"%.2f\"))\n",
        "    print()\n",
        "\n",
        "    # 4. Comparative Robustness Analysis\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   COMPARATIVE ROBUSTNESS IMPROVEMENT\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    baseline_model = 'GAP_Baseline'\n",
        "    baseline_metrics = robustness_df[robustness_df['Model'] == baseline_model].iloc[0]\n",
        "\n",
        "    print(f\"Baseline Model: {baseline_model}\")\n",
        "    print(f\"  - Clean Accuracy: {baseline_metrics['Clean Acc']:.2f}%\")\n",
        "    print(f\"  - Average Drop: {baseline_metrics['Avg Drop (%)']:.2f}%\")\n",
        "    print(f\"  - Average Retention: {baseline_metrics['Avg Retention (%)']:.2f}%\\n\")\n",
        "\n",
        "    print(\"Improvements over Baseline:\\n\")\n",
        "    print(f\"{'Model':<20} | {'Drop Reduction':>15} | {'Retention Gain':>15} | {'Robustness Score':>18}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for _, row in robustness_df.iterrows():\n",
        "        if row['Model'] != baseline_model:\n",
        "            drop_reduction = baseline_metrics['Avg Drop (%)'] - row['Avg Drop (%)']\n",
        "            retention_gain = row['Avg Retention (%)'] - baseline_metrics['Avg Retention (%)']\n",
        "\n",
        "            # Robustness Score: combination of clean acc and retention\n",
        "            robustness_score = (row['Clean Acc'] * 0.3 + row['Avg Retention (%)'] * 0.7)\n",
        "\n",
        "            print(f\"{row['Model']:<20} | {drop_reduction:+14.2f}% | {retention_gain:+14.2f}% | {robustness_score:17.2f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 5. Noise Type Vulnerability Analysis\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   NOISE TYPE VULNERABILITY ANALYSIS\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    print(\"Most Challenging Noise Types (Highest Average Drop):\\n\")\n",
        "\n",
        "    noise_difficulty = []\n",
        "    for noise_type in noise_types:\n",
        "        noise_data = df_results[df_results['Noise Type'] == noise_type]\n",
        "        avg_drops = []\n",
        "\n",
        "        for model in models:\n",
        "            model_row = df_results[df_results['Model'] == model]\n",
        "            clean_acc = model_row[model_row['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "            noise_row = noise_data[noise_data['Model'] == model]\n",
        "\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                drop = clean_acc - acc\n",
        "                avg_drops.append(drop)\n",
        "\n",
        "        noise_difficulty.append({\n",
        "            'Noise Type': noise_type,\n",
        "            'Avg Drop Across Models': np.mean(avg_drops),\n",
        "            'Max Drop': np.max(avg_drops),\n",
        "            'Min Drop': np.min(avg_drops)\n",
        "        })\n",
        "\n",
        "    difficulty_df = pd.DataFrame(noise_difficulty).sort_values('Avg Drop Across Models', ascending=False)\n",
        "\n",
        "    print(f\"{'Rank':<6} | {'Noise Type':<25} | {'Avg Drop':>10} | {'Max Drop':>10} | {'Min Drop':>10}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for idx, row in difficulty_df.iterrows():\n",
        "        rank = difficulty_df.index.get_loc(idx) + 1\n",
        "        print(f\"{rank:<6} | {row['Noise Type']:<25} | {row['Avg Drop Across Models']:9.2f}% | \"\n",
        "              f\"{row['Max Drop']:9.2f}% | {row['Min Drop']:9.2f}%\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # 6. Model-Specific Strengths\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   MODEL-SPECIFIC STRENGTHS & WEAKNESSES\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    for model in models:\n",
        "        print(f\"\\n📌 {model}\")\n",
        "        model_details = robustness_df[robustness_df['Model'] == model].iloc[0]\n",
        "        noise_details = pd.DataFrame(model_details['noise_details'])\n",
        "\n",
        "        # Best performance (lowest drop)\n",
        "        best_noise = noise_details.loc[noise_details['Drop (%)'].idxmin()]\n",
        "        worst_noise = noise_details.loc[noise_details['Drop (%)'].idxmax()]\n",
        "\n",
        "        print(f\"  Overall:\")\n",
        "        print(f\"    - Clean Accuracy: {model_details['Clean Acc']:.2f}%\")\n",
        "        print(f\"    - Average Drop: {model_details['Avg Drop (%)']:.2f}%\")\n",
        "        print(f\"    - Average Retention: {model_details['Avg Retention (%)']:.2f}%\")\n",
        "        print(f\"  Strongest Against: {best_noise['Noise Type']} (Drop: {best_noise['Drop (%)']:.2f}%)\")\n",
        "        print(f\"  Weakest Against: {worst_noise['Noise Type']} (Drop: {worst_noise['Drop (%)']:.2f}%)\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    return robustness_df\n",
        "\n",
        "def print_drop_heatmap(df_results):\n",
        "    \"\"\"Print drop percentage heatmap with color coding\"\"\"\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   DROP PERCENTAGE HEATMAP\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    models = df_results['Model'].unique()\n",
        "    noise_types = [nt for nt in df_results['Noise Type'].unique() if nt != 'Clean']\n",
        "\n",
        "    # Print header\n",
        "    print(f\"{'Noise Type':<25} | \", end=\"\")\n",
        "    for model in models:\n",
        "        print(f\"{model:>20} | \", end=\"\")\n",
        "    print()\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    # Print drops with visual indicators\n",
        "    for noise_type in noise_types:\n",
        "        print(f\"{noise_type:<25} | \", end=\"\")\n",
        "\n",
        "        for model in models:\n",
        "            model_row = df_results[df_results['Model'] == model]\n",
        "            clean_acc = model_row[model_row['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "            noise_row = model_row[model_row['Noise Type'] == noise_type]\n",
        "\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                drop = clean_acc - acc\n",
        "                drop_ratio = (drop / clean_acc) * 100\n",
        "\n",
        "                # Visual indicator\n",
        "                if drop < 1:\n",
        "                    indicator = \"🟢\"\n",
        "                elif drop < 3:\n",
        "                    indicator = \"🟡\"\n",
        "                elif drop < 5:\n",
        "                    indicator = \"🟠\"\n",
        "                else:\n",
        "                    indicator = \"🔴\"\n",
        "\n",
        "                print(f\"{drop:6.2f}% ({drop_ratio:5.1f}%) {indicator} | \", end=\"\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(\"\\nLegend: 🟢 <1% | 🟡 1-3% | 🟠 3-5% | 🔴 >5%\")\n",
        "    print()\n",
        "\n",
        "def save_robustness_report(df_results, robustness_df, save_dir):\n",
        "    \"\"\"Save comprehensive robustness report\"\"\"\n",
        "\n",
        "    # 1. Save detailed CSV\n",
        "    detailed_results = []\n",
        "\n",
        "    for model in df_results['Model'].unique():\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "\n",
        "        for _, row in model_data.iterrows():\n",
        "            if row['Noise Type'] != 'Clean':\n",
        "                drop = clean_acc - row['Accuracy']\n",
        "                drop_ratio = (drop / clean_acc) * 100\n",
        "                retention = 100 - drop_ratio\n",
        "\n",
        "                detailed_results.append({\n",
        "                    'Model': model,\n",
        "                    'Noise Type': row['Noise Type'],\n",
        "                    'Clean Accuracy': clean_acc,\n",
        "                    'Noisy Accuracy': row['Accuracy'],\n",
        "                    'Drop (%)': drop,\n",
        "                    'Drop Ratio (%)': drop_ratio,\n",
        "                    'Retention (%)': retention,\n",
        "                    'F1': row['F1']\n",
        "                })\n",
        "\n",
        "    detailed_df = pd.DataFrame(detailed_results)\n",
        "    detailed_df.to_csv(os.path.join(save_dir, 'robustness_detailed.csv'), index=False)\n",
        "\n",
        "    # 2. Save summary JSON\n",
        "    summary = {}\n",
        "    for _, row in robustness_df.iterrows():\n",
        "        summary[row['Model']] = {\n",
        "            'clean_accuracy': float(row['Clean Acc']),\n",
        "            'average_drop': float(row['Avg Drop (%)']),\n",
        "            'max_drop': float(row['Max Drop (%)']),\n",
        "            'min_drop': float(row['Min Drop (%)']),\n",
        "            'std_drop': float(row['Std Drop (%)']),\n",
        "            'average_retention': float(row['Avg Retention (%)']),\n",
        "            'worst_retention': float(row['Worst Retention (%)'])\n",
        "        }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'robustness_summary.json'), 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Robustness reports saved to:\")\n",
        "    print(f\"  - {os.path.join(save_dir, 'robustness_detailed.csv')}\")\n",
        "    print(f\"  - {os.path.join(save_dir, 'robustness_summary.json')}\")\n",
        "\n",
        "# ========================\n",
        "# 7) Noise Robustness Study\n",
        "# ========================\n",
        "def run_noise_robustness_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)\n",
        "\n",
        "    indices = np.arange(len(X_full))\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_full\n",
        "    )\n",
        "\n",
        "    print(f\"Total samples: {len(X_full)}\")\n",
        "    print(f\"  → Train: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices,\n",
        "        augment=True,\n",
        "        max_noise=cfg.max_train_noise,\n",
        "        augment_prob=cfg.train_augment_prob\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std, augment=False)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    # Define noise configurations for testing\n",
        "    noise_configs = [\n",
        "        {'name': 'Clean', 'type': 'none', 'level': 0.0},\n",
        "        {'name': '20% Gaussian', 'type': 'gaussian', 'level': 0.2},\n",
        "        {'name': '40% Gaussian', 'type': 'gaussian', 'level': 0.4},\n",
        "        {'name': '30% Temporal Mask', 'type': 'temporal', 'level': 0.3},\n",
        "        {'name': '20% Channel Drift', 'type': 'drift', 'level': 0.2},\n",
        "        {'name': '20% Channel Drop', 'type': 'drop', 'level': 0.2},\n",
        "        {'name': '5% Spike Noise', 'type': 'spike', 'level': 0.05},\n",
        "    ]\n",
        "\n",
        "    # Define 3 model configurations\n",
        "    model_configs = [\n",
        "        {\"name\": \"GAP_Baseline\", \"model_type\": \"gap\", \"use_mask\": False},\n",
        "        {\"name\": \"TPA\", \"model_type\": \"tpa\", \"use_mask\": False},\n",
        "        {\"name\": \"TPA_WithMask\", \"model_type\": \"tpa\", \"use_mask\": True},\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    NOISE ROBUSTNESS STUDY: 3-Model Comparison\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Models: GAP (Baseline), TPA, TPA+Mask\")\n",
        "    print(f\"   Training: Mixed noise augmentation (prob={cfg.train_augment_prob}, max={cfg.max_train_noise})\")\n",
        "    print(f\"   Testing: 7 noise scenarios\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_cfg in model_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   MODEL: {model_cfg['name']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'seg_kernel': cfg.tpa_seg_kernel,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            model_type=model_cfg[\"model_type\"],\n",
        "            use_mask=model_cfg[\"use_mask\"],\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True,\n",
        "                                 num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {model_cfg['name']} for up to {cfg.epochs} epochs (patience={cfg.patience})...\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "            val_acc, val_f1 = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "            log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "            if improved:\n",
        "                log_str += \" ✓\"\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == 1:\n",
        "                print(log_str)\n",
        "\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            model_path = os.path.join(cfg.save_dir, f\"model_{model_cfg['name']}.pth\")\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Test on all noise scenarios\n",
        "        print(f\"\\n   Testing on {len(noise_configs)} noise scenarios...\")\n",
        "        print(f\"\\n{'Noise Type':<25} | {'Accuracy':>8} | {'F1':>6}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for noise_cfg in noise_configs:\n",
        "            X_test = test_set_orig.X.copy()\n",
        "\n",
        "            # Apply noise and generate mask\n",
        "            if noise_cfg['type'] != 'none':\n",
        "                if noise_cfg['type'] == 'gaussian':\n",
        "                    X_test, test_masks = add_gaussian_noise(X_test, noise_cfg['level'])\n",
        "                elif noise_cfg['type'] == 'temporal':\n",
        "                    X_test, test_masks = add_temporal_mask(X_test, noise_cfg['level'])\n",
        "                elif noise_cfg['type'] == 'drift':\n",
        "                    X_test, test_masks = add_channel_drift(X_test, noise_cfg['level'])\n",
        "                elif noise_cfg['type'] == 'drop':\n",
        "                    X_test, test_masks = add_channel_drop(X_test, noise_cfg['level'])\n",
        "                elif noise_cfg['type'] == 'spike':\n",
        "                    X_test, test_masks = add_spike_noise(X_test, noise_cfg['level'])\n",
        "            else:\n",
        "                # Clean - all masks True\n",
        "                test_masks = np.ones((X_test.shape[0], X_test.shape[2]), dtype=bool)\n",
        "\n",
        "            # Create test dataset with masks\n",
        "            test_ds_noisy = UCIHARInertialWithMask(\n",
        "                X_test, test_set_orig.y, test_masks\n",
        "            )\n",
        "            test_loader_noisy = DataLoader(test_ds_noisy, cfg.batch_size,\n",
        "                                          num_workers=cfg.num_workers)\n",
        "\n",
        "            acc, f1 = evaluate(model, test_loader_noisy, cfg)\n",
        "\n",
        "            print(f\"{noise_cfg['name']:<25} | {acc*100:7.2f}% | {f1:5.3f}\")\n",
        "\n",
        "            results_table.append({\n",
        "                'Model': model_cfg['name'],\n",
        "                'Noise Type': noise_cfg['name'],\n",
        "                'Accuracy': acc * 100,\n",
        "                'F1': f1\n",
        "            })\n",
        "\n",
        "        print()\n",
        "\n",
        "    # ========================\n",
        "    # Analysis with Drop Metrics\n",
        "    # ========================\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   COMPREHENSIVE ROBUSTNESS ANALYSIS\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    df_results = pd.DataFrame(results_table)\n",
        "\n",
        "    # 1. Print robustness comparison\n",
        "    robustness_df = print_robustness_comparison(df_results)\n",
        "\n",
        "    # 2. Print drop heatmap\n",
        "    print_drop_heatmap(df_results)\n",
        "\n",
        "    # 3. Original pivot table (for reference)\n",
        "    pivot_acc = df_results.pivot(index='Noise Type', columns='Model', values='Accuracy')\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   ACCURACY (%) REFERENCE TABLE\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "    print(pivot_acc.to_string(float_format=\"%.2f\"))\n",
        "    print()\n",
        "\n",
        "    # 4. Save reports\n",
        "    save_robustness_report(df_results, robustness_df, cfg.save_dir)\n",
        "\n",
        "    # 5. Key findings summary\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(\"   KEY FINDINGS SUMMARY\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    best_robust = robustness_df.loc[robustness_df['Avg Drop (%)'].idxmin()]\n",
        "    worst_robust = robustness_df.loc[robustness_df['Avg Drop (%)'].idxmax()]\n",
        "\n",
        "    print(f\"🏆 Most Robust Model: {best_robust['Model']}\")\n",
        "    print(f\"   - Average Drop: {best_robust['Avg Drop (%)']:.2f}%\")\n",
        "    print(f\"   - Average Retention: {best_robust['Avg Retention (%)']:.2f}%\")\n",
        "    print(f\"   - Clean Accuracy: {best_robust['Clean Acc']:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    print(f\"⚠️  Least Robust Model: {worst_robust['Model']}\")\n",
        "    print(f\"   - Average Drop: {worst_robust['Avg Drop (%)']:.2f}%\")\n",
        "    print(f\"   - Average Retention: {worst_robust['Avg Retention (%)']:.2f}%\")\n",
        "    print(f\"   - Clean Accuracy: {worst_robust['Clean Acc']:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Comparative improvement\n",
        "    if 'GAP_Baseline' in robustness_df['Model'].values:\n",
        "        baseline = robustness_df[robustness_df['Model'] == 'GAP_Baseline'].iloc[0]\n",
        "\n",
        "        print(\"📈 Improvements over Baseline:\")\n",
        "        for _, row in robustness_df.iterrows():\n",
        "            if row['Model'] != 'GAP_Baseline':\n",
        "                drop_improvement = baseline['Avg Drop (%)'] - row['Avg Drop (%)']\n",
        "                print(f\"   {row['Model']}: {drop_improvement:+.2f}% drop reduction\")\n",
        "\n",
        "    print(f\"\\n{'='*100}\\n\")\n",
        "\n",
        "    # Save main results\n",
        "    df_results.to_csv(os.path.join(cfg.save_dir, 'noise_robustness_results.csv'), index=False)\n",
        "\n",
        "    return df_results, robustness_df\n",
        "\n",
        "# ========================\n",
        "# 8) Visualization\n",
        "# ========================\n",
        "def plot_noise_robustness_comparison(df_results, robustness_df, save_path):\n",
        "    \"\"\"Create comprehensive visualization of noise robustness results\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Noise Robustness Study: 3-Model Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Drop by noise type\n",
        "    models = df_results['Model'].unique()\n",
        "    noise_types = [nt for nt in df_results['Noise Type'].unique() if nt != 'Clean']\n",
        "\n",
        "    drop_data = []\n",
        "    for model in models:\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "\n",
        "        for noise_type in noise_types:\n",
        "            noise_row = model_data[model_data['Noise Type'] == noise_type]\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                drop = clean_acc - acc\n",
        "                drop_data.append({'Model': model, 'Noise Type': noise_type, 'Drop': drop})\n",
        "\n",
        "    drop_df = pd.DataFrame(drop_data)\n",
        "    pivot_drop = drop_df.pivot(index='Noise Type', columns='Model', values='Drop')\n",
        "    pivot_drop.plot(kind='bar', ax=axes[0, 0], width=0.8, edgecolor='black')\n",
        "    axes[0, 0].set_title('Performance Drop by Noise Type (Lower is Better)', fontsize=14)\n",
        "    axes[0, 0].set_ylabel('Drop (%)', fontsize=12)\n",
        "    axes[0, 0].set_xlabel('Noise Type', fontsize=12)\n",
        "    axes[0, 0].legend(title='Model', loc='upper left')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Plot 2: Average drop comparison\n",
        "    avg_drops = robustness_df.set_index('Model')['Avg Drop (%)']\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71']\n",
        "    avg_drops.plot(kind='bar', ax=axes[0, 1], color=colors, edgecolor='black', width=0.6)\n",
        "    axes[0, 1].set_title('Average Drop Across All Noise Types', fontsize=14)\n",
        "    axes[0, 1].set_ylabel('Average Drop (%)', fontsize=12)\n",
        "    axes[0, 1].set_xlabel('Model', fontsize=12)\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(avg_drops):\n",
        "        axes[0, 1].text(i, v + 0.2, f\"{v:.2f}%\", ha='center', fontweight='bold')\n",
        "\n",
        "    # Plot 3: Retention rates\n",
        "    retention_data = []\n",
        "    for model in models:\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "\n",
        "        for noise_type in noise_types:\n",
        "            noise_row = model_data[model_data['Noise Type'] == noise_type]\n",
        "            if len(noise_row) > 0:\n",
        "                acc = noise_row['Accuracy'].values[0]\n",
        "                retention = (acc / clean_acc) * 100\n",
        "                retention_data.append({'Model': model, 'Noise Type': noise_type, 'Retention': retention})\n",
        "\n",
        "    retention_df = pd.DataFrame(retention_data)\n",
        "    pivot_retention = retention_df.pivot(index='Noise Type', columns='Model', values='Retention')\n",
        "    pivot_retention.plot(kind='line', ax=axes[1, 0], marker='o', linewidth=2, markersize=8)\n",
        "    axes[1, 0].set_title('Retention Rate by Noise Type (Higher is Better)', fontsize=14)\n",
        "    axes[1, 0].set_ylabel('Retention (%)', fontsize=12)\n",
        "    axes[1, 0].set_xlabel('Noise Type', fontsize=12)\n",
        "    axes[1, 0].legend(title='Model', loc='lower left')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 0].axhline(y=90, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
        "\n",
        "    # Plot 4: Clean vs Average Noisy Accuracy\n",
        "    clean_noisy_data = []\n",
        "    for model in models:\n",
        "        model_data = df_results[df_results['Model'] == model]\n",
        "        clean_acc = model_data[model_data['Noise Type'] == 'Clean']['Accuracy'].values[0]\n",
        "        noisy_accs = model_data[model_data['Noise Type'] != 'Clean']['Accuracy']\n",
        "        avg_noisy = noisy_accs.mean()\n",
        "        clean_noisy_data.append({'Model': model, 'Clean': clean_acc, 'Avg Noisy': avg_noisy})\n",
        "\n",
        "    cn_df = pd.DataFrame(clean_noisy_data).set_index('Model')\n",
        "    cn_df.plot(kind='bar', ax=axes[1, 1], width=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_title('Clean vs Average Noisy Accuracy', fontsize=14)\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    axes[1, 1].set_xlabel('Model', fontsize=12)\n",
        "    axes[1, 1].legend(title='Condition')\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"✓ Visualization saved to: {save_path}\")\n",
        "\n",
        "# ========================\n",
        "# 9) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    # Training augmentation settings\n",
        "    config.train_augment_prob = 0.7\n",
        "    config.max_train_noise = 0.4\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_seg_kernel = 9\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    NOISE ROBUSTNESS STUDY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"Training Augmentation: prob={config.train_augment_prob}, max_noise={config.max_train_noise}\")\n",
        "    print(f\"\\n3 Models to Compare:\")\n",
        "    print(f\"  1) GAP_Baseline:  Standard GAP (baseline)\")\n",
        "    print(f\"  2) TPA:           Temporal Prototype Attention\")\n",
        "    print(f\"  3) TPA_WithMask:  TPA + mask filtering\")\n",
        "    print(f\"\\n7 Noise Scenarios:\")\n",
        "    print(f\"  - Clean (no noise)\")\n",
        "    print(f\"  - Gaussian noise (20%, 40%)\")\n",
        "    print(f\"  - Temporal mask (30%)\")\n",
        "    print(f\"  - Channel drift (20%)\")\n",
        "    print(f\"  - Channel drop (20%)\")\n",
        "    print(f\"  - Spike noise (5%)\")\n",
        "    print(f\"\\nTPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Run study\n",
        "    df_results, robustness_df = run_noise_robustness_study(config)\n",
        "\n",
        "    # Generate visualization\n",
        "    plot_path = os.path.join(config.save_dir, \"noise_robustness_comparison.png\")\n",
        "    plot_noise_robustness_comparison(df_results, robustness_df, plot_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STUDY COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(f\"  1. {os.path.join(config.save_dir, 'noise_robustness_results.csv')}\")\n",
        "    print(f\"  2. {os.path.join(config.save_dir, 'robustness_detailed.csv')}\")\n",
        "    print(f\"  3. {os.path.join(config.save_dir, 'robustness_summary.json')}\")\n",
        "    print(f\"  4. {os.path.join(config.save_dir, 'noise_robustness_comparison.png')}\")\n",
        "    print(f\"  5. model_*.pth files for all 3 models\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ]
    }
  ]
}