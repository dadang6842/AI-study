{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GAP과 TPA를 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj9K7uZU7k3f",
        "outputId": "1b2b26be-103d-4cbd-de64-b28992628f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSK_vcXE7lBQ",
        "outputId": "4b71b346-cb36-4f82-86a3-b3ad6ca36792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "    UCI-HAR GAP vs TPA COMPARISON\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "Train Augmentation: prob=0.25, mix=0.35\n",
            "\n",
            "2 Configurations to Compare:\n",
            "  1) GAP:  Global Average Pooling (baseline)\n",
            "  2) TPA:  Temporal Prototype Attention\n",
            "\n",
            "This allows us to measure:\n",
            "  • Effect of TPA vs GAP\n",
            "  • Robustness to transitional noise\n",
            "\n",
            "TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total train samples: 7352\n",
            "  → Train split: 5881 (80%)\n",
            "  → Val split:   1471 (20%)\n",
            "\n",
            "Class distribution check:\n",
            "  Class 0 (WALKING): Train=981, Val=245\n",
            "  Class 1 (WALKING_UPSTAIRS): Train=858, Val=215\n",
            "  Class 2 (WALKING_DOWNSTAIRS): Train=789, Val=197\n",
            "  Class 3 (SITTING): Train=1029, Val=257\n",
            "  Class 4 (STANDING): Train=1099, Val=275\n",
            "  Class 5 (LAYING): Train=1125, Val=282\n",
            "\n",
            "======================================================================\n",
            "    GAP vs TPA COMPARISON\n",
            "======================================================================\n",
            "   Goal: Compare GAP and TPA pooling methods\n",
            "\n",
            "   Training: Both configs use SAME augmentation\n",
            "   Testing: Differ in pooling method only\n",
            "   Scenarios: 17 extreme transitions (p=0.70-0.75, mix=0.55-0.58)\n",
            "======================================================================\n",
            "\n",
            "  [1/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 743/2947 (25.2%)\n",
            "  [2/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 749/2947 (25.4%)\n",
            "  [3/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 708/2947 (24.0%)\n",
            "  [4/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 713/2947 (24.2%)\n",
            "  [5/17] SITTING ↔ LAYING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [6/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 747/2947 (25.3%)\n",
            "  [7/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 674/2947 (22.9%)\n",
            "  [8/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 670/2947 (22.7%)\n",
            "  [9/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [10/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 752/2947 (25.5%)\n",
            "  [11/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 795/2947 (27.0%)\n",
            "  [12/17] WALKING ↔ WALKING_UPSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 758/2947 (25.7%)\n",
            "  [13/17] SITTING ↔ LAYING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 800/2947 (27.1%)\n",
            "  [14/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 718/2947 (24.4%)\n",
            "  [15/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 803/2947 (27.2%)\n",
            "  [16/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 672/2947 (22.8%)\n",
            "  [17/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 745/2947 (25.3%)\n",
            "\n",
            "✓ 17 transitional test sets created.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP\n",
            "   Pooling: GAP\n",
            "======================================================================\n",
            "\n",
            "Training GAP for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4672 A:0.5246 Aug:16 | Val A:0.8226 F1:0.8098 ✓\n",
            "[10/100] Train L:0.4497 A:0.9342 Aug:16 | Val A:0.9381 F1:0.9419\n",
            "[20/100] Train L:0.3894 A:0.9442 Aug:10 | Val A:0.9565 F1:0.9598\n",
            "[30/100] Train L:0.3731 A:0.9497 Aug:9 | Val A:0.9517 F1:0.9553\n",
            "[40/100] Train L:0.3755 A:0.9510 Aug:13 | Val A:0.9545 F1:0.9579\n",
            "\n",
            "⚠ Early stopping triggered at epoch 47\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9626 (epoch 27)\n",
            "\n",
            "✓ Best Val Acc: 0.9626 (epoch 27)\n",
            "  Final Test Acc: 0.9084, F1: 0.9097\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8242 Drop=0.0842 [Vulnerable]\n",
            "    Scenario 2: Acc=0.8812 Drop=0.0271 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.7696 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9016 Drop=0.0068 [Very Robust]\n",
            "    Scenario 5: Acc=0.7645 Drop=0.1439 [Vulnerable]\n",
            "    Scenario 6: Acc=0.9209 Drop=-0.0126 [Very Robust]\n",
            "    Scenario 7: Acc=0.7665 Drop=0.1418 [Vulnerable]\n",
            "    Scenario 8: Acc=0.8955 Drop=0.0129 [Very Robust]\n",
            "    Scenario 9: Acc=0.8300 Drop=0.0784 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8836 Drop=0.0248 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.8083 Drop=0.1001 [Vulnerable]\n",
            "    Scenario 12: Acc=0.7340 Drop=0.1744 [Vulnerable]\n",
            "    Scenario 13: Acc=0.7133 Drop=0.1951 [Vulnerable]\n",
            "    Scenario 14: Acc=0.7329 Drop=0.1754 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8073 Drop=0.1011 [Vulnerable]\n",
            "    Scenario 16: Acc=0.7720 Drop=0.1364 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9199 Drop=-0.0115 [Very Robust]\n",
            "\n",
            " GAP Summary:\n",
            "   Original Test:      0.9084\n",
            "   Avg Transition:     0.8191\n",
            "   Avg Drop:           0.0892\n",
            "   Retention:          90.18%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: TPA\n",
            "   Pooling: TPA\n",
            "======================================================================\n",
            "\n",
            "Training TPA for up to 100 epochs (patience=20)...\n",
            "TPA: prototypes=16, heads=4, temp=0.07\n",
            "[01/100] Train L:1.4688 A:0.5402 Aug:16 | Val A:0.6941 F1:0.6292 | Conf:0.484 ✓\n",
            "[10/100] Train L:0.3974 A:0.9435 Aug:16 | Val A:0.9524 F1:0.9557 | Conf:0.391\n",
            "[20/100] Train L:0.3535 A:0.9520 Aug:10 | Val A:0.9565 F1:0.9598 | Conf:0.386\n",
            "[30/100] Train L:0.3313 A:0.9594 Aug:9 | Val A:0.9680 F1:0.9705 | Conf:0.413\n",
            "[40/100] Train L:0.3194 A:0.9645 Aug:13 | Val A:0.9701 F1:0.9724 | Conf:0.421\n",
            "[50/100] Train L:0.3053 A:0.9708 Aug:5 | Val A:0.9755 F1:0.9774 | Conf:0.422\n",
            "[60/100] Train L:0.2952 A:0.9794 Aug:13 | Val A:0.9762 F1:0.9780 | Conf:0.440\n",
            "[70/100] Train L:0.2864 A:0.9818 Aug:17 | Val A:0.9830 F1:0.9843 | Conf:0.441\n",
            "[80/100] Train L:0.2725 A:0.9886 Aug:14 | Val A:0.9884 F1:0.9893 | Conf:0.438\n",
            "[90/100] Train L:0.2640 A:0.9918 Aug:7 | Val A:0.9912 F1:0.9918 | Conf:0.430 ✓\n",
            "[100/100] Train L:0.2609 A:0.9937 Aug:12 | Val A:0.9912 F1:0.9918 | Conf:0.436\n",
            "\n",
            "✓ Best Val Acc: 0.9912 (epoch 90)\n",
            "  Final Test Acc: 0.9515, F1: 0.9511\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9318 Drop=0.0197 [Very Robust]\n",
            "    Scenario 2: Acc=0.9294 Drop=0.0221 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.8395 Drop=0.1120 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9281 Drop=0.0234 [Slightly Vulnerable]\n",
            "    Scenario 5: Acc=0.9481 Drop=0.0034 [Very Robust]\n",
            "    Scenario 6: Acc=0.9430 Drop=0.0085 [Very Robust]\n",
            "    Scenario 7: Acc=0.8456 Drop=0.1059 [Vulnerable]\n",
            "    Scenario 8: Acc=0.9253 Drop=0.0261 [Slightly Vulnerable]\n",
            "    Scenario 9: Acc=0.9270 Drop=0.0244 [Slightly Vulnerable]\n",
            "    Scenario 10: Acc=0.9311 Drop=0.0204 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.9199 Drop=0.0316 [Slightly Vulnerable]\n",
            "    Scenario 12: Acc=0.8127 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 13: Acc=0.9522 Drop=-0.0007 [Very Robust]\n",
            "    Scenario 14: Acc=0.8171 Drop=0.1344 [Vulnerable]\n",
            "    Scenario 15: Acc=0.9233 Drop=0.0282 [Slightly Vulnerable]\n",
            "    Scenario 16: Acc=0.8195 Drop=0.1320 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9440 Drop=0.0075 [Very Robust]\n",
            "\n",
            " TPA Summary:\n",
            "   Original Test:      0.9515\n",
            "   Avg Transition:     0.9022\n",
            "   Avg Drop:           0.0493\n",
            "   Retention:          94.82%\n",
            "\n",
            "======================================================================\n",
            "   GAP vs TPA RESULTS\n",
            "======================================================================\n",
            "\n",
            "Config          Pooling    Orig     Trans    Drop     Retention \n",
            "----------------------------------------------------------------------\n",
            "GAP             GAP        0.9084   0.8191   0.0892   90.18     %\n",
            "TPA             TPA        0.9515   0.9022   0.0493   94.82     %\n",
            "\n",
            "======================================================================\n",
            "   EFFECT ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "TPA EFFECT:\n",
            "   GAP  →  TPA\n",
            "   Drop: 0.0892 → 0.0493\n",
            "   Improvement: +44.80% drop reduction\n",
            "   Retention gain: +4.65pp\n",
            "   ✓ TPA helps by reducing performance drop on transitional data\n",
            "\n",
            "✓ Results saved to '/content/drive/MyDrive/AI_data/ablation_gap_tpa/gap_tpa_results.json'\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "UCI-HAR Ablation Study: GAP vs TPA (No Mask)\n",
        "\"\"\"\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str = \"ablation\"\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/ablation_gap_tpa\"\n",
        "\n",
        "    epochs: int = 25\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    train_augment_prob: float = 0.25\n",
        "    train_augment_mix: float = 0.35\n",
        "\n",
        "    d_model: int = 128\n",
        "    use_tpa: bool = False\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_seg_kernel: int = 9\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"), (\"Inertial Signals/total_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_x_\", \"txt\"), (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"), (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "_LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None] for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# ========================\n",
        "# 2) Online Transition Augmentation\n",
        "# ========================\n",
        "def apply_transition_augmentation(x: torch.Tensor, y: torch.Tensor, mix_ratio: float = 0.25) -> torch.Tensor:\n",
        "    B, C, T = x.shape\n",
        "\n",
        "    mix_pts = int(T * mix_ratio)\n",
        "\n",
        "    for i in range(B):\n",
        "        if random.random() < 0.5:\n",
        "            other_class_indices = (y != y[i]).nonzero(as_tuple=True)[0]\n",
        "            if len(other_class_indices) > 0:\n",
        "                j = other_class_indices[random.randint(0, len(other_class_indices)-1)]\n",
        "                x[i, :, -mix_pts:] = x[j, :, :mix_pts].clone()\n",
        "\n",
        "    return x\n",
        "\n",
        "# ========================\n",
        "# 3) TPA Module\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Temporal Prototype Attention\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        pad = (seg_kernel - 1) // 2\n",
        "        self.lowpass = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim, bias=False)\n",
        "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
        "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, return_confidence=False):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_filtered = self.lowpass(x.transpose(1, 2)).transpose(1, 2)\n",
        "        xloc = self.pw(self.dw(x_filtered.transpose(1, 2))).transpose(1, 2)\n",
        "        xloc = self.pre_norm(xloc) + x\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        # Compute GAP for fallback\n",
        "        z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 4) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1)) for k in branches])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        # [B, D, T] -> [B, T, D]\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        pooled = features.mean(dim=1)\n",
        "        logits = self.fc(pooled)\n",
        "        aux = {\"confidence\": None}\n",
        "        return logits, aux\n",
        "\n",
        "class TPAHead(nn.Module):\n",
        "    \"\"\"TPA: Temporal Prototype Attention\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, seg_kernel: int = 9,\n",
        "                 heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=num_prototypes,\n",
        "            seg_kernel=seg_kernel,\n",
        "            heads=heads,\n",
        "            dropout=dropout,\n",
        "            temperature=temperature,\n",
        "            topk_ratio=topk_ratio\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        \"\"\"\n",
        "        Fmap: [B, D, T]\n",
        "        \"\"\"\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        z, confidence = self.tpa(features, return_confidence=True)\n",
        "        logits = self.classifier(z)\n",
        "        aux = {\"confidence\": confidence.mean().item()}\n",
        "        return logits, aux\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, use_tpa=False, tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.use_tpa = use_tpa\n",
        "\n",
        "        if use_tpa:\n",
        "            self.head = TPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                seg_kernel=tpa_config.get('seg_kernel', 9),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25)\n",
        "            )\n",
        "        else:\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = self.backbone(x)\n",
        "        return self.head(fmap)\n",
        "\n",
        "# ========================\n",
        "# 5) Train / Eval\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config, verbose_epoch: bool = False):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    aug_count = 0\n",
        "    confidence_vals = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device).float(), y.to(cfg.device)\n",
        "\n",
        "        if random.random() < cfg.train_augment_prob:\n",
        "            x = apply_transition_augmentation(x, y, cfg.train_augment_mix)\n",
        "            aug_count += 1\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, aux = model(x)\n",
        "\n",
        "        cls_loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "        loss = cls_loss\n",
        "        if torch.isnan(loss):\n",
        "            if verbose_epoch:\n",
        "                print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            if aux[\"confidence\"] is not None:\n",
        "                confidence_vals.append(aux[\"confidence\"])\n",
        "\n",
        "    stats = {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "        \"aug_count\": aug_count,\n",
        "        \"avg_confidence\": np.mean(confidence_vals) if confidence_vals else None\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config, classes=6):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits, _ = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(classes)))\n",
        "    report = classification_report(y_true, y_pred, target_names=[_CODE_TO_LABEL_NAME[i] for i in range(classes)], digits=4)\n",
        "    return acc, f1, cm, report\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_simple(model, loader, cfg: Config):\n",
        "    \"\"\"Simple evaluation for accuracy only\"\"\"\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits, _ = model(x)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "\n",
        "        ys.append(y.cpu().numpy())\n",
        "        ps.append(pred.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ========================\n",
        "# 6) Extreme Transition Test Set\n",
        "# ========================\n",
        "def create_transitional_test_set(orig_dataset: UCIHARInertial, class_A: str, class_B: str,\n",
        "                                 p: float=0.05, mix: float=0.25, profile: str='abrupt',\n",
        "                                 pos: str='tail', segments: int=1) -> Tuple[UCIHARInertial, dict]:\n",
        "    \"\"\"Create transitional test set\"\"\"\n",
        "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
        "    N, C, T = X.shape\n",
        "\n",
        "    code_A, code_B = _LABEL_NAME_TO_CODE[class_A], _LABEL_NAME_TO_CODE[class_B]\n",
        "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
        "    mix_pts = int(T * mix)\n",
        "\n",
        "    modified_indices = []\n",
        "\n",
        "    if segments > 1:\n",
        "        seg_length = mix_pts // segments\n",
        "        remaining = mix_pts % segments\n",
        "    else:\n",
        "        seg_length = mix_pts\n",
        "        remaining = 0\n",
        "\n",
        "    def get_transition_positions(T, mix_pts, pos, segments):\n",
        "        positions = []\n",
        "\n",
        "        if segments == 1:\n",
        "            if pos == 'tail':\n",
        "                positions = [T - mix_pts]\n",
        "            elif pos == 'middle':\n",
        "                positions = [(T - mix_pts) // 2]\n",
        "            elif pos == 'random':\n",
        "                positions = [random.randint(0, max(0, T - mix_pts))]\n",
        "        else:\n",
        "            seg_len = mix_pts // segments\n",
        "            if pos == 'tail':\n",
        "                start = T - mix_pts\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * seg_len)\n",
        "            elif pos == 'middle':\n",
        "                center = T // 2\n",
        "                total_span = mix_pts + (segments - 1) * seg_len\n",
        "                start = center - total_span // 2\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * (seg_len * 2))\n",
        "            elif pos == 'random':\n",
        "                available_positions = list(range(0, T - seg_len))\n",
        "                random.shuffle(available_positions)\n",
        "                positions = sorted(available_positions[:segments])\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def apply_transition(target_data, source_data, start_pos, length, profile):\n",
        "        end_pos = start_pos + length\n",
        "\n",
        "        if profile == 'abrupt':\n",
        "            target_data[:, start_pos:end_pos] = source_data[:, start_pos:end_pos].copy()\n",
        "        elif profile == 'fade':\n",
        "            alpha = np.linspace(0, 1, length).reshape(1, -1)\n",
        "            target_segment = target_data[:, start_pos:end_pos]\n",
        "            source_segment = source_data[:, start_pos:end_pos]\n",
        "            target_data[:, start_pos:end_pos] = (\n",
        "                target_segment * (1 - alpha) + source_segment * alpha\n",
        "            )\n",
        "\n",
        "    # Apply transitions for class A\n",
        "    n_targets_A = max(1, int(len(idx_A) * p))\n",
        "    targets_A = np.random.choice(idx_A, n_targets_A, replace=False)\n",
        "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_A, sources_B):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    # Apply transitions for class B\n",
        "    n_targets_B = max(1, int(len(idx_B) * p))\n",
        "    targets_B = np.random.choice(idx_B, n_targets_B, replace=False)\n",
        "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_B, sources_A):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    if p > 0.5:\n",
        "        mid_start = T // 3\n",
        "        mid_end = 2 * T // 3\n",
        "        mid_length = mid_end - mid_start\n",
        "\n",
        "        extra_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p * 0.3)), replace=False)\n",
        "        extra_B_src = np.random.choice(idx_B, len(extra_A), replace=True)\n",
        "\n",
        "        for t, s in zip(extra_A, extra_B_src):\n",
        "            if t not in modified_indices:\n",
        "                apply_transition(X[t], orig_dataset.X[s], mid_start, mid_length, profile)\n",
        "                modified_indices.append(t)\n",
        "\n",
        "    mod_dataset = UCIHARInertial(\n",
        "        root=\"\", split=\"test\",\n",
        "        mean=orig_dataset.mean, std=orig_dataset.std,\n",
        "        preloaded_data=(X, y)\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'total_samples': N,\n",
        "        'modified_samples': len(modified_indices),\n",
        "        'modified_ratio': len(modified_indices) / N,\n",
        "        'mix_frames': mix_pts,\n",
        "        'primary_class_ratio': 1 - mix,\n",
        "        'class_A_modified': len(targets_A),\n",
        "        'class_B_modified': len(targets_B),\n",
        "        'profile': profile,\n",
        "        'position': pos,\n",
        "        'segments': segments\n",
        "    }\n",
        "\n",
        "    return mod_dataset, info\n",
        "\n",
        "def get_transition_scenarios():\n",
        "    \"\"\"Return all transitional test scenarios\"\"\"\n",
        "    scenarios_core = [\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_stress = [\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_ctrl = [\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"middle\",2),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"middle\",2),\n",
        "    ]\n",
        "\n",
        "    return scenarios_core + scenarios_stress + scenarios_ctrl\n",
        "\n",
        "# ========================\n",
        "# 7) GAP vs TPA Comparison\n",
        "# ========================\n",
        "def run_ablation_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # Load full train set\n",
        "    train_set_full = UCIHARInertial(cfg.data_dir, \"train\")\n",
        "\n",
        "    # Split train into train + validation (stratified by class)\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    n_samples = len(train_set_full)\n",
        "    indices = np.arange(n_samples)\n",
        "    labels = train_set_full.y\n",
        "\n",
        "    # Stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Total train samples: {n_samples}\")\n",
        "    print(f\"  → Train split: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val split:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    # Verify class distribution\n",
        "    train_labels = labels[train_indices]\n",
        "    val_labels = labels[val_indices]\n",
        "    print(f\"\\nClass distribution check:\")\n",
        "    for cls in range(6):\n",
        "        train_count = (train_labels == cls).sum()\n",
        "        val_count = (val_labels == cls).sum()\n",
        "        print(f\"  Class {cls} ({_CODE_TO_LABEL_NAME[cls]}): Train={train_count}, Val={val_count}\")\n",
        "\n",
        "    # Create datasets using indices\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)\n",
        "\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader_orig = DataLoader(test_set_orig, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    scenarios = get_transition_scenarios()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    GAP vs TPA COMPARISON\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Goal: Compare GAP and TPA pooling methods\")\n",
        "    print(f\"\\n   Training: Both configs use SAME augmentation\")\n",
        "    print(f\"   Testing: Differ in pooling method only\")\n",
        "    print(f\"   Scenarios: {len(scenarios)} extreme transitions (p=0.70-0.75, mix=0.55-0.58)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    # Create transitional test sets\n",
        "    transition_sets, transition_infos = [], []\n",
        "    for i, scenario in enumerate(scenarios):\n",
        "        clsA, clsB, p, mix, profile, pos, segments = scenario\n",
        "        print(f\"  [{i+1}/{len(scenarios)}] {clsA} ↔ {clsB} (p={p:.2f}, mix={mix:.2f}, {profile}, {pos}, seg={segments})\")\n",
        "        print(f\"      Primary class: {(1-mix)*100:.0f}% | Transition: {mix*100:.0f}%\")\n",
        "        test_set_mod, info = create_transitional_test_set(\n",
        "            test_set_orig, clsA, clsB, p=p, mix=mix, profile=profile, pos=pos, segments=segments\n",
        "        )\n",
        "        transition_sets.append(test_set_mod)\n",
        "        transition_infos.append(info)\n",
        "        print(f\"      Modified: {info['modified_samples']}/{info['total_samples']} ({info['modified_ratio']*100:.1f}%)\")\n",
        "\n",
        "    transition_loaders = [DataLoader(ts, cfg.batch_size, num_workers=cfg.num_workers) for ts in transition_sets]\n",
        "    print(f\"\\n✓ {len(transition_loaders)} transitional test sets created.\\n\")\n",
        "\n",
        "    # 2-way ablation configurations\n",
        "    ablation_configs = [\n",
        "        {\"name\": \"GAP\", \"use_tpa\": False},\n",
        "        {\"name\": \"TPA\", \"use_tpa\": True},\n",
        "    ]\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    # Train and evaluate each configuration\n",
        "    for ab_cfg in ablation_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   CONFIG: {ab_cfg['name']}\")\n",
        "        print(f\"   Pooling: {ab_cfg['name']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        cfg.use_tpa = ab_cfg[\"use_tpa\"]\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        model_path = os.path.join(cfg.save_dir, f\"model_{ab_cfg['name']}.pth\")\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'seg_kernel': cfg.tpa_seg_kernel,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            use_tpa=cfg.use_tpa,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {ab_cfg['name']} for up to {cfg.epochs} epochs (patience={cfg.patience})...\")\n",
        "        if cfg.use_tpa:\n",
        "            print(f\"TPA: prototypes={cfg.tpa_num_prototypes}, heads={cfg.tpa_heads}, temp={cfg.tpa_temperature}\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            # Verbose output for first epoch and every 10th epoch\n",
        "            verbose = (epoch == 1 or epoch % 10 == 0)\n",
        "\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg, verbose_epoch=verbose)\n",
        "\n",
        "            # Evaluate on VALIDATION set for early stopping\n",
        "            val_acc, val_f1, _, _ = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            # Early stopping logic based on VALIDATION accuracy\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # Only print for verbose epochs\n",
        "            if verbose:\n",
        "                log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "                log_str += f\" Aug:{stats['aug_count']}\"\n",
        "                log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "                if stats['avg_confidence'] is not None:\n",
        "                    log_str += f\" | Conf:{stats['avg_confidence']:.3f}\"\n",
        "                if improved:\n",
        "                    log_str += \" ✓\"\n",
        "                print(log_str)\n",
        "\n",
        "            # Early stopping check\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  No improvement for {cfg.patience} epochs on validation set\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Final evaluation on TEST set\n",
        "        acc_orig, f1_orig, _, _ = evaluate(model, test_loader_orig, cfg)\n",
        "        print(f\"  Final Test Acc: {acc_orig:.4f}, F1: {f1_orig:.4f}\")\n",
        "\n",
        "        print(f\"\\n   Evaluating on {len(transition_loaders)} transitional test sets...\")\n",
        "        transition_accs = []\n",
        "        scenario_details = []\n",
        "\n",
        "        for i, loader in enumerate(transition_loaders):\n",
        "            result = evaluate_simple(model, loader, cfg)\n",
        "            acc_mod = result['accuracy']\n",
        "            transition_accs.append(acc_mod)\n",
        "\n",
        "            scenario = scenarios[i]\n",
        "            clsA, clsB, p, mix = scenario[0], scenario[1], scenario[2], scenario[3]\n",
        "            primary_ratio = (1 - mix) * 100\n",
        "            drop_from_orig = acc_orig - acc_mod\n",
        "\n",
        "            if drop_from_orig < 0.02:\n",
        "                grade = \"Very Robust\"\n",
        "            elif drop_from_orig < 0.05:\n",
        "                grade = \"Slightly Vulnerable\"\n",
        "            else:\n",
        "                grade = \"Vulnerable\"\n",
        "\n",
        "            scenario_details.append({\n",
        "                'scenario': f\"{clsA}↔{clsB}\",\n",
        "                'primary_ratio': primary_ratio,\n",
        "                'acc': acc_mod,\n",
        "                'drop': drop_from_orig,\n",
        "                'grade': grade\n",
        "            })\n",
        "\n",
        "            print(f\"    Scenario {i+1}: Acc={acc_mod:.4f} Drop={drop_from_orig:.4f} [{grade}]\")\n",
        "\n",
        "        avg_trans_acc = np.mean(transition_accs)\n",
        "        avg_drop = acc_orig - avg_trans_acc\n",
        "        retention = (1 - avg_drop/acc_orig) * 100 if acc_orig > 0 else 0\n",
        "\n",
        "        result = {\n",
        "            \"config\": ab_cfg[\"name\"],\n",
        "            \"use_tpa\": ab_cfg[\"use_tpa\"],\n",
        "            \"orig_acc\": acc_orig,\n",
        "            \"avg_trans_acc\": avg_trans_acc,\n",
        "            \"avg_drop\": avg_drop,\n",
        "            \"retention\": retention,\n",
        "            \"scenario_details\": scenario_details\n",
        "        }\n",
        "        results_table.append(result)\n",
        "\n",
        "        print(f\"\\n {ab_cfg['name']} Summary:\")\n",
        "        print(f\"   Original Test:      {acc_orig:.4f}\")\n",
        "        print(f\"   Avg Transition:     {avg_trans_acc:.4f}\")\n",
        "        print(f\"   Avg Drop:           {avg_drop:.4f}\")\n",
        "        print(f\"   Retention:          {retention:.2f}%\")\n",
        "\n",
        "    # ========================\n",
        "    # Analysis and Comparison\n",
        "    # ========================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   GAP vs TPA RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"{'Config':<15} {'Pooling':<10} {'Orig':<8} {'Trans':<8} {'Drop':<8} {'Retention':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for r in results_table:\n",
        "        pooling = \"TPA\" if r['use_tpa'] else \"GAP\"\n",
        "        print(f\"{r['config']:<15} {pooling:<10} {r['orig_acc']:<8.4f} {r['avg_trans_acc']:<8.4f} {r['avg_drop']:<8.4f} {r['retention']:<10.2f}%\")\n",
        "\n",
        "    # Compute effects\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   EFFECT ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Find each config\n",
        "    gap = next(r for r in results_table if not r['use_tpa'])\n",
        "    tpa = next(r for r in results_table if r['use_tpa'])\n",
        "\n",
        "    # TPA effect\n",
        "    tpa_effect = gap['avg_drop'] - tpa['avg_drop']\n",
        "    tpa_improve = (tpa_effect / gap['avg_drop'] * 100) if gap['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"TPA EFFECT:\")\n",
        "    print(f\"   GAP  →  TPA\")\n",
        "    print(f\"   Drop: {gap['avg_drop']:.4f} → {tpa['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {tpa_improve:+.2f}% drop reduction\")\n",
        "    print(f\"   Retention gain: {tpa['retention'] - gap['retention']:+.2f}pp\")\n",
        "\n",
        "    if tpa_effect > 0:\n",
        "        print(f\"   ✓ TPA helps by reducing performance drop on transitional data\")\n",
        "    else:\n",
        "        print(f\"   ✗ TPA does not help compared to GAP\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(cfg.save_dir, \"gap_tpa_results.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            'results': results_table,\n",
        "            'analysis': {\n",
        "                'tpa_effect': float(tpa_effect),\n",
        "                'tpa_improve_pct': float(tpa_improve)\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Results saved to '{cfg.save_dir}/gap_tpa_results.json'\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ========================\n",
        "# 8) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.mode = \"ablation\"\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    config.train_augment_prob = 0.25\n",
        "    config.train_augment_mix = 0.35\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_seg_kernel = 9\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    UCI-HAR GAP vs TPA COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"Train Augmentation: prob={config.train_augment_prob}, mix={config.train_augment_mix}\")\n",
        "    print(f\"\\n2 Configurations to Compare:\")\n",
        "    print(f\"  1) GAP:  Global Average Pooling (baseline)\")\n",
        "    print(f\"  2) TPA:  Temporal Prototype Attention\")\n",
        "    print(f\"\\nThis allows us to measure:\")\n",
        "    print(f\"  • Effect of TPA vs GAP\")\n",
        "    print(f\"  • Robustness to transitional noise\")\n",
        "    print(f\"\\nTPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    if config.mode == \"ablation\":\n",
        "        run_ablation_study(config)\n",
        "    else:\n",
        "        print(\"✗ Invalid mode. Set config.mode = 'ablation'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
