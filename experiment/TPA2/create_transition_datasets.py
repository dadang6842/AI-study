# -*- coding: utf-8 -*-
"""create_transition_datasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJa5TJp_Ut5rQXAqvEgIssGzxLRYzwHj
"""

"""
UCI-HAR 데이터셋에 전이 구간을 추가하여 24개 데이터셋 생성

시퀀스 분할 방식:
- 앞 클래스의 앞부분 + 뒤 클래스의 뒷부분 연결
- 예: STAND→SIT 10% = STAND 90% + SIT 10%
- 레이블: 앞 클래스
"""

import numpy as np
import os
from pathlib import Path
import pickle

class TransitionDatasetCreator:
    def __init__(self, uci_data_path):
        """
        Args:
            uci_data_path: UCI HAR Dataset 경로
        """
        self.uci_data_path = Path(uci_data_path)

        # 활동 레이블 매핑 (UCI-HAR 기준)
        self.activity_labels = {
            1: 'WALKING',
            2: 'WALKING_UPSTAIRS',
            3: 'WALKING_DOWNSTAIRS',
            4: 'SITTING',
            5: 'STANDING',
            6: 'LAYING'
        }

        # 전이 구간 정의 (from_activity -> to_activity)
        self.transitions = [
            ('STANDING', 'SITTING'),   # STAND_TO_SIT
            ('SITTING', 'STANDING'),   # SIT_TO_STAND
            ('SITTING', 'LAYING'),     # SIT_TO_LIE
            ('LAYING', 'SITTING'),     # LIE_TO_SIT
            ('STANDING', 'LAYING'),    # STAND_TO_LIE
            ('LAYING', 'STANDING')     # LIE_TO_STAND
        ]

        # 뒤 클래스 혼합 비율
        self.mixing_ratios = [0.1, 0.2, 0.3, 0.4]

        # Inertial Signals 파일 목록
        self.signal_files = [
            'body_acc_x', 'body_acc_y', 'body_acc_z',
            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',
            'total_acc_x', 'total_acc_y', 'total_acc_z'
        ]

    def load_inertial_signals(self, split='train'):
        """
        Inertial Signals 데이터 로드

        Returns:
            X: shape (N, 9, 128) - N개 샘플, 9개 센서, 128 timesteps
            y: shape (N,) - 레이블
        """
        base_path = self.uci_data_path / split / 'Inertial Signals'

        # 각 신호 파일 로드
        signals = []
        for signal_name in self.signal_files:
            file_path = base_path / f'{signal_name}_{split}.txt'
            signal_data = np.loadtxt(file_path)  # shape: (N, 128)
            signals.append(signal_data)

        # (N, 9, 128) 형태로 결합
        X = np.stack(signals, axis=1)

        # 레이블 로드
        label_path = self.uci_data_path / split / f'y_{split}.txt'
        y = np.loadtxt(label_path, dtype=int)

        return X, y

    def get_activity_code(self, activity_name):
        """활동 이름으로 코드 찾기"""
        for code, name in self.activity_labels.items():
            if name == activity_name:
                return code
        return None

    def create_transition_sample(self, from_sample, to_sample, mixing_ratio):
        """
        시퀀스 분할 방식으로 전이 샘플 생성

        Args:
            from_sample: shape (9, 128) - 앞 클래스 샘플
            to_sample: shape (9, 128) - 뒤 클래스 샘플
            mixing_ratio: 뒤 클래스 비율 (0.1, 0.2, 0.3, 0.4)

        Returns:
            transition_sample: shape (9, 128)
            - 앞부분: from_sample의 앞 (1-mixing_ratio) %
            - 뒷부분: to_sample의 뒤 mixing_ratio %
        """
        C, T = from_sample.shape  # (9, 128)

        # 분할 지점 계산
        split_point = int(T * (1 - mixing_ratio))

        # 전이 샘플 생성
        transition_sample = np.zeros_like(from_sample)
        transition_sample[:, :split_point] = from_sample[:, :split_point]  # 앞 부분
        transition_sample[:, split_point:] = to_sample[:, split_point:]    # 뒷 부분

        return transition_sample

    def create_dataset_with_transition(self, X, y, from_activity, to_activity,
                                       mixing_ratio, augmentation_ratio):
        """
        특정 전이 구간으로 데이터셋 증강

        Args:
            X: shape (N, 9, 128)
            y: shape (N,)
            from_activity: 앞 클래스
            to_activity: 뒤 클래스
            mixing_ratio: 뒤 클래스 혼합 비율 (10%, 20%, 30%, 40%)
            augmentation_ratio: 증강 비율 (전체 데이터 대비)

        Returns:
            X_augmented, y_augmented, augmentation_count
        """
        from_code = self.get_activity_code(from_activity)
        to_code = self.get_activity_code(to_activity)

        # 해당 활동의 샘플 인덱스
        from_indices = np.where(y == from_code)[0]
        to_indices = np.where(y == to_code)[0]

        # 증강할 샘플 수
        num_augmentation = int(len(X) * augmentation_ratio)

        # 전이 샘플 생성
        transition_samples = []
        transition_labels = []

        for _ in range(num_augmentation):
            # 랜덤하게 앞/뒤 클래스 샘플 선택
            from_idx = np.random.choice(from_indices)
            to_idx = np.random.choice(to_indices)

            # 시퀀스 분할 방식으로 전이 샘플 생성
            transition_sample = self.create_transition_sample(
                X[from_idx], X[to_idx], mixing_ratio
            )

            transition_samples.append(transition_sample)
            # 레이블은 앞 클래스
            transition_labels.append(from_code)

        # 원본 + 전이 샘플 결합
        X_augmented = np.concatenate([X, np.array(transition_samples)], axis=0)
        y_augmented = np.concatenate([y, np.array(transition_labels)], axis=0)

        # 셔플
        indices = np.random.permutation(len(X_augmented))
        X_augmented = X_augmented[indices]
        y_augmented = y_augmented[indices]

        return X_augmented, y_augmented, num_augmentation

    def generate_all_datasets(self, output_dir='./datasets', augmentation_ratio=0.10):
        """
        24개 데이터셋 생성

        Args:
            output_dir: 출력 디렉토리
            augmentation_ratio: 증강 비율 (기본 10%)
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        print("=" * 80)
        print("Loading UCI-HAR Inertial Signals...")
        print("=" * 80)

        # Inertial Signals 데이터 로드
        X_train, y_train = self.load_inertial_signals('train')
        X_test, y_test = self.load_inertial_signals('test')

        print(f"\nOriginal train: {X_train.shape} (samples, channels, timesteps)")
        print(f"Original test: {X_test.shape}")
        print(f"\nAugmentation ratio: {int(augmentation_ratio * 100)}% per transition")
        print(f"Expected augmentation: ~{int(len(X_train) * augmentation_ratio)} samples\n")

        # 메타데이터
        metadata = {
            'original_train_shape': X_train.shape,
            'original_test_shape': X_test.shape,
            'activity_labels': self.activity_labels,
            'transitions': self.transitions,
            'mixing_ratios': self.mixing_ratios,
            'augmentation_ratio': augmentation_ratio,
            'method': 'sequence_splitting',
            'datasets': []
        }

        # 24개 조합 생성
        dataset_count = 0
        for from_activity, to_activity in self.transitions:
            transition_name = f"{from_activity}_TO_{to_activity}"

            for mixing_ratio in self.mixing_ratios:
                dataset_count += 1
                ratio_pct = int(mixing_ratio * 100)
                dataset_name = f"{transition_name}_{ratio_pct}pct"

                print("=" * 80)
                print(f"[{dataset_count}/24] Creating: {dataset_name}")
                print("=" * 80)
                print(f"Transition: {from_activity} → {to_activity}")
                print(f"Mixing: Front {100-ratio_pct}% {from_activity} + Back {ratio_pct}% {to_activity}")
                print(f"Label: {from_activity} ({self.get_activity_code(from_activity)})")

                # Train 증강
                X_train_aug, y_train_aug, train_count = self.create_dataset_with_transition(
                    X_train.copy(), y_train.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # Test 증강
                X_test_aug, y_test_aug, test_count = self.create_dataset_with_transition(
                    X_test.copy(), y_test.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # 저장
                dataset_dir = output_path / dataset_name
                dataset_dir.mkdir(exist_ok=True)

                np.save(dataset_dir / 'X_train.npy', X_train_aug)
                np.save(dataset_dir / 'y_train.npy', y_train_aug)
                np.save(dataset_dir / 'X_test.npy', X_test_aug)
                np.save(dataset_dir / 'y_test.npy', y_test_aug)

                # 정보 저장
                dataset_info = {
                    'name': dataset_name,
                    'transition': transition_name,
                    'from_activity': from_activity,
                    'to_activity': to_activity,
                    'from_label': self.get_activity_code(from_activity),
                    'mixing_ratio': mixing_ratio,
                    'augmentation_ratio': augmentation_ratio,
                    'original_train_size': len(X_train),
                    'original_test_size': len(X_test),
                    'augmented_train_size': len(X_train_aug),
                    'augmented_test_size': len(X_test_aug),
                    'train_augmentation_count': train_count,
                    'test_augmentation_count': test_count
                }

                with open(dataset_dir / 'info.txt', 'w') as f:
                    f.write(f"Dataset: {dataset_name}\n")
                    f.write("=" * 60 + "\n\n")
                    f.write(f"Method: Sequence Splitting\n")
                    f.write(f"Transition: {from_activity} → {to_activity}\n")
                    f.write(f"Composition: Front {100-ratio_pct}% {from_activity} + Back {ratio_pct}% {to_activity}\n")
                    f.write(f"Label: {from_activity} (code: {dataset_info['from_label']})\n")
                    f.write(f"Augmentation: {int(augmentation_ratio*100)}%\n\n")
                    f.write(f"Train: {len(X_train)} → {len(X_train_aug)} (+{train_count})\n")
                    f.write(f"Test: {len(X_test)} → {len(X_test_aug)} (+{test_count})\n")
                    f.write(f"Shape: (samples, channels=9, timesteps=128)\n")

                metadata['datasets'].append(dataset_info)

                print(f"\n✓ Train: {len(X_train)} → {len(X_train_aug)} (+{train_count})")
                print(f"✓ Test: {len(X_test)} → {len(X_test_aug)} (+{test_count})")
                print(f"✓ Saved to: {dataset_dir}\n")

        # 메타데이터 저장
        with open(output_path / 'metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)

        # 요약 저장
        self._save_summary(output_path, metadata)

        print("\n" + "=" * 80)
        print("✓ All 24 datasets created successfully!")
        print("=" * 80)
        print(f"Output: {output_path.absolute()}")
        print(f"Summary: {output_path / 'summary.txt'}")

        return metadata

    def _save_summary(self, output_path, metadata):
        """요약 저장"""
        with open(output_path / 'summary.txt', 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("UCI-HAR TRANSITION DATASETS (Sequence Splitting Method)\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Total datasets: 24\n")
            f.write(f"Method: Sequence Splitting (Time-based)\n")
            f.write(f"Transitions: 6\n")
            f.write(f"Mixing ratios: 4 (10%, 20%, 30%, 40%)\n")
            f.write(f"Augmentation ratio: {int(metadata['augmentation_ratio']*100)}%\n")
            f.write(f"Data shape: (samples, channels=9, timesteps=128)\n")
            f.write(f"Labeling: Front class\n\n")

            f.write("=" * 80 + "\n")
            f.write("METHOD EXPLANATION\n")
            f.write("=" * 80 + "\n")
            f.write("Sequence Splitting:\n")
            f.write("  - 10%: Front 90% from class A + Back 10% from class B\n")
            f.write("  - 20%: Front 80% from class A + Back 20% from class B\n")
            f.write("  - 30%: Front 70% from class A + Back 30% from class B\n")
            f.write("  - 40%: Front 60% from class A + Back 40% from class B\n")
            f.write("  - Label: Always class A (front class)\n\n")

            f.write("Example (STANDING→SITTING 10%):\n")
            f.write("  [STAND STAND ... STAND | SIT SIT ... SIT]\n")
            f.write("   |------ 115 pts ------|  |--- 13 pts --|\n")
            f.write("   Label: STANDING (5)\n\n")

            f.write("=" * 80 + "\n")
            f.write("DATASET LIST\n")
            f.write("=" * 80 + "\n\n")

            for i, ds in enumerate(metadata['datasets'], 1):
                f.write(f"{i:2d}. {ds['name']}\n")
                f.write(f"    {ds['from_activity']} → {ds['to_activity']}\n")
                f.write(f"    Mix: Front {100-int(ds['mixing_ratio']*100)}% + Back {int(ds['mixing_ratio']*100)}%\n")
                f.write(f"    Label: {ds['from_activity']} ({ds['from_label']})\n")
                f.write(f"    Train: {ds['original_train_size']} → {ds['augmented_train_size']} "
                       f"(+{ds['train_augmentation_count']})\n")
                f.write(f"    Test: {ds['original_test_size']} → {ds['augmented_test_size']} "
                       f"(+{ds['test_augmentation_count']})\n\n")


def main():
    """메인 실행"""
    import sys

    if len(sys.argv) < 2:
        print("=" * 80)
        print("UCI-HAR Transition Dataset Generator (Sequence Splitting)")
        print("=" * 80)
        print("\nUsage:")
        print("  python create_transition_datasets.py <UCI_HAR_path> [output] [aug_ratio]")
        print("\nArguments:")
        print("  UCI_HAR_path : UCI HAR Dataset path (required)")
        print("  output       : Output directory (default: ./datasets)")
        print("  aug_ratio    : Augmentation ratio (default: 0.10)")
        print("\nExamples:")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset ./datasets 0.15")
        print("\nMethod:")
        print("  - Sequence splitting (time-based)")
        print("  - Front X% + Back (100-X)%")
        print("  - Label: Front class")
        print("=" * 80)
        sys.exit(1)

    uci_data_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else './datasets'
    augmentation_ratio = float(sys.argv[3]) if len(sys.argv) > 3 else 0.10

    if not os.path.exists(uci_data_path):
        print(f"❌ Error: Path not found: {uci_data_path}")
        sys.exit(1)

    if augmentation_ratio <= 0 or augmentation_ratio >= 1:
        print(f"❌ Error: aug_ratio must be 0 < ratio < 1, got {augmentation_ratio}")
        sys.exit(1)

    print("\n" + "=" * 80)
    print("CONFIGURATION")
    print("=" * 80)
    print(f"UCI-HAR path: {uci_data_path}")
    print(f"Output: {output_dir}")
    print(f"Augmentation: {int(augmentation_ratio*100)}%")
    print(f"Method: Sequence Splitting")
    print("=" * 80 + "\n")

    creator = TransitionDatasetCreator(uci_data_path)
    metadata = creator.generate_all_datasets(output_dir, augmentation_ratio)

    print("\n✓ Completed!\n")


if __name__ == "__main__":
    main()
    def __init__(self, uci_data_path):
        """
        Args:
            uci_data_path: UCI HAR Dataset 경로
        """
        self.uci_data_path = Path(uci_data_path)

        # 활동 레이블 매핑 (UCI-HAR 기준)
        self.activity_labels = {
            1: 'WALKING',
            2: 'WALKING_UPSTAIRS',
            3: 'WALKING_DOWNSTAIRS',
            4: 'SITTING',
            5: 'STANDING',
            6: 'LAYING'
        }

        # 전이 구간 정의 (from_activity -> to_activity)
        # 앞 클래스 → 뒤 클래스
        self.transitions = [
            ('STANDING', 'SITTING'),   # STAND_TO_SIT
            ('SITTING', 'STANDING'),   # SIT_TO_STAND
            ('SITTING', 'LAYING'),     # SIT_TO_LIE
            ('LAYING', 'SITTING'),     # LIE_TO_SIT
            ('STANDING', 'LAYING'),    # STAND_TO_LIE
            ('LAYING', 'STANDING')     # LIE_TO_STAND
        ]

        # 뒤 클래스 혼합 비율
        self.mixing_ratios = [0.1, 0.2, 0.3, 0.4]

    def load_uci_data(self, split='train'):
        """UCI-HAR 데이터 로드"""
        if split == 'train':
            X_file = self.uci_data_path / 'train' / 'X_train.txt'
            y_file = self.uci_data_path / 'train' / 'y_train.txt'
        else:
            X_file = self.uci_data_path / 'test' / 'X_test.txt'
            y_file = self.uci_data_path / 'test' / 'y_test.txt'

        X = np.loadtxt(X_file)
        y = np.loadtxt(y_file, dtype=int)

        return X, y

    def get_activity_code(self, activity_name):
        """활동 이름으로 코드 찾기"""
        for code, name in self.activity_labels.items():
            if name == activity_name:
                return code
        return None

    def create_transition_sample(self, from_features, to_features, mixing_ratio):
        """
        전이 샘플 생성 (선형 보간)

        Args:
            from_features: 앞 클래스의 특징 벡터
            to_features: 뒤 클래스의 특징 벡터
            mixing_ratio: 뒤 클래스의 혼합 비율 (0.1, 0.2, 0.3, 0.4)

        Returns:
            전이 샘플 = 앞 클래스 * (1 - mixing_ratio) + 뒤 클래스 * mixing_ratio
        """
        transition_sample = from_features * (1 - mixing_ratio) + to_features * mixing_ratio
        return transition_sample

    def create_dataset_with_transition(self, X, y, from_activity, to_activity,
                                       mixing_ratio, augmentation_ratio):
        """
        특정 전이 구간과 혼합 비율로 데이터셋 생성

        Args:
            X: 원본 특징 데이터
            y: 원본 레이블 데이터
            from_activity: 앞 클래스 (시작 활동)
            to_activity: 뒤 클래스 (목표 활동)
            mixing_ratio: 뒤 클래스 혼합 비율 (10%, 20%, 30%, 40%)
            augmentation_ratio: 증강 비율 (전체 데이터 대비, 예: 0.10 = 10%)

        Returns:
            X_augmented: 증강된 특징 데이터
            y_augmented: 증강된 레이블 데이터 (전이 샘플은 앞 클래스로 레이블링)
            augmentation_count: 추가된 전이 샘플 수
        """
        from_code = self.get_activity_code(from_activity)
        to_code = self.get_activity_code(to_activity)

        # 해당 활동의 샘플 인덱스 찾기
        from_indices = np.where(y == from_code)[0]
        to_indices = np.where(y == to_code)[0]

        # 증강할 샘플 수 계산 (전체 데이터의 augmentation_ratio %)
        num_augmentation = int(len(X) * augmentation_ratio)

        # 전이 샘플 생성
        transition_X = []
        transition_y = []

        for _ in range(num_augmentation):
            # 랜덤하게 앞/뒤 클래스 샘플 선택
            from_idx = np.random.choice(from_indices)
            to_idx = np.random.choice(to_indices)

            # 전이 샘플 생성 (앞 클래스 + 뒤 클래스 혼합)
            trans_sample = self.create_transition_sample(
                X[from_idx], X[to_idx], mixing_ratio
            )

            transition_X.append(trans_sample)
            # 레이블은 앞 클래스로!
            transition_y.append(from_code)

        # 원본 데이터 + 전이 샘플 결합
        X_augmented = np.vstack([X, np.array(transition_X)])
        y_augmented = np.hstack([y, np.array(transition_y)])

        # 셔플
        indices = np.random.permutation(len(X_augmented))
        X_augmented = X_augmented[indices]
        y_augmented = y_augmented[indices]

        return X_augmented, y_augmented, num_augmentation

    def generate_all_datasets(self, output_dir='./datasets', augmentation_ratio=0.10):
        """
        24개 데이터셋 모두 생성

        Args:
            output_dir: 출력 디렉토리
            augmentation_ratio: 증강 비율 (전체 데이터 대비, 기본값 10%)
                               예: 0.05 = 5%, 0.10 = 10%, 0.15 = 15%
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # 원본 데이터 로드
        print("=" * 80)
        print("Loading UCI-HAR dataset...")
        print("=" * 80)
        X_train, y_train = self.load_uci_data('train')
        X_test, y_test = self.load_uci_data('test')

        print(f"\nOriginal train set: {X_train.shape[0]} samples, {X_train.shape[1]} features")
        print(f"Original test set: {X_test.shape[0]} samples, {X_test.shape[1]} features")
        print(f"\nAugmentation ratio: {int(augmentation_ratio * 100)}% per transition")
        print(f"Expected augmentation per dataset: ~{int(len(X_train) * augmentation_ratio)} samples\n")

        # 메타데이터 저장
        metadata = {
            'original_train_shape': X_train.shape,
            'original_test_shape': X_test.shape,
            'activity_labels': self.activity_labels,
            'transitions': self.transitions,
            'mixing_ratios': self.mixing_ratios,
            'augmentation_ratio': augmentation_ratio,
            'datasets': []
        }

        # 24개 조합 생성
        dataset_count = 0
        for from_activity, to_activity in self.transitions:
            transition_name = f"{from_activity}_TO_{to_activity}"

            for mixing_ratio in self.mixing_ratios:
                dataset_count += 1
                ratio_pct = int(mixing_ratio * 100)
                dataset_name = f"{transition_name}_{ratio_pct}pct"

                print("=" * 80)
                print(f"[{dataset_count}/24] Creating dataset: {dataset_name}")
                print("=" * 80)
                print(f"Transition: {from_activity} → {to_activity}")
                print(f"Mixing ratio: {ratio_pct}% of {to_activity} features")
                print(f"Label: {from_activity} ({self.get_activity_code(from_activity)})")
                print(f"Augmentation: {int(augmentation_ratio * 100)}% of total data")

                # Train 데이터셋 생성
                X_train_aug, y_train_aug, train_aug_count = self.create_dataset_with_transition(
                    X_train.copy(), y_train.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # Test 데이터셋 생성
                X_test_aug, y_test_aug, test_aug_count = self.create_dataset_with_transition(
                    X_test.copy(), y_test.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # 데이터셋 저장
                dataset_dir = output_path / dataset_name
                dataset_dir.mkdir(exist_ok=True)

                np.save(dataset_dir / 'X_train.npy', X_train_aug)
                np.save(dataset_dir / 'y_train.npy', y_train_aug)
                np.save(dataset_dir / 'X_test.npy', X_test_aug)
                np.save(dataset_dir / 'y_test.npy', y_test_aug)

                # 데이터셋 정보 저장
                dataset_info = {
                    'name': dataset_name,
                    'transition': transition_name,
                    'from_activity': from_activity,
                    'to_activity': to_activity,
                    'from_label': self.get_activity_code(from_activity),
                    'to_label': self.get_activity_code(to_activity),
                    'mixing_ratio': mixing_ratio,
                    'augmentation_ratio': augmentation_ratio,
                    'original_train_size': len(X_train),
                    'original_test_size': len(X_test),
                    'augmented_train_size': len(X_train_aug),
                    'augmented_test_size': len(X_test_aug),
                    'train_augmentation_count': train_aug_count,
                    'test_augmentation_count': test_aug_count
                }

                with open(dataset_dir / 'info.txt', 'w') as f:
                    f.write(f"Dataset: {dataset_name}\n")
                    f.write(f"=" * 60 + "\n\n")
                    f.write(f"Transition: {from_activity} → {to_activity}\n")
                    f.write(f"Mixing Ratio: {ratio_pct}% {to_activity} features\n")
                    f.write(f"Label: {from_activity} (code: {dataset_info['from_label']})\n")
                    f.write(f"Augmentation Ratio: {int(augmentation_ratio * 100)}%\n\n")
                    f.write(f"Train:\n")
                    f.write(f"  Original: {dataset_info['original_train_size']} samples\n")
                    f.write(f"  Augmented: {train_aug_count} samples added\n")
                    f.write(f"  Total: {dataset_info['augmented_train_size']} samples\n\n")
                    f.write(f"Test:\n")
                    f.write(f"  Original: {dataset_info['original_test_size']} samples\n")
                    f.write(f"  Augmented: {test_aug_count} samples added\n")
                    f.write(f"  Total: {dataset_info['augmented_test_size']} samples\n")

                metadata['datasets'].append(dataset_info)

                print(f"\n✓ Train: {X_train.shape[0]} → {X_train_aug.shape[0]} (+{train_aug_count})")
                print(f"✓ Test: {X_test.shape[0]} → {X_test_aug.shape[0]} (+{test_aug_count})")
                print(f"✓ Saved to: {dataset_dir}\n")

        # 전체 메타데이터 저장
        with open(output_path / 'metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)

        # 요약 정보 저장
        self._save_summary(output_path, metadata)

        print("\n" + "=" * 80)
        print("✓ All 24 datasets created successfully!")
        print("=" * 80)
        print(f"Output directory: {output_path.absolute()}")
        print(f"Summary: {output_path / 'summary.txt'}")
        print(f"Metadata: {output_path / 'metadata.pkl'}")

        return metadata

    def _save_summary(self, output_path, metadata):
        """요약 정보 저장"""
        with open(output_path / 'summary.txt', 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("UCI-HAR TRANSITION DATASETS SUMMARY\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Total datasets created: 24\n")
            f.write(f"Transitions: 6\n")
            f.write(f"Mixing ratios per transition: 4 (10%, 20%, 30%, 40%)\n")
            f.write(f"Augmentation ratio: {int(metadata['augmentation_ratio'] * 100)}%\n")
            f.write(f"Labeling: Front class (앞 클래스)\n\n")

            f.write("=" * 80 + "\n")
            f.write("DATASET LIST\n")
            f.write("=" * 80 + "\n\n")

            for i, ds in enumerate(metadata['datasets'], 1):
                f.write(f"{i:2d}. {ds['name']}\n")
                f.write(f"    Transition: {ds['from_activity']} → {ds['to_activity']}\n")
                f.write(f"    Mixing: {int(ds['mixing_ratio']*100)}% {ds['to_activity']} features\n")
                f.write(f"    Label: {ds['from_activity']} (code: {ds['from_label']})\n")
                f.write(f"    Train: {ds['original_train_size']} → {ds['augmented_train_size']} "
                       f"(+{ds['train_augmentation_count']})\n")
                f.write(f"    Test: {ds['original_test_size']} → {ds['augmented_test_size']} "
                       f"(+{ds['test_augmentation_count']})\n\n")


def main():
    """메인 실행 함수"""
    import sys

    if len(sys.argv) < 2:
        print("=" * 80)
        print("UCI-HAR Transition Dataset Generator")
        print("=" * 80)
        print("\nUsage:")
        print("  python create_transition_datasets.py <UCI_HAR_path> [output_dir] [aug_ratio]")
        print("\nArguments:")
        print("  UCI_HAR_path : Path to UCI HAR Dataset folder (required)")
        print("  output_dir   : Output directory (optional, default: ./datasets)")
        print("  aug_ratio    : Augmentation ratio (optional, default: 0.10)")
        print("                 Examples: 0.05 (5%), 0.10 (10%), 0.15 (15%)")
        print("\nExamples:")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset ./my_datasets")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset ./datasets 0.15")
        print("\nOutput:")
        print("  - 24 datasets (6 transitions × 4 mixing ratios)")
        print("  - Each transition adds samples with 10%, 20%, 30%, 40% mixing")
        print("  - Transition samples labeled as front class")
        print("=" * 80)
        sys.exit(1)

    uci_data_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else './datasets'
    augmentation_ratio = float(sys.argv[3]) if len(sys.argv) > 3 else 0.10

    # 입력 검증
    if not os.path.exists(uci_data_path):
        print(f"❌ Error: Path not found: {uci_data_path}")
        sys.exit(1)

    if augmentation_ratio <= 0 or augmentation_ratio >= 1:
        print(f"❌ Error: augmentation_ratio must be between 0 and 1, got {augmentation_ratio}")
        sys.exit(1)

    print("\n" + "=" * 80)
    print("CONFIGURATION")
    print("=" * 80)
    print(f"UCI-HAR Dataset path: {uci_data_path}")
    print(f"Output directory: {output_dir}")
    print(f"Augmentation ratio: {int(augmentation_ratio * 100)}% per transition")
    print("=" * 80 + "\n")

    # 데이터셋 생성
    creator = TransitionDatasetCreator(uci_data_path)
    metadata = creator.generate_all_datasets(
        output_dir=output_dir,
        augmentation_ratio=augmentation_ratio
    )

    print("\n✓ Dataset creation completed successfully!\n")


if __name__ == "__main__":
    main()