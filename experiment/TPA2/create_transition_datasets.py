# -*- coding: utf-8 -*-
"""create_transition_datasets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJa5TJp_Ut5rQXAqvEgIssGzxLRYzwHj
"""

"""
UCI-HAR 데이터셋에 전이 구간을 추가하여 24개 데이터셋 생성

시퀀스 분할 방식:
- 앞 클래스의 앞부분 + 뒤 클래스의 뒷부분 연결
- 예: STAND→SIT 10% = STAND 90% + SIT 10%
- 레이블: 앞 클래스
"""

import numpy as np
import os
from pathlib import Path
import pickle

class TransitionDatasetCreator:
    def __init__(self, uci_data_path):
        """
        Args:
            uci_data_path: UCI HAR Dataset 경로
        """
        self.uci_data_path = Path(uci_data_path)

        # 활동 레이블 매핑 (UCI-HAR 기준)
        self.activity_labels = {
            1: 'WALKING',
            2: 'WALKING_UPSTAIRS',
            3: 'WALKING_DOWNSTAIRS',
            4: 'SITTING',
            5: 'STANDING',
            6: 'LAYING'
        }

        # 전이 구간 정의 (from_activity -> to_activity)
        self.transitions = [
            # 기존 정적 활동 전이 (6개)
            ('STANDING', 'SITTING'),   # STAND_TO_SIT
            ('SITTING', 'STANDING'),   # SIT_TO_STAND
            ('SITTING', 'LAYING'),     # SIT_TO_LIE
            ('LAYING', 'SITTING'),     # LIE_TO_SIT
            ('STANDING', 'LAYING'),    # STAND_TO_LIE
            ('LAYING', 'STANDING'),    # LIE_TO_STAND
            # Walking 관련 전이 (4개)
            ('WALKING', 'WALKING_UPSTAIRS'),     # WALK_TO_WALK_UP
            ('WALKING', 'WALKING_DOWNSTAIRS'),   # WALK_TO_WALK_DOWN
            ('WALKING_UPSTAIRS', 'WALKING'),     # WALK_UP_TO_WALK
            ('WALKING_DOWNSTAIRS', 'WALKING')    # WALK_DOWN_TO_WALK
        ]

        # 뒤 클래스 분할 비율
        self.mixing_ratios = [0.1, 0.2, 0.3, 0.4]

        # Inertial Signals 파일 목록
        self.signal_files = [
            'body_acc_x', 'body_acc_y', 'body_acc_z',
            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',
            'total_acc_x', 'total_acc_y', 'total_acc_z'
        ]

    def load_inertial_signals(self, split='train'):
        """
        Inertial Signals 데이터 로드

        Returns:
            X: shape (N, 9, 128) - N개 샘플, 9개 센서, 128 timesteps
            y: shape (N,) - 레이블
        """
        base_path = self.uci_data_path / split / 'Inertial Signals'

        # 각 신호 파일 로드
        signals = []
        for signal_name in self.signal_files:
            file_path = base_path / f'{signal_name}_{split}.txt'
            signal_data = np.loadtxt(file_path)  # shape: (N, 128)
            signals.append(signal_data)

        # (N, 9, 128) 형태로 결합
        X = np.stack(signals, axis=1)

        # 레이블 로드
        label_path = self.uci_data_path / split / f'y_{split}.txt'
        y = np.loadtxt(label_path, dtype=int)

        return X, y

    def get_activity_code(self, activity_name):
        """활동 이름으로 코드 찾기"""
        for code, name in self.activity_labels.items():
            if name == activity_name:
                return code
        return None

    def create_transition_sample(self, from_sample, to_sample, mixing_ratio):
        """
        시퀀스 분할 방식으로 전이 샘플 생성

        Args:
            from_sample: shape (9, 128) - 앞 클래스 샘플
            to_sample: shape (9, 128) - 뒤 클래스 샘플
            split_ratio: 뒤 클래스 분할 비율 (0.1, 0.2, 0.3, 0.4)

        Returns:
            transition_sample: shape (9, 128)
            - 앞부분: from_sample의 앞 (1-mixing_ratio) %
            - 뒷부분: to_sample의 앞 mixing_ratio %
        """
        C, T = from_sample.shape  # (9, 128)

        # 분할 지점 계산
        split_point = int(T * (1 - mixing_ratio))

        # 전이 샘플 생성
        transition_sample = np.zeros_like(from_sample)
        transition_sample[:, :split_point] = from_sample[:, :split_point]
        transition_sample[:, split_point:] = to_sample[:, :T - split_point]

        return transition_sample

    def create_dataset_with_transition(self, X, y, from_activity, to_activity,
                                       mixing_ratio, augmentation_ratio):
        """
        특정 전이 구간으로 데이터셋 증강

        Args:
            X: shape (N, 9, 128)
            y: shape (N,)
            from_activity: 앞 클래스
            to_activity: 뒤 클래스
            mixing_ratio: 뒤 클래스 분할 비율 (10%, 20%, 30%, 40%)
            augmentation_ratio: 증강 비율 (전체 데이터 대비)

        Returns:
            X_augmented, y_augmented, augmentation_count
        """
        from_code = self.get_activity_code(from_activity)
        to_code = self.get_activity_code(to_activity)

        # 해당 활동의 샘플 인덱스
        from_indices = np.where(y == from_code)[0]
        to_indices = np.where(y == to_code)[0]

        # 증강할 샘플 수
        num_augmentation = int(len(X) * augmentation_ratio)

        # 전이 샘플 생성
        transition_samples = []
        transition_labels = []

        for _ in range(num_augmentation):
            # 랜덤하게 앞/뒤 클래스 샘플 선택
            from_idx = np.random.choice(from_indices)
            to_idx = np.random.choice(to_indices)

            # 시퀀스 분할 방식으로 전이 샘플 생성
            transition_sample = self.create_transition_sample(
                X[from_idx], X[to_idx], mixing_ratio
            )

            transition_samples.append(transition_sample)
            # 레이블은 앞 클래스
            transition_labels.append(from_code)

        # 원본 + 전이 샘플 결합
        X_augmented = np.concatenate([X, np.array(transition_samples)], axis=0)
        y_augmented = np.concatenate([y, np.array(transition_labels)], axis=0)

        # 셔플
        indices = np.random.permutation(len(X_augmented))
        X_augmented = X_augmented[indices]
        y_augmented = y_augmented[indices]

        return X_augmented, y_augmented, num_augmentation

    def generate_all_datasets(self, output_dir='./datasets', augmentation_ratio=0.10):
        """
        24개 데이터셋 생성

        Args:
            output_dir: 출력 디렉토리
            augmentation_ratio: 증강 비율 (기본 10%)
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        print("=" * 80)
        print("Loading UCI-HAR Inertial Signals...")
        print("=" * 80)

        # Inertial Signals 데이터 로드
        X_train, y_train = self.load_inertial_signals('train')
        X_test, y_test = self.load_inertial_signals('test')

        print(f"\nOriginal train: {X_train.shape} (samples, channels, timesteps)")
        print(f"Original test: {X_test.shape}")
        print(f"\nAugmentation ratio: {int(augmentation_ratio * 100)}% per transition")
        print(f"Expected augmentation: ~{int(len(X_train) * augmentation_ratio)} samples\n")

        # 메타데이터
        metadata = {
            'original_train_shape': X_train.shape,
            'original_test_shape': X_test.shape,
            'activity_labels': self.activity_labels,
            'transitions': self.transitions,
            'mixing_ratios': self.mixing_ratios,
            'augmentation_ratio': augmentation_ratio,
            'method': 'sequence_splitting',
            'datasets': []
        }

        # 0. 원본 데이터셋 저장 (전이 없음)
        print("=" * 80)
        print("[0/41] Creating: ORIGINAL (No Transition)")
        print("=" * 80)
        print("Original dataset without any transition augmentation")

        original_dir = output_path / 'ORIGINAL'
        original_dir.mkdir(exist_ok=True)

        np.save(original_dir / 'X_train.npy', X_train)
        np.save(original_dir / 'y_train.npy', y_train)
        np.save(original_dir / 'X_test.npy', X_test)
        np.save(original_dir / 'y_test.npy', y_test)

        with open(original_dir / 'info.txt', 'w') as f:
            f.write(f"Dataset: ORIGINAL\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"Original UCI-HAR dataset without transition augmentation\n\n")
            f.write(f"Train: {len(X_train)} samples\n")
            f.write(f"Test: {len(X_test)} samples\n")
            f.write(f"Shape: (samples, channels=9, timesteps=128)\n")

        original_info = {
            'name': 'ORIGINAL',
            'transition': 'None',
            'from_activity': 'None',
            'to_activity': 'None',
            'from_label': None,
            'mixing_ratio': 0.0,
            'augmentation_ratio': 0.0,
            'original_train_size': len(X_train),
            'original_test_size': len(X_test),
            'augmented_train_size': len(X_train),
            'augmented_test_size': len(X_test),
            'train_augmentation_count': 0,
            'test_augmentation_count': 0
        }
        metadata['datasets'].append(original_info)

        print(f"\n✓ Train: {len(X_train)} samples")
        print(f"✓ Test: {len(X_test)} samples")
        print(f"✓ Saved to: {original_dir}\n")

        # 40개 조합 생성 (10개 전이 × 4개 비율)
        dataset_count = 0
        for from_activity, to_activity in self.transitions:
            transition_name = f"{from_activity}_TO_{to_activity}"

            for mixing_ratio in self.mixing_ratios:
                dataset_count += 1
                ratio_pct = int(mixing_ratio * 100)
                dataset_name = f"{transition_name}_{ratio_pct}pct"

                print("=" * 80)
                print(f"[{dataset_count}/40] Creating: {dataset_name}")
                print("=" * 80)
                print(f"Transition: {from_activity} → {to_activity}")
                print(f"Mixing: Front {100-ratio_pct}% {from_activity} + Back {ratio_pct}% {to_activity}")
                print(f"Label: {from_activity} ({self.get_activity_code(from_activity)})")

                # Train 증강
                X_train_aug, y_train_aug, train_count = self.create_dataset_with_transition(
                    X_train.copy(), y_train.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # Test 증강
                X_test_aug, y_test_aug, test_count = self.create_dataset_with_transition(
                    X_test.copy(), y_test.copy(),
                    from_activity, to_activity, mixing_ratio, augmentation_ratio
                )

                # 저장
                dataset_dir = output_path / dataset_name
                dataset_dir.mkdir(exist_ok=True)

                np.save(dataset_dir / 'X_train.npy', X_train_aug)
                np.save(dataset_dir / 'y_train.npy', y_train_aug)
                np.save(dataset_dir / 'X_test.npy', X_test_aug)
                np.save(dataset_dir / 'y_test.npy', y_test_aug)

                # 정보 저장
                dataset_info = {
                    'name': dataset_name,
                    'transition': transition_name,
                    'from_activity': from_activity,
                    'to_activity': to_activity,
                    'from_label': self.get_activity_code(from_activity),
                    'mixing_ratio': mixing_ratio,
                    'augmentation_ratio': augmentation_ratio,
                    'original_train_size': len(X_train),
                    'original_test_size': len(X_test),
                    'augmented_train_size': len(X_train_aug),
                    'augmented_test_size': len(X_test_aug),
                    'train_augmentation_count': train_count,
                    'test_augmentation_count': test_count
                }

                with open(dataset_dir / 'info.txt', 'w') as f:
                    f.write(f"Dataset: {dataset_name}\n")
                    f.write("=" * 60 + "\n\n")
                    f.write(f"Method: Sequence Splitting\n")
                    f.write(f"Transition: {from_activity} → {to_activity}\n")
                    f.write(f"Composition: Front {100-ratio_pct}% {from_activity} + Back {ratio_pct}% {to_activity}\n")
                    f.write(f"Label: {from_activity} (code: {dataset_info['from_label']})\n")
                    f.write(f"Augmentation: {int(augmentation_ratio*100)}%\n\n")
                    f.write(f"Train: {len(X_train)} → {len(X_train_aug)} (+{train_count})\n")
                    f.write(f"Test: {len(X_test)} → {len(X_test_aug)} (+{test_count})\n")
                    f.write(f"Shape: (samples, channels=9, timesteps=128)\n")

                metadata['datasets'].append(dataset_info)

                print(f"\n✓ Train: {len(X_train)} → {len(X_train_aug)} (+{train_count})")
                print(f"✓ Test: {len(X_test)} → {len(X_test_aug)} (+{test_count})")
                print(f"✓ Saved to: {dataset_dir}\n")

        # 메타데이터 저장
        with open(output_path / 'metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)

        # 요약 저장
        self._save_summary(output_path, metadata)

        print("\n" + "=" * 80)
        print("✓ All 41 datasets created successfully! (1 original + 40 augmented)")
        print("=" * 80)
        print(f"Output: {output_path.absolute()}")
        print(f"Summary: {output_path / 'summary.txt'}")

        return metadata

    def _save_summary(self, output_path, metadata):
        """요약 저장"""
        with open(output_path / 'summary.txt', 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("UCI-HAR TRANSITION DATASETS (Sequence Splitting Method)\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"Total datasets: 41\n")
            f.write(f"  - Original: 1 (no augmentation)\n")
            f.write(f"  - Augmented: 40 (with transitions)\n")
            f.write(f"Method: Sequence Splitting (Time-based)\n")
            f.write(f"Transitions: 10\n")
            f.write(f"  - Static transitions: 6 (STANDING↔SITTING↔LAYING)\n")
            f.write(f"  - Walking transitions: 4 (WALKING↔UPSTAIRS/DOWNSTAIRS)\n")
            f.write(f"Mixing ratios: 4 (10%, 20%, 30%, 40%)\n")
            f.write(f"Augmentation ratio: {int(metadata['augmentation_ratio']*100)}%\n")
            f.write(f"Data shape: (samples, channels=9, timesteps=128)\n")
            f.write(f"Labeling: Front class\n\n")

            f.write("=" * 80 + "\n")
            f.write("METHOD EXPLANATION\n")
            f.write("=" * 80 + "\n")
            f.write("Sequence Splitting:\n")
            f.write("  - 10%: Front 90% from class A + Back 10% from class B\n")
            f.write("  - 20%: Front 80% from class A + Back 20% from class B\n")
            f.write("  - 30%: Front 70% from class A + Back 30% from class B\n")
            f.write("  - 40%: Front 60% from class A + Back 40% from class B\n")
            f.write("  - Label: Always class A (front class)\n\n")

            f.write("Example (STANDING→SITTING 10%):\n")
            f.write("  [STAND STAND ... STAND | SIT SIT ... SIT]\n")
            f.write("   |------ 115 pts ------|  |--- 13 pts --|\n")
            f.write("   Label: STANDING (5)\n\n")

            f.write("=" * 80 + "\n")
            f.write("DATASET LIST\n")
            f.write("=" * 80 + "\n\n")

            for i, ds in enumerate(metadata['datasets'], 1):
                f.write(f"{i:2d}. {ds['name']}\n")

                # 원본 데이터셋 (전이 없음)인 경우
                if ds['name'] == 'ORIGINAL':
                    f.write(f"    Original dataset (no transition)\n")
                    f.write(f"    Train: {ds['original_train_size']} samples\n")
                    f.write(f"    Test: {ds['original_test_size']} samples\n\n")
                else:
                    # 전이 데이터셋인 경우
                    f.write(f"    {ds['from_activity']} → {ds['to_activity']}\n")
                    f.write(f"    Mix: Front {100-int(ds['mixing_ratio']*100)}% + Back {int(ds['mixing_ratio']*100)}%\n")
                    f.write(f"    Label: {ds['from_activity']} ({ds['from_label']})\n")
                    f.write(f"    Train: {ds['original_train_size']} → {ds['augmented_train_size']} "
                           f"(+{ds['train_augmentation_count']})\n")
                    f.write(f"    Test: {ds['original_test_size']} → {ds['augmented_test_size']} "
                           f"(+{ds['test_augmentation_count']})\n\n")


def main():
    """메인 실행"""
    import sys

    if len(sys.argv) < 2:
        print("=" * 80)
        print("UCI-HAR Transition Dataset Generator (Sequence Splitting)")
        print("=" * 80)
        print("\nUsage:")
        print("  python create_transition_datasets.py <UCI_HAR_path> [output] [aug_ratio]")
        print("\nArguments:")
        print("  UCI_HAR_path : UCI HAR Dataset path (required)")
        print("  output       : Output directory (default: ./datasets)")
        print("  aug_ratio    : Augmentation ratio (default: 0.10)")
        print("\nExamples:")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset")
        print("  python create_transition_datasets.py ./UCI_HAR_Dataset ./datasets 0.15")
        print("\nTransitions (10 types):")
        print("  Static: STANDING↔SITTING↔LAYING (6 transitions)")
        print("  Walking: WALKING↔UPSTAIRS/DOWNSTAIRS (4 transitions)")
        print("\nOutput:")
        print("  - 41 datasets total")
        print("    * 1 original (no transition)")
        print("    * 40 augmented (10 transitions × 4 split ratios)")
        print("\nMethod:")
        print("  - Sequence splitting (time-based)")
        print("  - Front X% + Back (100-X)%")
        print("  - Label: Front class")
        print("=" * 80)
        sys.exit(1)

    uci_data_path = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else './datasets'
    augmentation_ratio = float(sys.argv[3]) if len(sys.argv) > 3 else 0.10

    if not os.path.exists(uci_data_path):
        print(f"❌ Error: Path not found: {uci_data_path}")
        sys.exit(1)

    if augmentation_ratio <= 0 or augmentation_ratio >= 1:
        print(f"❌ Error: aug_ratio must be 0 < ratio < 1, got {augmentation_ratio}")
        sys.exit(1)

    print("\n" + "=" * 80)
    print("CONFIGURATION")
    print("=" * 80)
    print(f"UCI-HAR path: {uci_data_path}")
    print(f"Output: {output_dir}")
    print(f"Augmentation: {int(augmentation_ratio*100)}%")
    print(f"Method: Sequence Splitting")
    print("=" * 80 + "\n")

    creator = TransitionDatasetCreator(uci_data_path)
    metadata = creator.generate_all_datasets(output_dir, augmentation_ratio)

    print("\n✓ Completed!\n")


if __name__ == "__main__":
    main()