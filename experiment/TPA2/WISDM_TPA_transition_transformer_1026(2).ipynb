{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "supE8VPD0o0t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5268a6df-e8cb-45e8-d5ba-127f8ac3f654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "================================================================================\n",
            "UNIFIED MODEL COMPARISON: GAP vs TPA vs Gated-TPA\n",
            "WITH TRANSFORMER ENCODER BACKBONE\n",
            "Testing on 41 Datasets (1 Original + 40 Transition)\n",
            "================================================================================\n",
            "\n",
            "Total datasets to test: 33\n",
            "  - transitions: 33\n",
            "\n",
            "[Progress: 1/33]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: SITTING_TO_STANDING_40pct\n",
            "================================================================================\n",
            "\n",
            "Loading SITTING_TO_STANDING_40pct...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets/SITTING_TO_STANDING_40pct\n",
            "  Train: (24156, 200, 3), Test: (6040, 200, 3)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 19324, Val: 4832, Test: 6040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Training GAP]\n",
            "  Epoch  10: Train Acc=0.9063, Val Acc=0.8971, F1=0.8393\n",
            "  Epoch  20: Train Acc=0.9486, Val Acc=0.9361, F1=0.9063\n",
            "  Epoch  30: Train Acc=0.9635, Val Acc=0.9433, F1=0.9167\n",
            "  Epoch  40: Train Acc=0.9716, Val Acc=0.9489, F1=0.9221\n",
            "  Epoch  50: Train Acc=0.9777, Val Acc=0.9625, F1=0.9440\n",
            "  Epoch  60: Train Acc=0.9803, Val Acc=0.9592, F1=0.9396\n",
            "  Epoch  70: Train Acc=0.9835, Val Acc=0.9623, F1=0.9433\n",
            "  Epoch  80: Train Acc=0.9865, Val Acc=0.9563, F1=0.9306\n",
            "  Early stopping at epoch 87\n",
            "  Best Val Acc: 0.9654\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9654\n",
            "  Test Acc: 0.9674, F1: 0.9490\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9100, Val Acc=0.9120, F1=0.8632\n",
            "  Epoch  20: Train Acc=0.9488, Val Acc=0.9371, F1=0.9032\n",
            "  Epoch  30: Train Acc=0.9650, Val Acc=0.9557, F1=0.9319\n",
            "  Epoch  40: Train Acc=0.9719, Val Acc=0.9545, F1=0.9293\n",
            "  Epoch  50: Train Acc=0.9775, Val Acc=0.9572, F1=0.9343\n",
            "  Epoch  60: Train Acc=0.9815, Val Acc=0.9656, F1=0.9499\n",
            "  Epoch  70: Train Acc=0.9836, Val Acc=0.9526, F1=0.9275\n",
            "  Epoch  80: Train Acc=0.9869, Val Acc=0.9638, F1=0.9448\n",
            "  Early stopping at epoch 88\n",
            "  Best Val Acc: 0.9706\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9706\n",
            "  Test Acc: 0.9732, F1: 0.9566\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9124, Val Acc=0.9025, F1=0.8504\n",
            "  Epoch  20: Train Acc=0.9446, Val Acc=0.9427, F1=0.9122\n",
            "  Epoch  30: Train Acc=0.9623, Val Acc=0.9356, F1=0.9029\n",
            "  Epoch  40: Train Acc=0.9732, Val Acc=0.9570, F1=0.9365\n",
            "  Epoch  50: Train Acc=0.9777, Val Acc=0.9594, F1=0.9366\n",
            "  Epoch  60: Train Acc=0.9829, Val Acc=0.9690, F1=0.9522\n",
            "  Epoch  70: Train Acc=0.9872, Val Acc=0.9644, F1=0.9482\n",
            "  Epoch  80: Train Acc=0.9881, Val Acc=0.9659, F1=0.9495\n",
            "  Early stopping at epoch 85\n",
            "  Best Val Acc: 0.9754\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9754\n",
            "  Test Acc: 0.9724, F1: 0.9557\n",
            "\n",
            "[Progress: 2/33]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: STANDING_TO_WALKING_10pct\n",
            "================================================================================\n",
            "\n",
            "Loading STANDING_TO_WALKING_10pct...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets/STANDING_TO_WALKING_10pct\n",
            "  Train: (24156, 200, 3), Test: (6040, 200, 3)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 19324, Val: 4832, Test: 6040\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9108, Val Acc=0.9100, F1=0.8700\n",
            "  Epoch  20: Train Acc=0.9502, Val Acc=0.9325, F1=0.9069\n",
            "  Epoch  30: Train Acc=0.9645, Val Acc=0.9404, F1=0.9151\n",
            "  Epoch  40: Train Acc=0.9712, Val Acc=0.9582, F1=0.9416\n",
            "  Epoch  50: Train Acc=0.9754, Val Acc=0.9580, F1=0.9416\n",
            "  Epoch  60: Train Acc=0.9810, Val Acc=0.9644, F1=0.9505\n",
            "  Epoch  70: Train Acc=0.9825, Val Acc=0.9636, F1=0.9491\n",
            "  Epoch  80: Train Acc=0.9869, Val Acc=0.9698, F1=0.9571\n",
            "  Epoch  90: Train Acc=0.9892, Val Acc=0.9663, F1=0.9526\n",
            "  Epoch 100: Train Acc=0.9903, Val Acc=0.9634, F1=0.9509\n",
            "  Best Val Acc: 0.9708\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9708\n",
            "  Test Acc: 0.9697, F1: 0.9550\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9178, Val Acc=0.9133, F1=0.8724\n",
            "  Epoch  20: Train Acc=0.9532, Val Acc=0.9538, F1=0.9350\n",
            "  Epoch  30: Train Acc=0.9676, Val Acc=0.9580, F1=0.9425\n",
            "  Epoch  40: Train Acc=0.9750, Val Acc=0.9663, F1=0.9501\n",
            "  Epoch  50: Train Acc=0.9818, Val Acc=0.9758, F1=0.9660\n",
            "  Epoch  60: Train Acc=0.9840, Val Acc=0.9706, F1=0.9559\n",
            "  Epoch  70: Train Acc=0.9864, Val Acc=0.9776, F1=0.9673\n",
            "  Epoch  80: Train Acc=0.9880, Val Acc=0.9785, F1=0.9690\n",
            "  Epoch  90: Train Acc=0.9899, Val Acc=0.9779, F1=0.9697\n",
            "  Epoch 100: Train Acc=0.9923, Val Acc=0.9832, F1=0.9763\n",
            "  Best Val Acc: 0.9834\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9834\n",
            "  Test Acc: 0.9763, F1: 0.9646\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9125, Val Acc=0.9176, F1=0.8721\n",
            "  Epoch  20: Train Acc=0.9492, Val Acc=0.9259, F1=0.8970\n",
            "  Epoch  30: Train Acc=0.9621, Val Acc=0.9348, F1=0.9117\n",
            "  Epoch  40: Train Acc=0.9726, Val Acc=0.9567, F1=0.9395\n",
            "  Epoch  50: Train Acc=0.9792, Val Acc=0.9683, F1=0.9540\n",
            "  Epoch  60: Train Acc=0.9831, Val Acc=0.9716, F1=0.9605\n",
            "  Epoch  70: Train Acc=0.9853, Val Acc=0.9764, F1=0.9661\n",
            "  Epoch  80: Train Acc=0.9871, Val Acc=0.9733, F1=0.9596\n",
            "  Epoch  90: Train Acc=0.9901, Val Acc=0.9799, F1=0.9714\n",
            "  Epoch 100: Train Acc=0.9917, Val Acc=0.9756, F1=0.9641\n",
            "  Best Val Acc: 0.9805\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9805\n",
            "  Test Acc: 0.9763, F1: 0.9654\n",
            "\n",
            "[Progress: 3/33]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: STANDING_TO_WALKING_20pct\n",
            "================================================================================\n",
            "\n",
            "Loading STANDING_TO_WALKING_20pct...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets/STANDING_TO_WALKING_20pct\n",
            "  Train: (24156, 200, 3), Test: (6040, 200, 3)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 19324, Val: 4832, Test: 6040\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9096, Val Acc=0.9062, F1=0.8668\n",
            "  Epoch  20: Train Acc=0.9482, Val Acc=0.9272, F1=0.8980\n",
            "  Epoch  30: Train Acc=0.9619, Val Acc=0.9427, F1=0.9172\n",
            "  Epoch  40: Train Acc=0.9712, Val Acc=0.9507, F1=0.9286\n",
            "  Epoch  50: Train Acc=0.9757, Val Acc=0.9553, F1=0.9346\n",
            "  Epoch  60: Train Acc=0.9812, Val Acc=0.9609, F1=0.9421\n",
            "  Epoch  70: Train Acc=0.9837, Val Acc=0.9607, F1=0.9416\n",
            "  Epoch  80: Train Acc=0.9843, Val Acc=0.9615, F1=0.9449\n",
            "  Epoch  90: Train Acc=0.9869, Val Acc=0.9656, F1=0.9512\n",
            "  Epoch 100: Train Acc=0.9895, Val Acc=0.9692, F1=0.9559\n",
            "  Best Val Acc: 0.9692\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9692\n",
            "  Test Acc: 0.9672, F1: 0.9542\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9217, Val Acc=0.9218, F1=0.8823\n",
            "  Epoch  20: Train Acc=0.9555, Val Acc=0.9487, F1=0.9243\n",
            "  Epoch  30: Train Acc=0.9662, Val Acc=0.9576, F1=0.9362\n",
            "  Epoch  40: Train Acc=0.9739, Val Acc=0.9698, F1=0.9544\n",
            "  Epoch  50: Train Acc=0.9782, Val Acc=0.9731, F1=0.9593\n",
            "  Epoch  60: Train Acc=0.9835, Val Acc=0.9721, F1=0.9585\n",
            "  Epoch  70: Train Acc=0.9867, Val Acc=0.9760, F1=0.9646\n",
            "  Epoch  80: Train Acc=0.9893, Val Acc=0.9768, F1=0.9660\n",
            "  Epoch  90: Train Acc=0.9909, Val Acc=0.9770, F1=0.9669\n",
            "  Early stopping at epoch 93\n",
            "  Best Val Acc: 0.9785\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9785\n",
            "  Test Acc: 0.9737, F1: 0.9633\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9068, Val Acc=0.9114, F1=0.8604\n",
            "  Epoch  20: Train Acc=0.9441, Val Acc=0.9317, F1=0.8964\n",
            "  Epoch  30: Train Acc=0.9618, Val Acc=0.9478, F1=0.9241\n",
            "  Epoch  40: Train Acc=0.9711, Val Acc=0.9505, F1=0.9298\n",
            "  Epoch  50: Train Acc=0.9794, Val Acc=0.9675, F1=0.9529\n",
            "  Epoch  60: Train Acc=0.9815, Val Acc=0.9596, F1=0.9436\n",
            "  Epoch  70: Train Acc=0.9851, Val Acc=0.9654, F1=0.9496\n",
            "  Early stopping at epoch 78\n",
            "  Best Val Acc: 0.9737\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9737\n",
            "  Test Acc: 0.9724, F1: 0.9617\n",
            "\n",
            "[Progress: 4/33]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: STANDING_TO_WALKING_30pct\n",
            "================================================================================\n",
            "\n",
            "Loading STANDING_TO_WALKING_30pct...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets/STANDING_TO_WALKING_30pct\n",
            "  Train: (24156, 200, 3), Test: (6040, 200, 3)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 19324, Val: 4832, Test: 6040\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9045, Val Acc=0.9178, F1=0.8819\n",
            "  Epoch  20: Train Acc=0.9431, Val Acc=0.9431, F1=0.9192\n",
            "  Epoch  30: Train Acc=0.9595, Val Acc=0.9530, F1=0.9323\n",
            "  Epoch  40: Train Acc=0.9692, Val Acc=0.9520, F1=0.9270\n",
            "  Epoch  50: Train Acc=0.9725, Val Acc=0.9586, F1=0.9370\n",
            "  Epoch  60: Train Acc=0.9789, Val Acc=0.9576, F1=0.9359\n",
            "  Epoch  70: Train Acc=0.9816, Val Acc=0.9570, F1=0.9372\n",
            "  Epoch  80: Train Acc=0.9852, Val Acc=0.9567, F1=0.9376\n",
            "  Epoch  90: Train Acc=0.9883, Val Acc=0.9692, F1=0.9529\n",
            "  Epoch 100: Train Acc=0.9897, Val Acc=0.9665, F1=0.9475\n",
            "  Best Val Acc: 0.9733\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9733\n",
            "  Test Acc: 0.9671, F1: 0.9543\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9175, Val Acc=0.9259, F1=0.8889\n",
            "  Epoch  20: Train Acc=0.9539, Val Acc=0.9377, F1=0.9113\n",
            "  Epoch  30: Train Acc=0.9682, Val Acc=0.9543, F1=0.9347\n",
            "  Epoch  40: Train Acc=0.9747, Val Acc=0.9661, F1=0.9497\n",
            "  Epoch  50: Train Acc=0.9805, Val Acc=0.9719, F1=0.9582\n",
            "  Epoch  60: Train Acc=0.9843, Val Acc=0.9752, F1=0.9611\n",
            "  Epoch  70: Train Acc=0.9865, Val Acc=0.9785, F1=0.9664\n",
            "  Epoch  80: Train Acc=0.9887, Val Acc=0.9793, F1=0.9677\n",
            "  Epoch  90: Train Acc=0.9902, Val Acc=0.9723, F1=0.9553\n",
            "  Epoch 100: Train Acc=0.9927, Val Acc=0.9781, F1=0.9658\n",
            "  Best Val Acc: 0.9822\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9822\n",
            "  Test Acc: 0.9810, F1: 0.9727\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9044, Val Acc=0.9096, F1=0.8591\n",
            "  Epoch  20: Train Acc=0.9458, Val Acc=0.9269, F1=0.8994\n",
            "  Epoch  30: Train Acc=0.9650, Val Acc=0.9501, F1=0.9254\n",
            "  Epoch  40: Train Acc=0.9733, Val Acc=0.9590, F1=0.9426\n",
            "  Epoch  50: Train Acc=0.9790, Val Acc=0.9690, F1=0.9549\n",
            "  Epoch  60: Train Acc=0.9823, Val Acc=0.9683, F1=0.9529\n",
            "  Epoch  70: Train Acc=0.9870, Val Acc=0.9725, F1=0.9593\n",
            "  Epoch  80: Train Acc=0.9879, Val Acc=0.9646, F1=0.9490\n",
            "  Epoch  90: Train Acc=0.9886, Val Acc=0.9787, F1=0.9670\n",
            "  Epoch 100: Train Acc=0.9913, Val Acc=0.9781, F1=0.9677\n",
            "  Best Val Acc: 0.9818\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9818\n",
            "  Test Acc: 0.9783, F1: 0.9693\n",
            "\n",
            "[Progress: 5/33]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: STANDING_TO_WALKING_40pct\n",
            "================================================================================\n",
            "\n",
            "Loading STANDING_TO_WALKING_40pct...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets/STANDING_TO_WALKING_40pct\n",
            "  Train: (24156, 200, 3), Test: (6040, 200, 3)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 19324, Val: 4832, Test: 6040\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9013, Val Acc=0.9073, F1=0.8674\n",
            "  Epoch  20: Train Acc=0.9418, Val Acc=0.9259, F1=0.9001\n",
            "  Epoch  30: Train Acc=0.9566, Val Acc=0.9222, F1=0.9027\n",
            "  Epoch  40: Train Acc=0.9668, Val Acc=0.9588, F1=0.9424\n",
            "  Epoch  50: Train Acc=0.9702, Val Acc=0.9559, F1=0.9400\n",
            "  Epoch  60: Train Acc=0.9764, Val Acc=0.9642, F1=0.9508\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-440869016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[Progress: {i}/{len(datasets)}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-440869016.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(dataset_name, cfg)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Create and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-440869016.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, cfg, model_name)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-440869016.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, opt, cfg)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Warning] NaN/Inf loss detected, skipping batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"unified_transition_transformer.py\n",
        "Unified Model Comparison with Transformer Encoder Backbone\n",
        "Compare three models on 41 pre-augmented transition datasets\n",
        "(1 original + 40 transition datasets)\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/TPA2/wisdm_transition_datasets\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/TPA2\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    d_model: int = 128\n",
        "\n",
        "    # Transformer hyperparameters\n",
        "    num_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    ff_dim: int = 256\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ========================\n",
        "# Dataset Class\n",
        "# ========================\n",
        "class PreloadedDataset(Dataset):\n",
        "    \"\"\"Dataset for pre-loaded numpy arrays\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        super().__init__()\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "\n",
        "        # Label 범위 확인 및 조정 (1-6 -> 0-5)\n",
        "        if y.min() >= 1:\n",
        "            y = y - 1\n",
        "\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ========================\n",
        "# Data Loading Functions\n",
        "# ========================\n",
        "def load_dataset(base_dir: str, dataset_name: str):\n",
        "    \"\"\"\n",
        "    Load pre-augmented dataset\n",
        "    Args:\n",
        "        base_dir: base directory containing all datasets\n",
        "        dataset_name: e.g., \"ORIGINAL\", \"STANDING_TO_SITTING_10pct\", etc.\n",
        "    Returns:\n",
        "        train_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
        "\n",
        "    print(f\"\\nLoading {dataset_name}...\")\n",
        "    print(f\"  Path: {dataset_dir}\")\n",
        "\n",
        "    # Load data\n",
        "    X_train = np.load(os.path.join(dataset_dir, \"X_train.npy\"))\n",
        "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
        "    X_test = np.load(os.path.join(dataset_dir, \"X_test.npy\"))\n",
        "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
        "\n",
        "    print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    train_dataset = PreloadedDataset(X_train, y_train)\n",
        "    test_dataset = PreloadedDataset(X_test, y_test)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# ========================\n",
        "# Transformer Backbone Components\n",
        "# ========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, D]\n",
        "        Returns:\n",
        "            [B, T, D]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight Transformer Encoder Backbone\n",
        "    - 2 layers\n",
        "    - d_model=128\n",
        "    - n_heads=4\n",
        "    - ff_dim=256\n",
        "    - Dropout=0.1\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection: [B, C, T] -> [B, T, D]\n",
        "        self.input_projection = nn.Linear(in_channels, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-LN for better stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Output normalization\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, C, T] - input sensor data\n",
        "        Returns:\n",
        "            [B, T, D] - transformed sequence\n",
        "        \"\"\"\n",
        "        # [B, C, T] -> [B, T, C]\n",
        "        # x = x.transpose(1, 2)\n",
        "\n",
        "        # Project to d_model: [B, T, C] -> [B, T, D]\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding: [B, T, D]\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Transformer encoding: [B, T, D]\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Final normalization: [B, T, D]\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ========================\n",
        "# GAP Model with Transformer\n",
        "# ========================\n",
        "class GAPModel(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling with Transformer Backbone\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 6):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        pooled = features.mean(dim=1)  # [B, D]\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Pure-TPA with Transformer\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Pure TPA\"\"\"\n",
        "    def __init__(self, dim, num_prototypes=16, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_norm = self.pre_norm(x)\n",
        "\n",
        "        K = self.k_proj(x_norm)\n",
        "        V = self.v_proj(x_norm)\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        z_tpa = proto_tokens.mean(dim=1)\n",
        "\n",
        "        z = self.fuse(z_tpa)\n",
        "\n",
        "        return z\n",
        "\n",
        "class TPAModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 6,\n",
        "                 tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        z = self.tpa(features)  # [B, D]\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Gated-TPA with Transformer\n",
        "# ========================\n",
        "class GatedTPAModel(nn.Module):\n",
        "    \"\"\"Gated-TPA: Hybrid of TPA and GAP\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 3,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 6,\n",
        "                 tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "\n",
        "        # TPA path\n",
        "        z_tpa = self.tpa(features)  # [B, D]\n",
        "\n",
        "        # GAP path\n",
        "        z_gap = features.mean(dim=1)  # [B, D]\n",
        "\n",
        "        # Gating\n",
        "        gate_input = torch.cat([z_tpa, z_gap], dim=-1)  # [B, 2D]\n",
        "        alpha = self.gate(gate_input)  # [B, D]\n",
        "\n",
        "        # Combine\n",
        "        z = alpha * z_tpa + (1 - alpha) * z_gap  # [B, D]\n",
        "\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Training & Evaluation\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    correct, total = 0, 0\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"[Warning] NaN/Inf loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "def train_model(model, train_loader, val_loader, cfg: Config, model_name: str):\n",
        "    \"\"\"Train a single model\"\"\"\n",
        "    print(f\"\\n[Training {model_name}]\")\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    best_acc, best_wts = 0.0, None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "        val_acc, val_f1 = evaluate(model, val_loader, cfg)\n",
        "\n",
        "        if val_acc > best_acc + cfg.min_delta:\n",
        "            best_acc = val_acc\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"  Epoch {epoch:3d}: Train Acc={stats['acc']:.4f}, Val Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
        "\n",
        "        if patience_counter >= cfg.patience:\n",
        "            print(f\"  Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_wts:\n",
        "        model.load_state_dict(best_wts)\n",
        "\n",
        "    print(f\"  Best Val Acc: {best_acc:.4f}\")\n",
        "    return best_acc\n",
        "\n",
        "def create_model(model_name: str, cfg: Config):\n",
        "    \"\"\"Create model by name\"\"\"\n",
        "    tpa_config = {\n",
        "        'num_prototypes': cfg.tpa_num_prototypes,\n",
        "        'heads': cfg.tpa_heads,\n",
        "        'dropout': cfg.tpa_dropout,\n",
        "        'temperature': cfg.tpa_temperature,\n",
        "        'topk_ratio': cfg.tpa_topk_ratio\n",
        "    }\n",
        "\n",
        "    if model_name == \"GAP\":\n",
        "        return GAPModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout\n",
        "        ).to(cfg.device).float()\n",
        "    elif model_name == \"TPA\":\n",
        "        return TPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "    elif model_name == \"Gated-TPA\":\n",
        "        return GatedTPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            num_layers=cfg.num_layers,\n",
        "            n_heads=cfg.n_heads,\n",
        "            ff_dim=cfg.ff_dim,\n",
        "            dropout=cfg.dropout,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "# ========================\n",
        "# Main Experiment\n",
        "# ========================\n",
        "def run_experiment(dataset_name: str, cfg: Config):\n",
        "    \"\"\"Run complete experiment for one dataset\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXPERIMENT: {dataset_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load data\n",
        "    train_dataset, test_dataset = load_dataset(cfg.data_dir, dataset_name)\n",
        "\n",
        "    # Split train into train/val using indices\n",
        "    n_total = len(train_dataset)\n",
        "    indices = np.arange(n_total)\n",
        "\n",
        "    # Get labels for stratification\n",
        "    y_labels = train_dataset.y.numpy()\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "\n",
        "    # Create subsets using Subset\n",
        "    from torch.utils.data import Subset\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_subset, cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, generator=g)\n",
        "    val_loader = DataLoader(val_subset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    print(f\"\\nDataset splits:\")\n",
        "    print(f\"  Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Train and evaluate all models\n",
        "    results = []\n",
        "    model_names = [\"GAP\", \"TPA\", \"Gated-TPA\"]\n",
        "\n",
        "    for model_name in model_names:\n",
        "        # Reset seed for each model\n",
        "        random.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "\n",
        "        # Create and train model\n",
        "        model = create_model(model_name, cfg)\n",
        "        best_val_acc = train_model(model, train_loader, val_loader, cfg, model_name)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_acc, test_f1 = evaluate(model, test_loader, cfg)\n",
        "\n",
        "        print(f\"\\n[{model_name} Results]\")\n",
        "        print(f\"  Val Acc: {best_val_acc:.4f}\")\n",
        "        print(f\"  Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Dataset': dataset_name,\n",
        "            'Val_Accuracy': float(best_val_acc),\n",
        "            'Test_Accuracy': float(test_acc),\n",
        "            'Test_F1_Score': float(test_f1)\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# ========================\n",
        "# Run All Experiments\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"UNIFIED MODEL COMPARISON: GAP vs TPA vs Gated-TPA\")\n",
        "    print(\"WITH TRANSFORMER ENCODER BACKBONE\")\n",
        "    print(\"Testing on 41 Datasets (1 Original + 40 Transition)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    datasets = [\"SITTING_TO_STANDING_40pct\"]\n",
        "\n",
        "    transitions = [\n",
        "        \"STANDING_TO_WALKING\",\n",
        "        \"WALKING_TO_STANDING\",\n",
        "        \"WALKING_TO_JOGGING\",\n",
        "        \"JOGGING_TO_WALKING\",\n",
        "        \"WALKING_TO_UPSTAIRS\",\n",
        "        \"WALKING_TO_DOWNSTAIRS\",\n",
        "        \"UPSTAIRS_TO_WALKING\",\n",
        "        \"DOWNSTAIRS_TO_WALKING\"\n",
        "    ]\n",
        "\n",
        "    # 모든 전이에 대해 10%, 20%, 30%, 40% 추가\n",
        "    mix_pcts = [10, 20, 30, 40]\n",
        "\n",
        "    for transition in transitions:\n",
        "        for pct in mix_pcts:\n",
        "            datasets.append(f\"{transition}_{pct}pct\")\n",
        "\n",
        "    print(f\"\\nTotal datasets to test: {len(datasets)}\")\n",
        "    print(f\"  - transitions: {len(transitions) * len(mix_pcts) + 1}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Run experiments\n",
        "    for i, dataset_name in enumerate(datasets, 1):\n",
        "        print(f\"\\n[Progress: {i}/{len(datasets)}]\")\n",
        "        results = run_experiment(dataset_name, cfg)\n",
        "        all_results.extend(results)\n",
        "\n",
        "    # Save all results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SAVING RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    results_dict = {\n",
        "        'experiment_info': {\n",
        "            'date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'models': ['GAP', 'TPA', 'Gated-TPA'],\n",
        "            'backbone': 'Transformer Encoder',\n",
        "            'total_datasets': len(datasets),\n",
        "            'datasets': datasets,\n",
        "            'config': {\n",
        "                'epochs': cfg.epochs,\n",
        "                'batch_size': cfg.batch_size,\n",
        "                'lr': cfg.lr,\n",
        "                'd_model': cfg.d_model,\n",
        "                'num_layers': cfg.num_layers,\n",
        "                'n_heads': cfg.n_heads,\n",
        "                'ff_dim': cfg.ff_dim,\n",
        "                'dropout': cfg.dropout,\n",
        "                'tpa_num_prototypes': cfg.tpa_num_prototypes,\n",
        "                'tpa_heads': cfg.tpa_heads\n",
        "            }\n",
        "        },\n",
        "        'results': all_results\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    json_path = os.path.join(cfg.save_dir, \"unified_transformer_41datasets_results.json\")\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_dict, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved to: {json_path}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total experiments: {len(all_results)}\")\n",
        "    print(f\"Total datasets tested: {len(datasets)}\")\n",
        "    print(f\"Models compared: 3 (GAP, TPA, Gated-TPA)\")\n",
        "\n",
        "    # Calculate average performance per model\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"AVERAGE PERFORMANCE (All Datasets)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for model_name in ['GAP', 'TPA', 'Gated-TPA']:\n",
        "        model_results = [r for r in all_results if r['Model'] == model_name]\n",
        "        avg_acc = np.mean([r['Test_Accuracy'] for r in model_results])\n",
        "        avg_f1 = np.mean([r['Test_F1_Score'] for r in model_results])\n",
        "        print(f\"{model_name:12s}: Acc={avg_acc:.4f}, F1={avg_f1:.4f}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(f\"{'='*80}\")"
      ]
    }
  ]
}