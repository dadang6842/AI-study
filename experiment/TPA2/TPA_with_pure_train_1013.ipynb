{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- train 데이터에 전이 증강을 하지 않은 버전"
      ],
      "metadata": {
        "id": "tAmB69QC0xkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNt2x8lY0s3R",
        "outputId": "0217c2b1-606b-45eb-916e-410685f04834"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALL-fcWB0aY1",
        "outputId": "bd93d43c-b940-45a0-c682-e97162e1f8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "    UCI-HAR 4-WAY ABLATION STUDY\n",
            "    (NO TRAINING AUGMENTATION VERSION)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "\n",
            "⚠️  IMPORTANT: NO data augmentation during training\n",
            "   Models trained on CLEAN data only\n",
            "   Transitions applied ONLY to test sets\n",
            "\n",
            "4 Configurations to Compare:\n",
            "  1) GAP_NoMask:    Standard GAP (baseline)\n",
            "  2) GAP_WithMask:  GAP + mask filtering\n",
            "  3) TPA_NoMask:    Prototype Attention only\n",
            "  4) TPA_WithMask:  Prototype Attention + mask filtering\n",
            "\n",
            "This measures:\n",
            "  • Effect of mask alone (without augmentation training)\n",
            "  • Effect of TPA alone (intrinsic robustness)\n",
            "  • Effect of TPA + mask combined\n",
            "  • Whether there's synergy between them\n",
            "\n",
            "TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total train samples: 7352\n",
            "  → Train split: 5881 (80%)\n",
            "  → Val split:   1471 (20%)\n",
            "\n",
            "======================================================================\n",
            "    4-WAY ABLATION STUDY: GAP/TPA × NoMask/WithMask\n",
            "======================================================================\n",
            "   Training: NO augmentation (clean data only)\n",
            "   Testing: Extreme transitions on test set\n",
            "   Scenarios: 17 transition configurations\n",
            "======================================================================\n",
            "\n",
            "  [1/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Modified: 743/2947 (25.2%)\n",
            "  [2/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Modified: 749/2947 (25.4%)\n",
            "  [3/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Modified: 708/2947 (24.0%)\n",
            "  [4/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Modified: 713/2947 (24.2%)\n",
            "  [5/17] SITTING ↔ LAYING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [6/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Modified: 747/2947 (25.3%)\n",
            "  [7/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Modified: 674/2947 (22.9%)\n",
            "  [8/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Modified: 670/2947 (22.7%)\n",
            "  [9/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [10/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Modified: 752/2947 (25.5%)\n",
            "  [11/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Modified: 795/2947 (27.0%)\n",
            "  [12/17] WALKING ↔ WALKING_UPSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Modified: 758/2947 (25.7%)\n",
            "  [13/17] SITTING ↔ LAYING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Modified: 800/2947 (27.1%)\n",
            "  [14/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Modified: 718/2947 (24.4%)\n",
            "  [15/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Modified: 803/2947 (27.2%)\n",
            "  [16/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, middle, seg=2)\n",
            "      Modified: 672/2947 (22.8%)\n",
            "  [17/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, middle, seg=2)\n",
            "      Modified: 745/2947 (25.3%)\n",
            "\n",
            "✓ 17 transitional test sets created.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP_NoMask\n",
            "   Pooling: GAP | Mask: No\n",
            "======================================================================\n",
            "\n",
            "Training GAP_NoMask (NO augmentation, clean data only)...\n",
            "Max epochs: 100, patience: 20\n",
            "[01/100] Train L:1.4400 A:0.5513 | Val A:0.8328 F1:0.8201 ✓\n",
            "[02/100] Train L:0.9501 A:0.8657 | Val A:0.8980 F1:0.8978 ✓\n",
            "[03/100] Train L:0.7441 A:0.9072 | Val A:0.9279 F1:0.9293 ✓\n",
            "[04/100] Train L:0.6211 A:0.9259 | Val A:0.9320 F1:0.9349 ✓\n",
            "[05/100] Train L:0.5420 A:0.9267 | Val A:0.9388 F1:0.9415 ✓\n",
            "[06/100] Train L:0.4886 A:0.9340 | Val A:0.9402 F1:0.9437 ✓\n",
            "[07/100] Train L:0.4508 A:0.9386 | Val A:0.9409 F1:0.9444 ✓\n",
            "[08/100] Train L:0.4294 A:0.9371 | Val A:0.9415 F1:0.9450 ✓\n",
            "[09/100] Train L:0.4124 A:0.9408 | Val A:0.9395 F1:0.9432\n",
            "[10/100] Train L:0.3988 A:0.9437 | Val A:0.9381 F1:0.9419\n",
            "[11/100] Train L:0.3933 A:0.9442 | Val A:0.9422 F1:0.9457 ✓\n",
            "[12/100] Train L:0.3852 A:0.9442 | Val A:0.9443 F1:0.9477 ✓\n",
            "[13/100] Train L:0.3815 A:0.9442 | Val A:0.9443 F1:0.9479\n",
            "[14/100] Train L:0.3747 A:0.9458 | Val A:0.9545 F1:0.9578 ✓\n",
            "[15/100] Train L:0.3728 A:0.9464 | Val A:0.9490 F1:0.9527\n",
            "[16/100] Train L:0.3721 A:0.9488 | Val A:0.9504 F1:0.9541\n",
            "[17/100] Train L:0.3682 A:0.9485 | Val A:0.9511 F1:0.9545\n",
            "[18/100] Train L:0.3670 A:0.9512 | Val A:0.9524 F1:0.9560\n",
            "[19/100] Train L:0.3649 A:0.9466 | Val A:0.9585 F1:0.9617 ✓\n",
            "[20/100] Train L:0.3640 A:0.9493 | Val A:0.9565 F1:0.9598\n",
            "[21/100] Train L:0.3599 A:0.9498 | Val A:0.9504 F1:0.9540\n",
            "[22/100] Train L:0.3627 A:0.9485 | Val A:0.9538 F1:0.9573\n",
            "[23/100] Train L:0.3594 A:0.9490 | Val A:0.9511 F1:0.9547\n",
            "[24/100] Train L:0.3574 A:0.9468 | Val A:0.9517 F1:0.9553\n",
            "[25/100] Train L:0.3584 A:0.9478 | Val A:0.9579 F1:0.9611\n",
            "[26/100] Train L:0.3550 A:0.9524 | Val A:0.9538 F1:0.9573\n",
            "[27/100] Train L:0.3553 A:0.9522 | Val A:0.9613 F1:0.9642 ✓\n",
            "[28/100] Train L:0.3538 A:0.9493 | Val A:0.9572 F1:0.9604\n",
            "[29/100] Train L:0.3513 A:0.9510 | Val A:0.9524 F1:0.9560\n",
            "[30/100] Train L:0.3518 A:0.9512 | Val A:0.9517 F1:0.9553\n",
            "[31/100] Train L:0.3513 A:0.9500 | Val A:0.9558 F1:0.9591\n",
            "[32/100] Train L:0.3542 A:0.9483 | Val A:0.9558 F1:0.9590\n",
            "[33/100] Train L:0.3498 A:0.9526 | Val A:0.9531 F1:0.9567\n",
            "[34/100] Train L:0.3519 A:0.9515 | Val A:0.9599 F1:0.9630\n",
            "[35/100] Train L:0.3501 A:0.9517 | Val A:0.9545 F1:0.9579\n",
            "[36/100] Train L:0.3472 A:0.9536 | Val A:0.9619 F1:0.9649 ✓\n",
            "[37/100] Train L:0.3502 A:0.9520 | Val A:0.9558 F1:0.9591\n",
            "[38/100] Train L:0.3502 A:0.9522 | Val A:0.9551 F1:0.9583\n",
            "[39/100] Train L:0.3505 A:0.9514 | Val A:0.9579 F1:0.9610\n",
            "[40/100] Train L:0.3486 A:0.9526 | Val A:0.9565 F1:0.9598\n",
            "[41/100] Train L:0.3454 A:0.9527 | Val A:0.9558 F1:0.9592\n",
            "[42/100] Train L:0.3466 A:0.9520 | Val A:0.9558 F1:0.9591\n",
            "[43/100] Train L:0.3442 A:0.9529 | Val A:0.9585 F1:0.9617\n",
            "[44/100] Train L:0.3455 A:0.9500 | Val A:0.9579 F1:0.9611\n",
            "[45/100] Train L:0.3448 A:0.9522 | Val A:0.9572 F1:0.9603\n",
            "[46/100] Train L:0.3440 A:0.9502 | Val A:0.9572 F1:0.9604\n",
            "[47/100] Train L:0.3438 A:0.9514 | Val A:0.9592 F1:0.9623\n",
            "[48/100] Train L:0.3427 A:0.9510 | Val A:0.9619 F1:0.9649\n",
            "[49/100] Train L:0.3422 A:0.9539 | Val A:0.9606 F1:0.9637\n",
            "[50/100] Train L:0.3451 A:0.9510 | Val A:0.9619 F1:0.9649\n",
            "[51/100] Train L:0.3416 A:0.9510 | Val A:0.9606 F1:0.9636\n",
            "[52/100] Train L:0.3405 A:0.9519 | Val A:0.9599 F1:0.9630\n",
            "[53/100] Train L:0.3420 A:0.9520 | Val A:0.9572 F1:0.9604\n",
            "[54/100] Train L:0.3407 A:0.9534 | Val A:0.9606 F1:0.9636\n",
            "[55/100] Train L:0.3420 A:0.9526 | Val A:0.9599 F1:0.9630\n",
            "[56/100] Train L:0.3391 A:0.9524 | Val A:0.9585 F1:0.9617\n",
            "\n",
            "⚠ Early stopping triggered at epoch 56\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9619 (epoch 36)\n",
            "\n",
            "✓ Best Val Acc: 0.9619 (epoch 36)\n",
            "  Original Test Acc: 0.9118, F1: 0.9130\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8276 Drop=0.0842 [Vulnerable]\n",
            "    Scenario 2: Acc=0.8846 Drop=0.0271 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.7733 Drop=0.1384 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9016 Drop=0.0102 [Very Robust]\n",
            "    Scenario 5: Acc=0.7475 Drop=0.1642 [Vulnerable]\n",
            "    Scenario 6: Acc=0.9223 Drop=-0.0105 [Very Robust]\n",
            "    Scenario 7: Acc=0.7655 Drop=0.1463 [Vulnerable]\n",
            "    Scenario 8: Acc=0.9002 Drop=0.0115 [Very Robust]\n",
            "    Scenario 9: Acc=0.8327 Drop=0.0791 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8877 Drop=0.0241 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.8130 Drop=0.0987 [Vulnerable]\n",
            "    Scenario 12: Acc=0.7346 Drop=0.1771 [Vulnerable]\n",
            "    Scenario 13: Acc=0.7021 Drop=0.2097 [Vulnerable]\n",
            "    Scenario 14: Acc=0.7282 Drop=0.1836 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8103 Drop=0.1015 [Vulnerable]\n",
            "    Scenario 16: Acc=0.7706 Drop=0.1412 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9203 Drop=-0.0085 [Very Robust]\n",
            "\n",
            " GAP_NoMask Summary:\n",
            "   Original Test:      0.9118\n",
            "   Avg Transition:     0.8190\n",
            "   Avg Drop:           0.0928\n",
            "   Retention:          89.82%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP_WithMask\n",
            "   Pooling: GAP | Mask: Yes\n",
            "======================================================================\n",
            "\n",
            "Training GAP_WithMask (NO augmentation, clean data only)...\n",
            "Max epochs: 100, patience: 20\n",
            "[01/100] Train L:1.4400 A:0.5513 | Val A:0.8328 F1:0.8201 ✓\n",
            "[02/100] Train L:0.9501 A:0.8657 | Val A:0.8980 F1:0.8978 ✓\n",
            "[03/100] Train L:0.7441 A:0.9072 | Val A:0.9279 F1:0.9293 ✓\n",
            "[04/100] Train L:0.6211 A:0.9259 | Val A:0.9320 F1:0.9349 ✓\n",
            "[05/100] Train L:0.5420 A:0.9267 | Val A:0.9388 F1:0.9415 ✓\n",
            "[06/100] Train L:0.4886 A:0.9340 | Val A:0.9402 F1:0.9437 ✓\n",
            "[07/100] Train L:0.4508 A:0.9386 | Val A:0.9409 F1:0.9444 ✓\n",
            "[08/100] Train L:0.4294 A:0.9371 | Val A:0.9415 F1:0.9450 ✓\n",
            "[09/100] Train L:0.4124 A:0.9408 | Val A:0.9395 F1:0.9432\n",
            "[10/100] Train L:0.3988 A:0.9437 | Val A:0.9381 F1:0.9419\n",
            "[11/100] Train L:0.3933 A:0.9442 | Val A:0.9422 F1:0.9457 ✓\n",
            "[12/100] Train L:0.3852 A:0.9442 | Val A:0.9443 F1:0.9477 ✓\n",
            "[13/100] Train L:0.3815 A:0.9442 | Val A:0.9443 F1:0.9479\n",
            "[14/100] Train L:0.3747 A:0.9458 | Val A:0.9545 F1:0.9578 ✓\n",
            "[15/100] Train L:0.3728 A:0.9464 | Val A:0.9490 F1:0.9527\n",
            "[16/100] Train L:0.3721 A:0.9488 | Val A:0.9504 F1:0.9541\n",
            "[17/100] Train L:0.3682 A:0.9485 | Val A:0.9511 F1:0.9545\n",
            "[18/100] Train L:0.3670 A:0.9512 | Val A:0.9524 F1:0.9560\n",
            "[19/100] Train L:0.3649 A:0.9466 | Val A:0.9585 F1:0.9617 ✓\n",
            "[20/100] Train L:0.3640 A:0.9493 | Val A:0.9565 F1:0.9598\n",
            "[21/100] Train L:0.3599 A:0.9498 | Val A:0.9504 F1:0.9540\n",
            "[22/100] Train L:0.3627 A:0.9485 | Val A:0.9538 F1:0.9573\n",
            "[23/100] Train L:0.3594 A:0.9490 | Val A:0.9511 F1:0.9547\n",
            "[24/100] Train L:0.3574 A:0.9468 | Val A:0.9517 F1:0.9553\n",
            "[25/100] Train L:0.3584 A:0.9478 | Val A:0.9579 F1:0.9611\n",
            "[26/100] Train L:0.3550 A:0.9524 | Val A:0.9538 F1:0.9573\n",
            "[27/100] Train L:0.3553 A:0.9522 | Val A:0.9613 F1:0.9642 ✓\n",
            "[28/100] Train L:0.3538 A:0.9493 | Val A:0.9572 F1:0.9604\n",
            "[29/100] Train L:0.3513 A:0.9510 | Val A:0.9524 F1:0.9560\n",
            "[30/100] Train L:0.3518 A:0.9512 | Val A:0.9517 F1:0.9553\n",
            "[31/100] Train L:0.3513 A:0.9500 | Val A:0.9558 F1:0.9591\n",
            "[32/100] Train L:0.3542 A:0.9483 | Val A:0.9558 F1:0.9590\n",
            "[33/100] Train L:0.3498 A:0.9526 | Val A:0.9531 F1:0.9567\n",
            "[34/100] Train L:0.3519 A:0.9515 | Val A:0.9599 F1:0.9630\n",
            "[35/100] Train L:0.3501 A:0.9517 | Val A:0.9545 F1:0.9579\n",
            "[36/100] Train L:0.3472 A:0.9536 | Val A:0.9619 F1:0.9649 ✓\n",
            "[37/100] Train L:0.3502 A:0.9520 | Val A:0.9558 F1:0.9591\n",
            "[38/100] Train L:0.3502 A:0.9522 | Val A:0.9551 F1:0.9583\n",
            "[39/100] Train L:0.3505 A:0.9514 | Val A:0.9579 F1:0.9610\n",
            "[40/100] Train L:0.3486 A:0.9526 | Val A:0.9565 F1:0.9598\n",
            "[41/100] Train L:0.3454 A:0.9527 | Val A:0.9558 F1:0.9592\n",
            "[42/100] Train L:0.3466 A:0.9520 | Val A:0.9558 F1:0.9591\n",
            "[43/100] Train L:0.3442 A:0.9529 | Val A:0.9585 F1:0.9617\n",
            "[44/100] Train L:0.3455 A:0.9500 | Val A:0.9579 F1:0.9611\n",
            "[45/100] Train L:0.3448 A:0.9522 | Val A:0.9572 F1:0.9603\n",
            "[46/100] Train L:0.3440 A:0.9502 | Val A:0.9572 F1:0.9604\n",
            "[47/100] Train L:0.3438 A:0.9514 | Val A:0.9592 F1:0.9623\n",
            "[48/100] Train L:0.3427 A:0.9510 | Val A:0.9619 F1:0.9649\n",
            "[49/100] Train L:0.3422 A:0.9539 | Val A:0.9606 F1:0.9637\n",
            "[50/100] Train L:0.3451 A:0.9510 | Val A:0.9619 F1:0.9649\n",
            "[51/100] Train L:0.3416 A:0.9510 | Val A:0.9606 F1:0.9636\n",
            "[52/100] Train L:0.3405 A:0.9519 | Val A:0.9599 F1:0.9630\n",
            "[53/100] Train L:0.3420 A:0.9520 | Val A:0.9572 F1:0.9604\n",
            "[54/100] Train L:0.3407 A:0.9534 | Val A:0.9606 F1:0.9636\n",
            "[55/100] Train L:0.3420 A:0.9526 | Val A:0.9599 F1:0.9630\n",
            "[56/100] Train L:0.3391 A:0.9524 | Val A:0.9585 F1:0.9617\n",
            "\n",
            "⚠ Early stopping triggered at epoch 56\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9619 (epoch 36)\n",
            "\n",
            "✓ Best Val Acc: 0.9619 (epoch 36)\n",
            "  Original Test Acc: 0.9118, F1: 0.9130\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9138 Drop=-0.0020 [Very Robust]\n",
            "    Scenario 2: Acc=0.9114 Drop=0.0003 [Very Robust]\n",
            "    Scenario 3: Acc=0.9101 Drop=0.0017 [Very Robust]\n",
            "    Scenario 4: Acc=0.9094 Drop=0.0024 [Very Robust]\n",
            "    Scenario 5: Acc=0.9128 Drop=-0.0010 [Very Robust]\n",
            "    Scenario 6: Acc=0.9131 Drop=-0.0014 [Very Robust]\n",
            "    Scenario 7: Acc=0.9104 Drop=0.0014 [Very Robust]\n",
            "    Scenario 8: Acc=0.9101 Drop=0.0017 [Very Robust]\n",
            "    Scenario 9: Acc=0.9121 Drop=-0.0003 [Very Robust]\n",
            "    Scenario 10: Acc=0.9111 Drop=0.0007 [Very Robust]\n",
            "    Scenario 11: Acc=0.9114 Drop=0.0003 [Very Robust]\n",
            "    Scenario 12: Acc=0.9091 Drop=0.0027 [Very Robust]\n",
            "    Scenario 13: Acc=0.9131 Drop=-0.0014 [Very Robust]\n",
            "    Scenario 14: Acc=0.9101 Drop=0.0017 [Very Robust]\n",
            "    Scenario 15: Acc=0.9128 Drop=-0.0010 [Very Robust]\n",
            "    Scenario 16: Acc=0.9091 Drop=0.0027 [Very Robust]\n",
            "    Scenario 17: Acc=0.9125 Drop=-0.0007 [Very Robust]\n",
            "\n",
            " GAP_WithMask Summary:\n",
            "   Original Test:      0.9118\n",
            "   Avg Transition:     0.9113\n",
            "   Avg Drop:           0.0005\n",
            "   Retention:          99.95%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: TPA_NoMask\n",
            "   Pooling: TPA | Mask: No\n",
            "======================================================================\n",
            "\n",
            "Training TPA_NoMask (NO augmentation, clean data only)...\n",
            "Max epochs: 100, patience: 20\n",
            "TPA: prototypes=16, heads=4, temp=0.07\n",
            "[01/100] Train L:1.4324 A:0.5553 | Val A:0.6785 F1:0.6030 | Conf:0.483 ✓\n",
            "[02/100] Train L:0.8788 A:0.8136 | Val A:0.8919 F1:0.8907 | Conf:0.382 ✓\n",
            "[03/100] Train L:0.6314 A:0.8927 | Val A:0.8994 F1:0.8985 | Conf:0.372 ✓\n",
            "[04/100] Train L:0.4875 A:0.9257 | Val A:0.9511 F1:0.9539 | Conf:0.390 ✓\n",
            "[05/100] Train L:0.4331 A:0.9396 | Val A:0.9483 F1:0.9517 | Conf:0.416\n",
            "[06/100] Train L:0.4009 A:0.9480 | Val A:0.9517 F1:0.9553 | Conf:0.428 ✓\n",
            "[07/100] Train L:0.3801 A:0.9492 | Val A:0.9517 F1:0.9553 | Conf:0.432\n",
            "[08/100] Train L:0.3756 A:0.9469 | Val A:0.9613 F1:0.9642 | Conf:0.432 ✓\n",
            "[09/100] Train L:0.3635 A:0.9500 | Val A:0.9585 F1:0.9617 | Conf:0.430\n",
            "[10/100] Train L:0.3589 A:0.9527 | Val A:0.9565 F1:0.9595 | Conf:0.432\n",
            "[11/100] Train L:0.3588 A:0.9493 | Val A:0.9606 F1:0.9636 | Conf:0.427\n",
            "[12/100] Train L:0.3514 A:0.9512 | Val A:0.9606 F1:0.9636 | Conf:0.431\n",
            "[13/100] Train L:0.3500 A:0.9536 | Val A:0.9626 F1:0.9654 | Conf:0.431 ✓\n",
            "[14/100] Train L:0.3439 A:0.9536 | Val A:0.9660 F1:0.9686 | Conf:0.434 ✓\n",
            "[15/100] Train L:0.3435 A:0.9561 | Val A:0.9646 F1:0.9674 | Conf:0.436\n",
            "[16/100] Train L:0.3415 A:0.9563 | Val A:0.9592 F1:0.9624 | Conf:0.443\n",
            "[17/100] Train L:0.3395 A:0.9554 | Val A:0.9640 F1:0.9667 | Conf:0.443\n",
            "[18/100] Train L:0.3378 A:0.9553 | Val A:0.9653 F1:0.9680 | Conf:0.445\n",
            "[19/100] Train L:0.3365 A:0.9551 | Val A:0.9579 F1:0.9611 | Conf:0.450\n",
            "[20/100] Train L:0.3334 A:0.9575 | Val A:0.9640 F1:0.9668 | Conf:0.447\n",
            "[21/100] Train L:0.3286 A:0.9607 | Val A:0.9633 F1:0.9660 | Conf:0.453\n",
            "[22/100] Train L:0.3301 A:0.9589 | Val A:0.9613 F1:0.9641 | Conf:0.454\n",
            "[23/100] Train L:0.3248 A:0.9600 | Val A:0.9667 F1:0.9692 | Conf:0.452 ✓\n",
            "[24/100] Train L:0.3273 A:0.9609 | Val A:0.9640 F1:0.9668 | Conf:0.456\n",
            "[25/100] Train L:0.3247 A:0.9609 | Val A:0.9599 F1:0.9630 | Conf:0.455\n",
            "[26/100] Train L:0.3163 A:0.9668 | Val A:0.9701 F1:0.9723 | Conf:0.458 ✓\n",
            "[27/100] Train L:0.3252 A:0.9634 | Val A:0.9694 F1:0.9718 | Conf:0.456\n",
            "[28/100] Train L:0.3127 A:0.9663 | Val A:0.9680 F1:0.9704 | Conf:0.458\n",
            "[29/100] Train L:0.3093 A:0.9697 | Val A:0.9714 F1:0.9736 | Conf:0.459 ✓\n",
            "[30/100] Train L:0.3109 A:0.9694 | Val A:0.9701 F1:0.9724 | Conf:0.460\n",
            "[31/100] Train L:0.3062 A:0.9721 | Val A:0.9646 F1:0.9674 | Conf:0.460\n",
            "[32/100] Train L:0.3037 A:0.9742 | Val A:0.9701 F1:0.9723 | Conf:0.463\n",
            "[33/100] Train L:0.3006 A:0.9730 | Val A:0.9742 F1:0.9761 | Conf:0.460 ✓\n",
            "[34/100] Train L:0.3061 A:0.9709 | Val A:0.9674 F1:0.9697 | Conf:0.459\n",
            "[35/100] Train L:0.3028 A:0.9728 | Val A:0.9701 F1:0.9724 | Conf:0.455\n",
            "[36/100] Train L:0.2966 A:0.9740 | Val A:0.9755 F1:0.9774 | Conf:0.458 ✓\n",
            "[37/100] Train L:0.2944 A:0.9779 | Val A:0.9762 F1:0.9780 | Conf:0.459 ✓\n",
            "[38/100] Train L:0.2969 A:0.9764 | Val A:0.9735 F1:0.9755 | Conf:0.458\n",
            "[39/100] Train L:0.3046 A:0.9711 | Val A:0.9633 F1:0.9661 | Conf:0.454\n",
            "[40/100] Train L:0.2949 A:0.9755 | Val A:0.9782 F1:0.9799 | Conf:0.456 ✓\n",
            "[41/100] Train L:0.2890 A:0.9779 | Val A:0.9844 F1:0.9855 | Conf:0.454 ✓\n",
            "[42/100] Train L:0.2894 A:0.9787 | Val A:0.9789 F1:0.9806 | Conf:0.454\n",
            "[43/100] Train L:0.2917 A:0.9799 | Val A:0.9823 F1:0.9837 | Conf:0.455\n",
            "[44/100] Train L:0.2924 A:0.9776 | Val A:0.9803 F1:0.9818 | Conf:0.454\n",
            "[45/100] Train L:0.2876 A:0.9813 | Val A:0.9769 F1:0.9787 | Conf:0.453\n",
            "[46/100] Train L:0.2790 A:0.9844 | Val A:0.9823 F1:0.9837 | Conf:0.454\n",
            "[47/100] Train L:0.2839 A:0.9833 | Val A:0.9830 F1:0.9843 | Conf:0.453\n",
            "[48/100] Train L:0.2814 A:0.9840 | Val A:0.9850 F1:0.9862 | Conf:0.454 ✓\n",
            "[49/100] Train L:0.2833 A:0.9832 | Val A:0.9803 F1:0.9818 | Conf:0.448\n",
            "[50/100] Train L:0.2823 A:0.9837 | Val A:0.9857 F1:0.9868 | Conf:0.452 ✓\n",
            "[51/100] Train L:0.2775 A:0.9854 | Val A:0.9857 F1:0.9868 | Conf:0.451\n",
            "[52/100] Train L:0.2795 A:0.9832 | Val A:0.9864 F1:0.9874 | Conf:0.452 ✓\n",
            "[53/100] Train L:0.2792 A:0.9847 | Val A:0.9864 F1:0.9875 | Conf:0.450\n",
            "[54/100] Train L:0.2712 A:0.9872 | Val A:0.9864 F1:0.9875 | Conf:0.451\n",
            "[55/100] Train L:0.2702 A:0.9884 | Val A:0.9844 F1:0.9855 | Conf:0.448\n",
            "[56/100] Train L:0.2710 A:0.9891 | Val A:0.9891 F1:0.9900 | Conf:0.449 ✓\n",
            "[57/100] Train L:0.2691 A:0.9895 | Val A:0.9850 F1:0.9862 | Conf:0.448\n",
            "[58/100] Train L:0.2660 A:0.9915 | Val A:0.9857 F1:0.9868 | Conf:0.448\n",
            "[59/100] Train L:0.2671 A:0.9893 | Val A:0.9898 F1:0.9906 | Conf:0.445 ✓\n",
            "[60/100] Train L:0.2669 A:0.9903 | Val A:0.9878 F1:0.9887 | Conf:0.450\n",
            "[61/100] Train L:0.2639 A:0.9912 | Val A:0.9898 F1:0.9906 | Conf:0.446\n",
            "[62/100] Train L:0.2626 A:0.9917 | Val A:0.9918 F1:0.9925 | Conf:0.442 ✓\n",
            "[63/100] Train L:0.2612 A:0.9923 | Val A:0.9878 F1:0.9887 | Conf:0.440\n",
            "[64/100] Train L:0.2620 A:0.9940 | Val A:0.9912 F1:0.9918 | Conf:0.443\n",
            "[65/100] Train L:0.2590 A:0.9942 | Val A:0.9898 F1:0.9906 | Conf:0.440\n",
            "[66/100] Train L:0.2598 A:0.9932 | Val A:0.9891 F1:0.9899 | Conf:0.437\n",
            "[67/100] Train L:0.2598 A:0.9930 | Val A:0.9898 F1:0.9906 | Conf:0.435\n",
            "[68/100] Train L:0.2581 A:0.9940 | Val A:0.9918 F1:0.9925 | Conf:0.438\n",
            "[69/100] Train L:0.2571 A:0.9949 | Val A:0.9932 F1:0.9937 | Conf:0.438 ✓\n",
            "[70/100] Train L:0.2550 A:0.9959 | Val A:0.9918 F1:0.9925 | Conf:0.437\n",
            "[71/100] Train L:0.2580 A:0.9944 | Val A:0.9898 F1:0.9906 | Conf:0.432\n",
            "[72/100] Train L:0.2526 A:0.9978 | Val A:0.9898 F1:0.9906 | Conf:0.436\n",
            "[73/100] Train L:0.2560 A:0.9949 | Val A:0.9864 F1:0.9874 | Conf:0.434\n",
            "[74/100] Train L:0.2552 A:0.9963 | Val A:0.9898 F1:0.9906 | Conf:0.433\n",
            "[75/100] Train L:0.2525 A:0.9973 | Val A:0.9918 F1:0.9925 | Conf:0.434\n",
            "[76/100] Train L:0.2525 A:0.9966 | Val A:0.9905 F1:0.9912 | Conf:0.434\n",
            "[77/100] Train L:0.2523 A:0.9968 | Val A:0.9884 F1:0.9893 | Conf:0.433\n",
            "[78/100] Train L:0.2528 A:0.9961 | Val A:0.9918 F1:0.9925 | Conf:0.434\n",
            "[79/100] Train L:0.2532 A:0.9971 | Val A:0.9884 F1:0.9893 | Conf:0.435\n",
            "[80/100] Train L:0.2545 A:0.9956 | Val A:0.9884 F1:0.9893 | Conf:0.437\n",
            "[81/100] Train L:0.2502 A:0.9976 | Val A:0.9898 F1:0.9906 | Conf:0.432\n",
            "[82/100] Train L:0.2503 A:0.9976 | Val A:0.9898 F1:0.9906 | Conf:0.434\n",
            "[83/100] Train L:0.2511 A:0.9974 | Val A:0.9912 F1:0.9918 | Conf:0.431\n",
            "[84/100] Train L:0.2510 A:0.9973 | Val A:0.9850 F1:0.9862 | Conf:0.434\n",
            "[85/100] Train L:0.2501 A:0.9976 | Val A:0.9884 F1:0.9893 | Conf:0.435\n",
            "[86/100] Train L:0.2499 A:0.9980 | Val A:0.9898 F1:0.9906 | Conf:0.438\n",
            "[87/100] Train L:0.2484 A:0.9985 | Val A:0.9918 F1:0.9925 | Conf:0.435\n",
            "[88/100] Train L:0.2487 A:0.9985 | Val A:0.9891 F1:0.9900 | Conf:0.435\n",
            "[89/100] Train L:0.2472 A:0.9986 | Val A:0.9925 F1:0.9931 | Conf:0.436\n",
            "\n",
            "⚠ Early stopping triggered at epoch 89\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9932 (epoch 69)\n",
            "\n",
            "✓ Best Val Acc: 0.9932 (epoch 69)\n",
            "  Original Test Acc: 0.9308, F1: 0.9312\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8286 Drop=0.1021 [Vulnerable]\n",
            "    Scenario 2: Acc=0.8636 Drop=0.0672 [Vulnerable]\n",
            "    Scenario 3: Acc=0.8117 Drop=0.1191 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9053 Drop=0.0254 [Slightly Vulnerable]\n",
            "    Scenario 5: Acc=0.7631 Drop=0.1676 [Vulnerable]\n",
            "    Scenario 6: Acc=0.8809 Drop=0.0499 [Slightly Vulnerable]\n",
            "    Scenario 7: Acc=0.7832 Drop=0.1476 [Vulnerable]\n",
            "    Scenario 8: Acc=0.8700 Drop=0.0607 [Vulnerable]\n",
            "    Scenario 9: Acc=0.8273 Drop=0.1035 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8646 Drop=0.0662 [Vulnerable]\n",
            "    Scenario 11: Acc=0.8185 Drop=0.1123 [Vulnerable]\n",
            "    Scenario 12: Acc=0.7920 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 13: Acc=0.7452 Drop=0.1856 [Vulnerable]\n",
            "    Scenario 14: Acc=0.7601 Drop=0.1707 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8039 Drop=0.1269 [Vulnerable]\n",
            "    Scenario 16: Acc=0.7838 Drop=0.1469 [Vulnerable]\n",
            "    Scenario 17: Acc=0.8775 Drop=0.0533 [Vulnerable]\n",
            "\n",
            " TPA_NoMask Summary:\n",
            "   Original Test:      0.9308\n",
            "   Avg Transition:     0.8223\n",
            "   Avg Drop:           0.1085\n",
            "   Retention:          88.35%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: TPA_WithMask\n",
            "   Pooling: TPA | Mask: Yes\n",
            "======================================================================\n",
            "\n",
            "Training TPA_WithMask (NO augmentation, clean data only)...\n",
            "Max epochs: 100, patience: 20\n",
            "TPA: prototypes=16, heads=4, temp=0.07\n",
            "[01/100] Train L:1.4324 A:0.5553 | Val A:0.6785 F1:0.6030 | Conf:0.483 ✓\n",
            "[02/100] Train L:0.8788 A:0.8136 | Val A:0.8919 F1:0.8907 | Conf:0.382 ✓\n",
            "[03/100] Train L:0.6314 A:0.8927 | Val A:0.8994 F1:0.8985 | Conf:0.372 ✓\n",
            "[04/100] Train L:0.4875 A:0.9257 | Val A:0.9511 F1:0.9539 | Conf:0.390 ✓\n",
            "[05/100] Train L:0.4331 A:0.9396 | Val A:0.9483 F1:0.9517 | Conf:0.416\n",
            "[06/100] Train L:0.4009 A:0.9480 | Val A:0.9517 F1:0.9553 | Conf:0.428 ✓\n",
            "[07/100] Train L:0.3801 A:0.9492 | Val A:0.9517 F1:0.9553 | Conf:0.432\n",
            "[08/100] Train L:0.3756 A:0.9469 | Val A:0.9613 F1:0.9642 | Conf:0.432 ✓\n",
            "[09/100] Train L:0.3635 A:0.9500 | Val A:0.9585 F1:0.9617 | Conf:0.430\n",
            "[10/100] Train L:0.3589 A:0.9527 | Val A:0.9565 F1:0.9595 | Conf:0.432\n",
            "[11/100] Train L:0.3588 A:0.9493 | Val A:0.9606 F1:0.9636 | Conf:0.427\n",
            "[12/100] Train L:0.3514 A:0.9512 | Val A:0.9606 F1:0.9636 | Conf:0.431\n",
            "[13/100] Train L:0.3500 A:0.9536 | Val A:0.9626 F1:0.9654 | Conf:0.431 ✓\n",
            "[14/100] Train L:0.3439 A:0.9536 | Val A:0.9660 F1:0.9686 | Conf:0.434 ✓\n",
            "[15/100] Train L:0.3435 A:0.9561 | Val A:0.9646 F1:0.9674 | Conf:0.436\n",
            "[16/100] Train L:0.3415 A:0.9563 | Val A:0.9592 F1:0.9624 | Conf:0.443\n",
            "[17/100] Train L:0.3395 A:0.9554 | Val A:0.9640 F1:0.9667 | Conf:0.443\n",
            "[18/100] Train L:0.3378 A:0.9553 | Val A:0.9653 F1:0.9680 | Conf:0.445\n",
            "[19/100] Train L:0.3365 A:0.9551 | Val A:0.9579 F1:0.9611 | Conf:0.450\n",
            "[20/100] Train L:0.3334 A:0.9575 | Val A:0.9640 F1:0.9668 | Conf:0.447\n",
            "[21/100] Train L:0.3286 A:0.9607 | Val A:0.9633 F1:0.9660 | Conf:0.453\n",
            "[22/100] Train L:0.3301 A:0.9589 | Val A:0.9613 F1:0.9641 | Conf:0.454\n",
            "[23/100] Train L:0.3248 A:0.9600 | Val A:0.9667 F1:0.9692 | Conf:0.452 ✓\n",
            "[24/100] Train L:0.3273 A:0.9609 | Val A:0.9640 F1:0.9668 | Conf:0.456\n",
            "[25/100] Train L:0.3247 A:0.9609 | Val A:0.9599 F1:0.9630 | Conf:0.455\n",
            "[26/100] Train L:0.3163 A:0.9668 | Val A:0.9701 F1:0.9723 | Conf:0.458 ✓\n",
            "[27/100] Train L:0.3252 A:0.9634 | Val A:0.9694 F1:0.9718 | Conf:0.456\n",
            "[28/100] Train L:0.3127 A:0.9663 | Val A:0.9680 F1:0.9704 | Conf:0.458\n",
            "[29/100] Train L:0.3093 A:0.9697 | Val A:0.9714 F1:0.9736 | Conf:0.459 ✓\n",
            "[30/100] Train L:0.3109 A:0.9694 | Val A:0.9701 F1:0.9724 | Conf:0.460\n",
            "[31/100] Train L:0.3062 A:0.9721 | Val A:0.9646 F1:0.9674 | Conf:0.460\n",
            "[32/100] Train L:0.3037 A:0.9742 | Val A:0.9701 F1:0.9723 | Conf:0.463\n",
            "[33/100] Train L:0.3006 A:0.9730 | Val A:0.9742 F1:0.9761 | Conf:0.460 ✓\n",
            "[34/100] Train L:0.3061 A:0.9709 | Val A:0.9674 F1:0.9697 | Conf:0.459\n",
            "[35/100] Train L:0.3028 A:0.9728 | Val A:0.9701 F1:0.9724 | Conf:0.455\n",
            "[36/100] Train L:0.2966 A:0.9740 | Val A:0.9755 F1:0.9774 | Conf:0.458 ✓\n",
            "[37/100] Train L:0.2944 A:0.9779 | Val A:0.9762 F1:0.9780 | Conf:0.459 ✓\n",
            "[38/100] Train L:0.2969 A:0.9764 | Val A:0.9735 F1:0.9755 | Conf:0.458\n",
            "[39/100] Train L:0.3046 A:0.9711 | Val A:0.9633 F1:0.9661 | Conf:0.454\n",
            "[40/100] Train L:0.2949 A:0.9755 | Val A:0.9782 F1:0.9799 | Conf:0.456 ✓\n",
            "[41/100] Train L:0.2890 A:0.9779 | Val A:0.9844 F1:0.9855 | Conf:0.454 ✓\n",
            "[42/100] Train L:0.2894 A:0.9787 | Val A:0.9789 F1:0.9806 | Conf:0.454\n",
            "[43/100] Train L:0.2917 A:0.9799 | Val A:0.9823 F1:0.9837 | Conf:0.455\n",
            "[44/100] Train L:0.2924 A:0.9776 | Val A:0.9803 F1:0.9818 | Conf:0.454\n",
            "[45/100] Train L:0.2876 A:0.9813 | Val A:0.9769 F1:0.9787 | Conf:0.453\n",
            "[46/100] Train L:0.2790 A:0.9844 | Val A:0.9823 F1:0.9837 | Conf:0.454\n",
            "[47/100] Train L:0.2839 A:0.9833 | Val A:0.9830 F1:0.9843 | Conf:0.453\n",
            "[48/100] Train L:0.2814 A:0.9840 | Val A:0.9850 F1:0.9862 | Conf:0.454 ✓\n",
            "[49/100] Train L:0.2833 A:0.9832 | Val A:0.9803 F1:0.9818 | Conf:0.448\n",
            "[50/100] Train L:0.2823 A:0.9837 | Val A:0.9857 F1:0.9868 | Conf:0.452 ✓\n",
            "[51/100] Train L:0.2775 A:0.9854 | Val A:0.9857 F1:0.9868 | Conf:0.451\n",
            "[52/100] Train L:0.2795 A:0.9832 | Val A:0.9864 F1:0.9874 | Conf:0.452 ✓\n",
            "[53/100] Train L:0.2792 A:0.9847 | Val A:0.9864 F1:0.9875 | Conf:0.450\n",
            "[54/100] Train L:0.2712 A:0.9872 | Val A:0.9864 F1:0.9875 | Conf:0.451\n",
            "[55/100] Train L:0.2702 A:0.9884 | Val A:0.9844 F1:0.9855 | Conf:0.448\n",
            "[56/100] Train L:0.2710 A:0.9891 | Val A:0.9891 F1:0.9900 | Conf:0.449 ✓\n",
            "[57/100] Train L:0.2691 A:0.9895 | Val A:0.9850 F1:0.9862 | Conf:0.448\n",
            "[58/100] Train L:0.2660 A:0.9915 | Val A:0.9857 F1:0.9868 | Conf:0.448\n",
            "[59/100] Train L:0.2671 A:0.9893 | Val A:0.9898 F1:0.9906 | Conf:0.445 ✓\n",
            "[60/100] Train L:0.2669 A:0.9903 | Val A:0.9878 F1:0.9887 | Conf:0.450\n",
            "[61/100] Train L:0.2639 A:0.9912 | Val A:0.9898 F1:0.9906 | Conf:0.446\n",
            "[62/100] Train L:0.2626 A:0.9917 | Val A:0.9918 F1:0.9925 | Conf:0.442 ✓\n",
            "[63/100] Train L:0.2612 A:0.9923 | Val A:0.9878 F1:0.9887 | Conf:0.440\n",
            "[64/100] Train L:0.2620 A:0.9940 | Val A:0.9912 F1:0.9918 | Conf:0.443\n",
            "[65/100] Train L:0.2590 A:0.9942 | Val A:0.9898 F1:0.9906 | Conf:0.440\n",
            "[66/100] Train L:0.2598 A:0.9932 | Val A:0.9891 F1:0.9899 | Conf:0.437\n",
            "[67/100] Train L:0.2598 A:0.9930 | Val A:0.9898 F1:0.9906 | Conf:0.435\n",
            "[68/100] Train L:0.2581 A:0.9940 | Val A:0.9918 F1:0.9925 | Conf:0.438\n",
            "[69/100] Train L:0.2571 A:0.9949 | Val A:0.9932 F1:0.9937 | Conf:0.438 ✓\n",
            "[70/100] Train L:0.2550 A:0.9959 | Val A:0.9918 F1:0.9925 | Conf:0.437\n",
            "[71/100] Train L:0.2580 A:0.9944 | Val A:0.9898 F1:0.9906 | Conf:0.432\n",
            "[72/100] Train L:0.2526 A:0.9978 | Val A:0.9898 F1:0.9906 | Conf:0.436\n",
            "[73/100] Train L:0.2560 A:0.9949 | Val A:0.9864 F1:0.9874 | Conf:0.434\n",
            "[74/100] Train L:0.2552 A:0.9963 | Val A:0.9898 F1:0.9906 | Conf:0.433\n",
            "[75/100] Train L:0.2525 A:0.9973 | Val A:0.9918 F1:0.9925 | Conf:0.434\n",
            "[76/100] Train L:0.2525 A:0.9966 | Val A:0.9905 F1:0.9912 | Conf:0.434\n",
            "[77/100] Train L:0.2523 A:0.9968 | Val A:0.9884 F1:0.9893 | Conf:0.433\n",
            "[78/100] Train L:0.2528 A:0.9961 | Val A:0.9918 F1:0.9925 | Conf:0.434\n",
            "[79/100] Train L:0.2532 A:0.9971 | Val A:0.9884 F1:0.9893 | Conf:0.435\n",
            "[80/100] Train L:0.2545 A:0.9956 | Val A:0.9884 F1:0.9893 | Conf:0.437\n",
            "[81/100] Train L:0.2502 A:0.9976 | Val A:0.9898 F1:0.9906 | Conf:0.432\n",
            "[82/100] Train L:0.2503 A:0.9976 | Val A:0.9898 F1:0.9906 | Conf:0.434\n",
            "[83/100] Train L:0.2511 A:0.9974 | Val A:0.9912 F1:0.9918 | Conf:0.431\n",
            "[84/100] Train L:0.2510 A:0.9973 | Val A:0.9850 F1:0.9862 | Conf:0.434\n",
            "[85/100] Train L:0.2501 A:0.9976 | Val A:0.9884 F1:0.9893 | Conf:0.435\n",
            "[86/100] Train L:0.2499 A:0.9980 | Val A:0.9898 F1:0.9906 | Conf:0.438\n",
            "[87/100] Train L:0.2484 A:0.9985 | Val A:0.9918 F1:0.9925 | Conf:0.435\n",
            "[88/100] Train L:0.2487 A:0.9985 | Val A:0.9891 F1:0.9900 | Conf:0.435\n",
            "[89/100] Train L:0.2472 A:0.9986 | Val A:0.9925 F1:0.9931 | Conf:0.436\n",
            "\n",
            "⚠ Early stopping triggered at epoch 89\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9932 (epoch 69)\n",
            "\n",
            "✓ Best Val Acc: 0.9932 (epoch 69)\n",
            "  Original Test Acc: 0.9308, F1: 0.9312\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8914 Drop=0.0394 [Slightly Vulnerable]\n",
            "    Scenario 2: Acc=0.9192 Drop=0.0115 [Very Robust]\n",
            "    Scenario 3: Acc=0.9253 Drop=0.0054 [Very Robust]\n",
            "    Scenario 4: Acc=0.9233 Drop=0.0075 [Very Robust]\n",
            "    Scenario 5: Acc=0.9206 Drop=0.0102 [Very Robust]\n",
            "    Scenario 6: Acc=0.9284 Drop=0.0024 [Very Robust]\n",
            "    Scenario 7: Acc=0.9243 Drop=0.0064 [Very Robust]\n",
            "    Scenario 8: Acc=0.9240 Drop=0.0068 [Very Robust]\n",
            "    Scenario 9: Acc=0.8972 Drop=0.0336 [Slightly Vulnerable]\n",
            "    Scenario 10: Acc=0.9199 Drop=0.0109 [Very Robust]\n",
            "    Scenario 11: Acc=0.8823 Drop=0.0485 [Slightly Vulnerable]\n",
            "    Scenario 12: Acc=0.9233 Drop=0.0075 [Very Robust]\n",
            "    Scenario 13: Acc=0.9186 Drop=0.0122 [Very Robust]\n",
            "    Scenario 14: Acc=0.9203 Drop=0.0105 [Very Robust]\n",
            "    Scenario 15: Acc=0.8833 Drop=0.0475 [Slightly Vulnerable]\n",
            "    Scenario 16: Acc=0.9169 Drop=0.0139 [Very Robust]\n",
            "    Scenario 17: Acc=0.9253 Drop=0.0054 [Very Robust]\n",
            "\n",
            " TPA_WithMask Summary:\n",
            "   Original Test:      0.9308\n",
            "   Avg Transition:     0.9143\n",
            "   Avg Drop:           0.0164\n",
            "   Retention:          98.23%\n",
            "\n",
            "======================================================================\n",
            "   4-WAY ABLATION RESULTS (NO TRAINING AUGMENTATION)\n",
            "======================================================================\n",
            "\n",
            "Config               Pooling  Mask   Orig     Trans    Drop     Retention \n",
            "-------------------------------------------------------------------------------------\n",
            "GAP_NoMask           GAP      No     0.9118   0.8190   0.0928   89.82     %\n",
            "GAP_WithMask         GAP      Yes    0.9118   0.9113   0.0005   99.95     %\n",
            "TPA_NoMask           TPA      No     0.9308   0.8223   0.1085   88.35     %\n",
            "TPA_WithMask         TPA      Yes    0.9308   0.9143   0.0164   98.23     %\n",
            "\n",
            "======================================================================\n",
            "   EFFECT ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "1️⃣  MASK EFFECT (in GAP):\n",
            "   GAP_NoMask  → GAP_WithMask\n",
            "   Drop: 0.0928 → 0.0005\n",
            "   Improvement: +99.51% drop reduction\n",
            "   Conclusion: Mask helps GAP by filtering transitions\n",
            "\n",
            "2️⃣  TPA EFFECT (without mask):\n",
            "   GAP_NoMask  → TPA_NoMask\n",
            "   Drop: 0.0928 → 0.1085\n",
            "   Improvement: -16.86% drop reduction\n",
            "   Conclusion: TPA alone does not help via prototype attention\n",
            "\n",
            "3️⃣  COMBINED EFFECT (TPA + Mask):\n",
            "   GAP_NoMask  → TPA_WithMask\n",
            "   Drop: 0.0928 → 0.0164\n",
            "   Improvement: +82.28% drop reduction\n",
            "   Retention gain: +8.41pp\n",
            "\n",
            "4️⃣  SYNERGY ANALYSIS:\n",
            "   Expected (additive): 0.0161\n",
            "   Actual (TPA+Mask):   0.0164\n",
            "   Synergy: -0.0003 (-0.37%)\n",
            "   = No synergy: Effects are simply additive\n",
            "\n",
            "======================================================================\n",
            "   BEST CONFIGURATION: GAP_WithMask\n",
            "======================================================================\n",
            "   Pooling: GAP\n",
            "   Mask: Yes\n",
            "   Original Acc: 0.9118\n",
            "   Transition Acc: 0.9113\n",
            "   Drop: 0.0005\n",
            "   Retention: 99.95%\n",
            "\n",
            "✓ Results saved to '/content/drive/MyDrive/AI_data/ablation_4way_no_aug/ablation_4way_no_aug_results.json'\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TPA Ablation Study - No Training Augmentation Version\"\"\"\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str = \"ablation\"\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/ablation_4way_no_aug\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    d_model: int = 128\n",
        "    use_tpa: bool = False\n",
        "    use_mask: bool = False\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_seg_kernel: int = 9\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/total_acc_z_\", \"txt\"), (\"Inertial Signals/body_acc_x_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\",\n",
        "              4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "_LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None]\n",
        "              for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 mask: np.ndarray | None = None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "            if mask is not None:\n",
        "                mask = mask[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "        T = self.X.shape[2]\n",
        "        if mask is None:\n",
        "            self.mask = np.ones((self.X.shape[0], T), dtype=bool)\n",
        "        else:\n",
        "            assert mask.shape == (self.X.shape[0], T)\n",
        "            self.mask = mask.astype(bool)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            torch.from_numpy(self.mask[idx]).float()\n",
        "        )\n",
        "\n",
        "# ========================\n",
        "# 2) TPA Module\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Temporal Prototype Attention with optional mask support\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        pad = (seg_kernel - 1) // 2\n",
        "        self.lowpass = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim, bias=False)\n",
        "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
        "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None, return_confidence=False, use_mask=True):\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_filtered = self.lowpass(x.transpose(1, 2)).transpose(1, 2)\n",
        "        xloc = self.pw(self.dw(x_filtered.transpose(1, 2))).transpose(1, 2)\n",
        "        xloc = self.pre_norm(xloc) + x\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            float_mask = mask.float()\n",
        "            float_mask_expanded = float_mask.unsqueeze(-1)\n",
        "            xloc = xloc * float_mask_expanded\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            boolean_mask = mask.bool()\n",
        "            mask_attn = boolean_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(~mask_attn, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            mask_expanded = float_mask.unsqueeze(-1).float()\n",
        "            z_gap = (x * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 3) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([\n",
        "            nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1))\n",
        "            for k in branches\n",
        "        ])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap, mask: torch.BoolTensor | None = None, use_mask: bool = False):\n",
        "        features = Fmap.transpose(1, 2)\n",
        "\n",
        "        if mask is not None and use_mask:\n",
        "            mask_expanded = mask.unsqueeze(-1).float()\n",
        "            pooled = (features * mask_expanded).sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
        "        else:\n",
        "            pooled = features.mean(dim=1)\n",
        "\n",
        "        logits = self.fc(pooled)\n",
        "        aux = {\"confidence\": None}\n",
        "        return logits, aux\n",
        "\n",
        "class TPAHead(nn.Module):\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, seg_kernel: int = 9,\n",
        "                 heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tpa = ProductionTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=num_prototypes,\n",
        "            seg_kernel=seg_kernel,\n",
        "            heads=heads,\n",
        "            dropout=dropout,\n",
        "            temperature=temperature,\n",
        "            topk_ratio=topk_ratio\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap, mask: torch.BoolTensor | None = None, use_mask: bool = False):\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        z, confidence = self.tpa(features, mask=mask, return_confidence=True, use_mask=use_mask)\n",
        "        logits = self.classifier(z)\n",
        "        aux = {\"confidence\": confidence.mean().item()}\n",
        "        return logits, aux\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, use_tpa=False, use_mask=False, tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.use_tpa = use_tpa\n",
        "        self.use_mask = use_mask\n",
        "\n",
        "        if use_tpa:\n",
        "            self.head = TPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                seg_kernel=tpa_config.get('seg_kernel', 9),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25)\n",
        "            )\n",
        "        else:\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, mask: torch.BoolTensor | None = None):\n",
        "        fmap = self.backbone(x)\n",
        "\n",
        "        if mask is not None:\n",
        "            stride = self.backbone.stride\n",
        "            mask_float = mask.float().unsqueeze(1)\n",
        "            mask_down = (F.avg_pool1d(mask_float, kernel_size=stride, stride=stride) == 1.0).squeeze(1)\n",
        "        else:\n",
        "            mask_down = None\n",
        "\n",
        "        return self.head(fmap, mask_down, use_mask=self.use_mask)\n",
        "\n",
        "# ========================\n",
        "# 4) Train / Eval (No Augmentation)\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    \"\"\"Training without any augmentation\"\"\"\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    confidence_vals = []\n",
        "\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device).float(), y.to(cfg.device), m.to(cfg.device).float()\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, aux = model(x, mask=m)\n",
        "\n",
        "        cls_loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "        loss = cls_loss\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            if aux[\"confidence\"] is not None:\n",
        "                confidence_vals.append(aux[\"confidence\"])\n",
        "\n",
        "    stats = {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "        \"avg_confidence\": np.mean(confidence_vals) if confidence_vals else None\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config, classes=6):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device), y.to(cfg.device), m.to(cfg.device)\n",
        "        logits, _ = model(x, mask=m)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(classes)))\n",
        "    report = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[_CODE_TO_LABEL_NAME[i] for i in range(classes)],\n",
        "        digits=4\n",
        "    )\n",
        "    return acc, f1, cm, report\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_mask_analysis(model, loader, cfg: Config):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y, m in loader:\n",
        "        x, y, m = x.to(cfg.device), y.to(cfg.device), m.to(cfg.device)\n",
        "        logits, _ = model(x, mask=m)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "\n",
        "        ys.append(y.cpu().numpy())\n",
        "        ps.append(pred.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ========================\n",
        "# 5) Transitional Test Set Creation\n",
        "# ========================\n",
        "def create_transitional_test_set(orig_dataset: UCIHARInertial, class_A: str, class_B: str,\n",
        "                                 p: float=0.05, mix: float=0.25, profile: str='abrupt',\n",
        "                                 pos: str='tail', segments: int=1) -> Tuple[UCIHARInertial, dict]:\n",
        "    \"\"\"Create transitional test set\"\"\"\n",
        "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
        "    N, C, T = X.shape\n",
        "    mask = np.ones((N, T), dtype=bool)\n",
        "\n",
        "    code_A, code_B = _LABEL_NAME_TO_CODE[class_A], _LABEL_NAME_TO_CODE[class_B]\n",
        "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
        "    mix_pts = int(T * mix)\n",
        "\n",
        "    modified_indices = []\n",
        "\n",
        "    if segments > 1:\n",
        "        seg_length = mix_pts // segments\n",
        "        remaining = mix_pts % segments\n",
        "    else:\n",
        "        seg_length = mix_pts\n",
        "        remaining = 0\n",
        "\n",
        "    def get_transition_positions(T, mix_pts, pos, segments):\n",
        "        positions = []\n",
        "\n",
        "        if segments == 1:\n",
        "            if pos == 'tail':\n",
        "                positions = [T - mix_pts]\n",
        "            elif pos == 'middle':\n",
        "                positions = [(T - mix_pts) // 2]\n",
        "            elif pos == 'random':\n",
        "                positions = [random.randint(0, max(0, T - mix_pts))]\n",
        "        else:\n",
        "            seg_len = mix_pts // segments\n",
        "            if pos == 'tail':\n",
        "                start = T - mix_pts\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * seg_len)\n",
        "            elif pos == 'middle':\n",
        "                center = T // 2\n",
        "                total_span = mix_pts + (segments - 1) * seg_len\n",
        "                start = center - total_span // 2\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * (seg_len * 2))\n",
        "            elif pos == 'random':\n",
        "                available_positions = list(range(0, T - seg_len))\n",
        "                random.shuffle(available_positions)\n",
        "                positions = sorted(available_positions[:segments])\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def apply_transition(target_data, source_data, start_pos, length, profile):\n",
        "        end_pos = start_pos + length\n",
        "\n",
        "        if profile == 'abrupt':\n",
        "            target_data[:, start_pos:end_pos] = source_data[:, start_pos:end_pos].copy()\n",
        "        elif profile == 'fade':\n",
        "            alpha = np.linspace(0, 1, length).reshape(1, -1)\n",
        "            target_segment = target_data[:, start_pos:end_pos]\n",
        "            source_segment = source_data[:, start_pos:end_pos]\n",
        "            target_data[:, start_pos:end_pos] = (\n",
        "                target_segment * (1 - alpha) + source_segment * alpha\n",
        "            )\n",
        "\n",
        "    # Apply transitions for class A\n",
        "    n_targets_A = max(1, int(len(idx_A) * p))\n",
        "    targets_A = np.random.choice(idx_A, n_targets_A, replace=False)\n",
        "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_A, sources_B):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "                mask[t, start:start+curr_len] = False\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    # Apply transitions for class B\n",
        "    n_targets_B = max(1, int(len(idx_B) * p))\n",
        "    targets_B = np.random.choice(idx_B, n_targets_B, replace=False)\n",
        "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_B, sources_A):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "                mask[t, start:start+curr_len] = False\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    if p > 0.5:\n",
        "        mid_start = T // 3\n",
        "        mid_end = 2 * T // 3\n",
        "        mid_length = mid_end - mid_start\n",
        "\n",
        "        extra_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p * 0.3)), replace=False)\n",
        "        extra_B_src = np.random.choice(idx_B, len(extra_A), replace=True)\n",
        "\n",
        "        for t, s in zip(extra_A, extra_B_src):\n",
        "            if t not in modified_indices:\n",
        "                apply_transition(X[t], orig_dataset.X[s], mid_start, mid_length, profile)\n",
        "                mask[t, mid_start:mid_end] = False\n",
        "                modified_indices.append(t)\n",
        "\n",
        "    mod_dataset = UCIHARInertial(\n",
        "        root=\"\", split=\"test\",\n",
        "        mean=orig_dataset.mean, std=orig_dataset.std,\n",
        "        mask=mask, preloaded_data=(X, y)\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'total_samples': N,\n",
        "        'modified_samples': len(modified_indices),\n",
        "        'modified_ratio': len(modified_indices) / N,\n",
        "        'mix_frames': mix_pts,\n",
        "        'primary_class_ratio': 1 - mix,\n",
        "        'class_A_modified': len(targets_A),\n",
        "        'class_B_modified': len(targets_B),\n",
        "        'profile': profile,\n",
        "        'position': pos,\n",
        "        'segments': segments\n",
        "    }\n",
        "\n",
        "    return mod_dataset, info\n",
        "\n",
        "def get_transition_scenarios():\n",
        "    \"\"\"Return all transitional test scenarios\"\"\"\n",
        "    scenarios_core = [\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_stress = [\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_ctrl = [\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"middle\",2),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"middle\",2),\n",
        "    ]\n",
        "\n",
        "    return scenarios_core + scenarios_stress + scenarios_ctrl\n",
        "\n",
        "# ========================\n",
        "# 6) 4-Way Ablation Study\n",
        "# ========================\n",
        "def run_ablation_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Load and split data\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)\n",
        "\n",
        "    n_samples = len(y_full)\n",
        "    indices = np.arange(n_samples)\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_full\n",
        "    )\n",
        "\n",
        "    print(f\"Total train samples: {n_samples}\")\n",
        "    print(f\"  → Train split: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val split:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader_orig = DataLoader(test_set_orig, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    # Create transitional test sets\n",
        "    scenarios = get_transition_scenarios()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    4-WAY ABLATION STUDY: GAP/TPA × NoMask/WithMask\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Training: NO augmentation (clean data only)\")\n",
        "    print(f\"   Testing: Extreme transitions on test set\")\n",
        "    print(f\"   Scenarios: {len(scenarios)} transition configurations\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    transition_sets, transition_infos = [], []\n",
        "    for i, scenario in enumerate(scenarios):\n",
        "        clsA, clsB, p, mix, profile, pos, segments = scenario\n",
        "        print(f\"  [{i+1}/{len(scenarios)}] {clsA} ↔ {clsB} (p={p:.2f}, mix={mix:.2f}, {profile}, {pos}, seg={segments})\")\n",
        "        test_set_mod, info = create_transitional_test_set(\n",
        "            test_set_orig, clsA, clsB, p=p, mix=mix, profile=profile, pos=pos, segments=segments\n",
        "        )\n",
        "        transition_sets.append(test_set_mod)\n",
        "        transition_infos.append(info)\n",
        "        print(f\"      Modified: {info['modified_samples']}/{info['total_samples']} ({info['modified_ratio']*100:.1f}%)\")\n",
        "\n",
        "    transition_loaders = [DataLoader(ts, cfg.batch_size, num_workers=cfg.num_workers) for ts in transition_sets]\n",
        "    print(f\"\\n✓ {len(transition_loaders)} transitional test sets created.\\n\")\n",
        "\n",
        "    # 4-way ablation configurations\n",
        "    ablation_configs = [\n",
        "        {\"name\": \"GAP_NoMask\", \"use_tpa\": False, \"use_mask\": False},\n",
        "        {\"name\": \"GAP_WithMask\", \"use_tpa\": False, \"use_mask\": True},\n",
        "        {\"name\": \"TPA_NoMask\", \"use_tpa\": True, \"use_mask\": False},\n",
        "        {\"name\": \"TPA_WithMask\", \"use_tpa\": True, \"use_mask\": True},\n",
        "    ]\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    # Train and evaluate each configuration\n",
        "    for ab_cfg in ablation_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   CONFIG: {ab_cfg['name']}\")\n",
        "        print(f\"   Pooling: {'TPA' if ab_cfg['use_tpa'] else 'GAP'} | Mask: {'Yes' if ab_cfg['use_mask'] else 'No'}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        cfg.use_tpa = ab_cfg[\"use_tpa\"]\n",
        "        cfg.use_mask = ab_cfg[\"use_mask\"]\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True,\n",
        "                                   num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        model_path = os.path.join(cfg.save_dir, f\"model_{ab_cfg['name']}.pth\")\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'seg_kernel': cfg.tpa_seg_kernel,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            use_tpa=cfg.use_tpa,\n",
        "            use_mask=cfg.use_mask,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {ab_cfg['name']} (NO augmentation, clean data only)...\")\n",
        "        print(f\"Max epochs: {cfg.epochs}, patience: {cfg.patience}\")\n",
        "        if cfg.use_tpa:\n",
        "            print(f\"TPA: prototypes={cfg.tpa_num_prototypes}, heads={cfg.tpa_heads}, temp={cfg.tpa_temperature}\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "\n",
        "            val_acc, val_f1, _, _ = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "            log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "            if stats['avg_confidence'] is not None:\n",
        "                log_str += f\" | Conf:{stats['avg_confidence']:.3f}\"\n",
        "            if improved:\n",
        "                log_str += \" ✓\"\n",
        "            print(log_str)\n",
        "\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  No improvement for {cfg.patience} epochs on validation set\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Evaluate on test sets\n",
        "        acc_orig, f1_orig, _, _ = evaluate(model, test_loader_orig, cfg)\n",
        "        print(f\"  Original Test Acc: {acc_orig:.4f}, F1: {f1_orig:.4f}\")\n",
        "\n",
        "        print(f\"\\n   Evaluating on {len(transition_loaders)} transitional test sets...\")\n",
        "        transition_accs = []\n",
        "        scenario_details = []\n",
        "\n",
        "        for i, loader in enumerate(transition_loaders):\n",
        "            result = evaluate_with_mask_analysis(model, loader, cfg)\n",
        "            acc_mod = result['accuracy']\n",
        "            transition_accs.append(acc_mod)\n",
        "\n",
        "            scenario = scenarios[i]\n",
        "            clsA, clsB, p, mix = scenario[0], scenario[1], scenario[2], scenario[3]\n",
        "            primary_ratio = (1 - mix) * 100\n",
        "            drop_from_orig = acc_orig - acc_mod\n",
        "\n",
        "            if drop_from_orig < 0.02:\n",
        "                grade = \"Very Robust\"\n",
        "            elif drop_from_orig < 0.05:\n",
        "                grade = \"Slightly Vulnerable\"\n",
        "            else:\n",
        "                grade = \"Vulnerable\"\n",
        "\n",
        "            scenario_details.append({\n",
        "                'scenario': f\"{clsA}↔{clsB}\",\n",
        "                'primary_ratio': primary_ratio,\n",
        "                'acc': acc_mod,\n",
        "                'drop': drop_from_orig,\n",
        "                'grade': grade\n",
        "            })\n",
        "\n",
        "            print(f\"    Scenario {i+1}: Acc={acc_mod:.4f} Drop={drop_from_orig:.4f} [{grade}]\")\n",
        "\n",
        "        avg_trans_acc = np.mean(transition_accs)\n",
        "        avg_drop = acc_orig - avg_trans_acc\n",
        "        retention = (1 - avg_drop/acc_orig) * 100 if acc_orig > 0 else 0\n",
        "\n",
        "        result = {\n",
        "            \"config\": ab_cfg[\"name\"],\n",
        "            \"use_tpa\": ab_cfg[\"use_tpa\"],\n",
        "            \"use_mask\": ab_cfg[\"use_mask\"],\n",
        "            \"orig_acc\": acc_orig,\n",
        "            \"avg_trans_acc\": avg_trans_acc,\n",
        "            \"avg_drop\": avg_drop,\n",
        "            \"retention\": retention,\n",
        "            \"scenario_details\": scenario_details\n",
        "        }\n",
        "        results_table.append(result)\n",
        "\n",
        "        print(f\"\\n {ab_cfg['name']} Summary:\")\n",
        "        print(f\"   Original Test:      {acc_orig:.4f}\")\n",
        "        print(f\"   Avg Transition:     {avg_trans_acc:.4f}\")\n",
        "        print(f\"   Avg Drop:           {avg_drop:.4f}\")\n",
        "        print(f\"   Retention:          {retention:.2f}%\")\n",
        "\n",
        "    # ========================\n",
        "    # Analysis and Comparison\n",
        "    # ========================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   4-WAY ABLATION RESULTS (NO TRAINING AUGMENTATION)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"{'Config':<20} {'Pooling':<8} {'Mask':<6} {'Orig':<8} {'Trans':<8} {'Drop':<8} {'Retention':<10}\")\n",
        "    print(\"-\" * 85)\n",
        "\n",
        "    for r in results_table:\n",
        "        pooling = \"TPA\" if r['use_tpa'] else \"GAP\"\n",
        "        mask = \"Yes\" if r['use_mask'] else \"No\"\n",
        "        print(f\"{r['config']:<20} {pooling:<8} {mask:<6} {r['orig_acc']:<8.4f} {r['avg_trans_acc']:<8.4f} {r['avg_drop']:<8.4f} {r['retention']:<10.2f}%\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   EFFECT ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    gap_nomask = next(r for r in results_table if not r['use_tpa'] and not r['use_mask'])\n",
        "    gap_mask = next(r for r in results_table if not r['use_tpa'] and r['use_mask'])\n",
        "    tpa_nomask = next(r for r in results_table if r['use_tpa'] and not r['use_mask'])\n",
        "    tpa_mask = next(r for r in results_table if r['use_tpa'] and r['use_mask'])\n",
        "\n",
        "    # 1) Effect of Mask (in GAP)\n",
        "    mask_effect_gap = gap_nomask['avg_drop'] - gap_mask['avg_drop']\n",
        "    mask_improve_gap = (mask_effect_gap / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"1️⃣  MASK EFFECT (in GAP):\")\n",
        "    print(f\"   GAP_NoMask  → GAP_WithMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {gap_mask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {mask_improve_gap:+.2f}% drop reduction\")\n",
        "    print(f\"   Conclusion: Mask {'helps' if mask_effect_gap > 0 else 'does not help'} GAP by filtering transitions\\n\")\n",
        "\n",
        "    # 2) Effect of TPA (without mask)\n",
        "    tpa_effect_nomask = gap_nomask['avg_drop'] - tpa_nomask['avg_drop']\n",
        "    tpa_improve_nomask = (tpa_effect_nomask / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"2️⃣  TPA EFFECT (without mask):\")\n",
        "    print(f\"   GAP_NoMask  → TPA_NoMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {tpa_nomask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {tpa_improve_nomask:+.2f}% drop reduction\")\n",
        "    print(f\"   Conclusion: TPA alone {'helps' if tpa_effect_nomask > 0 else 'does not help'} via prototype attention\\n\")\n",
        "\n",
        "    # 3) Combined effect\n",
        "    combined_effect = gap_nomask['avg_drop'] - tpa_mask['avg_drop']\n",
        "    combined_improve = (combined_effect / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"3️⃣  COMBINED EFFECT (TPA + Mask):\")\n",
        "    print(f\"   GAP_NoMask  → TPA_WithMask\")\n",
        "    print(f\"   Drop: {gap_nomask['avg_drop']:.4f} → {tpa_mask['avg_drop']:.4f}\")\n",
        "    print(f\"   Improvement: {combined_improve:+.2f}% drop reduction\")\n",
        "    print(f\"   Retention gain: {tpa_mask['retention'] - gap_nomask['retention']:+.2f}pp\\n\")\n",
        "\n",
        "    # 4) Synergy analysis\n",
        "    expected_additive = gap_nomask['avg_drop'] - mask_effect_gap - tpa_effect_nomask\n",
        "    actual = tpa_mask['avg_drop']\n",
        "    synergy = expected_additive - actual\n",
        "    synergy_pct = (synergy / gap_nomask['avg_drop'] * 100) if gap_nomask['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"4️⃣  SYNERGY ANALYSIS:\")\n",
        "    print(f\"   Expected (additive): {expected_additive:.4f}\")\n",
        "    print(f\"   Actual (TPA+Mask):   {actual:.4f}\")\n",
        "    print(f\"   Synergy: {synergy:+.4f} ({synergy_pct:+.2f}%)\")\n",
        "    if synergy > 0.005:\n",
        "        print(f\"   ✓ Positive synergy: TPA and Mask work better together!\")\n",
        "    elif synergy < -0.005:\n",
        "        print(f\"   ✗ Negative synergy: Interference between TPA and Mask\")\n",
        "    else:\n",
        "        print(f\"   = No synergy: Effects are simply additive\")\n",
        "\n",
        "    # Best configuration\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    best_config = min(results_table, key=lambda x: x['avg_drop'])\n",
        "    print(f\"   BEST CONFIGURATION: {best_config['config']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Pooling: {'TPA' if best_config['use_tpa'] else 'GAP'}\")\n",
        "    print(f\"   Mask: {'Yes' if best_config['use_mask'] else 'No'}\")\n",
        "    print(f\"   Original Acc: {best_config['orig_acc']:.4f}\")\n",
        "    print(f\"   Transition Acc: {best_config['avg_trans_acc']:.4f}\")\n",
        "    print(f\"   Drop: {best_config['avg_drop']:.4f}\")\n",
        "    print(f\"   Retention: {best_config['retention']:.2f}%\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(cfg.save_dir, \"ablation_4way_no_aug_results.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            'results': results_table,\n",
        "            'analysis': {\n",
        "                'mask_effect_gap': float(mask_effect_gap),\n",
        "                'tpa_effect_nomask': float(tpa_effect_nomask),\n",
        "                'combined_effect': float(combined_effect),\n",
        "                'synergy': float(synergy)\n",
        "            },\n",
        "            'training_note': 'No augmentation during training - clean data only'\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Results saved to '{cfg.save_dir}/ablation_4way_no_aug_results.json'\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ========================\n",
        "# 7) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.mode = \"ablation\"\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_seg_kernel = 9\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    UCI-HAR 4-WAY ABLATION STUDY\")\n",
        "    print(\"    (NO TRAINING AUGMENTATION VERSION)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"\\n⚠️  IMPORTANT: NO data augmentation during training\")\n",
        "    print(f\"   Models trained on CLEAN data only\")\n",
        "    print(f\"   Transitions applied ONLY to test sets\")\n",
        "    print(f\"\\n4 Configurations to Compare:\")\n",
        "    print(f\"  1) GAP_NoMask:    Standard GAP (baseline)\")\n",
        "    print(f\"  2) GAP_WithMask:  GAP + mask filtering\")\n",
        "    print(f\"  3) TPA_NoMask:    Prototype Attention only\")\n",
        "    print(f\"  4) TPA_WithMask:  Prototype Attention + mask filtering\")\n",
        "    print(f\"\\nThis measures:\")\n",
        "    print(f\"  • Effect of mask alone (without augmentation training)\")\n",
        "    print(f\"  • Effect of TPA alone (intrinsic robustness)\")\n",
        "    print(f\"  • Effect of TPA + mask combined\")\n",
        "    print(f\"  • Whether there's synergy between them\")\n",
        "    print(f\"\\nTPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    if config.mode == \"ablation\":\n",
        "        run_ablation_study(config)\n",
        "    else:\n",
        "        print(\"✗ Invalid mode. Set config.mode = 'ablation'\")"
      ]
    }
  ]
}