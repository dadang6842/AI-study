{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. TPA 2. GAP 3. TPA에 lowpass, dwconv 제거한 모델을 비교  "
      ],
      "metadata": {
        "id": "ocHGtrhfW0jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7fAXInsWziY",
        "outputId": "27539f2a-4414-4c81-80be-921593e154ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUycYP4uPgiq",
        "outputId": "389fdfc1-f8ca-48aa-a1be-24acc4755d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "    NOISE ROBUSTNESS STUDY: 3-Model Comparison\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "Training Augmentation: prob=0.7, max_noise=0.4\n",
            "\n",
            "3 Models to Compare:\n",
            "  1) GAP Baseline:  Standard Global Average Pooling\n",
            "  2) TPA (Full):    Temporal Prototype Attention with Conv layers\n",
            "  3) TPA-Simple:    TPA without lowpass/depthwise/pointwise conv\n",
            "\n",
            "7 Noise Scenarios:\n",
            "  - Clean (no noise)\n",
            "  - Gaussian noise (20%, 40%)\n",
            "  - Temporal mask (30%)\n",
            "  - Channel drift (20%)\n",
            "  - Channel drop (20%)\n",
            "  - Spike noise (5%)\n",
            "\n",
            "TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total samples: 7352\n",
            "  → Train: 5881 (80%)\n",
            "  → Val:   1471 (20%)\n",
            "\n",
            "======================================================================\n",
            "    NOISE ROBUSTNESS STUDY: 3-Model Comparison\n",
            "======================================================================\n",
            "   Models: GAP (Baseline), TPA (Full), TPA-Simple (No Conv)\n",
            "   Training: Mixed noise augmentation (prob=0.7, max=0.4)\n",
            "   Testing: 7 noise scenarios\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   MODEL: GAP_Baseline\n",
            "======================================================================\n",
            "\n",
            "Training GAP_Baseline for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4717 A:0.5215 | Val A:0.8192 F1:0.8030 ✓\n",
            "[10/100] Train L:0.4458 A:0.9299 | Val A:0.9409 F1:0.9444\n",
            "[20/100] Train L:0.3956 A:0.9366 | Val A:0.9483 F1:0.9521\n",
            "[30/100] Train L:0.3797 A:0.9424 | Val A:0.9545 F1:0.9577\n",
            "[40/100] Train L:0.3738 A:0.9413 | Val A:0.9545 F1:0.9577\n",
            "[50/100] Train L:0.3677 A:0.9459 | Val A:0.9599 F1:0.9630\n",
            "\n",
            "⚠ Early stopping triggered at epoch 54\n",
            "  Best validation acc: 0.9613 (epoch 34)\n",
            "\n",
            "✓ Best Val Acc: 0.9613 (epoch 34)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "======================================================================\n",
            "   MODEL: TPA\n",
            "======================================================================\n",
            "\n",
            "Training TPA for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4678 A:0.5372 | Val A:0.6798 F1:0.6075 ✓\n",
            "[10/100] Train L:0.3904 A:0.9388 | Val A:0.9463 F1:0.9497\n",
            "[20/100] Train L:0.3653 A:0.9437 | Val A:0.9558 F1:0.9590\n",
            "[30/100] Train L:0.3510 A:0.9481 | Val A:0.9613 F1:0.9642\n",
            "[40/100] Train L:0.3453 A:0.9502 | Val A:0.9626 F1:0.9655\n",
            "[50/100] Train L:0.3372 A:0.9553 | Val A:0.9701 F1:0.9724\n",
            "[60/100] Train L:0.3328 A:0.9549 | Val A:0.9694 F1:0.9718\n",
            "[70/100] Train L:0.3246 A:0.9614 | Val A:0.9755 F1:0.9774\n",
            "[80/100] Train L:0.3237 A:0.9629 | Val A:0.9646 F1:0.9674\n",
            "[90/100] Train L:0.3272 A:0.9599 | Val A:0.9789 F1:0.9806\n",
            "\n",
            "⚠ Early stopping triggered at epoch 97\n",
            "  Best validation acc: 0.9823 (epoch 77)\n",
            "\n",
            "✓ Best Val Acc: 0.9823 (epoch 77)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "======================================================================\n",
            "   MODEL: TPA_Simple\n",
            "======================================================================\n",
            "\n",
            "Training TPA_Simple for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.5036 A:0.6455 | Val A:0.7600 F1:0.7227 ✓\n",
            "[10/100] Train L:0.3952 A:0.9401 | Val A:0.9517 F1:0.9549\n",
            "[20/100] Train L:0.3630 A:0.9432 | Val A:0.9551 F1:0.9584\n",
            "[30/100] Train L:0.3492 A:0.9512 | Val A:0.9626 F1:0.9655 ✓\n",
            "[40/100] Train L:0.3463 A:0.9510 | Val A:0.9613 F1:0.9643\n",
            "[50/100] Train L:0.3397 A:0.9532 | Val A:0.9660 F1:0.9686 ✓\n",
            "[60/100] Train L:0.3301 A:0.9577 | Val A:0.9613 F1:0.9643\n",
            "[70/100] Train L:0.3188 A:0.9619 | Val A:0.9735 F1:0.9755 ✓\n",
            "[80/100] Train L:0.3237 A:0.9626 | Val A:0.9687 F1:0.9712\n",
            "[90/100] Train L:0.3162 A:0.9667 | Val A:0.9803 F1:0.9818\n",
            "[100/100] Train L:0.3069 A:0.9685 | Val A:0.9823 F1:0.9837 ✓\n",
            "\n",
            "✓ Best Val Acc: 0.9823 (epoch 100)\n",
            "\n",
            "   Testing on 7 noise scenarios...\n",
            "\n",
            "====================================================================================================\n",
            "   GAP BASELINE - DETAILED RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "Noise Type                |   Accuracy |   F1-Score |       Drop |    Retention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Clean                     |     91.79% |     0.919 |      0.00% |      100.00%\n",
            "20% Gaussian              |     91.79% |     0.919 |      0.00% |      100.00%\n",
            "40% Gaussian              |     91.69% |     0.918 |      0.10% |       99.89%\n",
            "30% Temporal Mask         |     91.82% |     0.920 |     -0.03% |      100.04%\n",
            "20% Channel Drift         |     91.55% |     0.917 |      0.24% |       99.74%\n",
            "20% Channel Drop          |     87.24% |     0.875 |      4.55% |       95.05%\n",
            "5% Spike Noise            |     91.58% |     0.917 |      0.20% |       99.78%\n",
            "----------------------------------------------------------------------------------------------------\n",
            "AVERAGE (excl. Clean)     |          - |          - |      0.84% |       99.08%\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   TPA (Full) - DETAILED RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "Noise Type                |   Accuracy |   F1-Score |       Drop |    Retention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Clean                     |     92.91% |     0.928 |      0.00% |      100.00%\n",
            "20% Gaussian              |     91.75% |     0.916 |      1.15% |       98.76%\n",
            "40% Gaussian              |     91.55% |     0.915 |      1.36% |       98.54%\n",
            "30% Temporal Mask         |     92.57% |     0.925 |      0.34% |       99.63%\n",
            "20% Channel Drift         |     92.67% |     0.926 |      0.24% |       99.74%\n",
            "20% Channel Drop          |     89.96% |     0.900 |      2.95% |       96.82%\n",
            "5% Spike Noise            |     91.52% |     0.915 |      1.39% |       98.50%\n",
            "----------------------------------------------------------------------------------------------------\n",
            "AVERAGE (excl. Clean)     |          - |          - |      1.24% |       98.67%\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "   TPA-Simple (No Conv) - DETAILED RESULTS\n",
            "====================================================================================================\n",
            "\n",
            "Noise Type                |   Accuracy |   F1-Score |       Drop |    Retention\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Clean                     |     95.22% |     0.952 |      0.00% |      100.00%\n",
            "20% Gaussian              |     93.93% |     0.939 |      1.29% |       98.65%\n",
            "40% Gaussian              |     92.98% |     0.930 |      2.24% |       97.65%\n",
            "30% Temporal Mask         |     94.91% |     0.949 |      0.31% |       99.68%\n",
            "20% Channel Drift         |     93.89% |     0.939 |      1.32% |       98.61%\n",
            "20% Channel Drop          |     91.69% |     0.918 |      3.53% |       96.29%\n",
            "5% Spike Noise            |     93.72% |     0.937 |      1.49% |       98.43%\n",
            "----------------------------------------------------------------------------------------------------\n",
            "AVERAGE (excl. Clean)     |          - |          - |      1.70% |       98.22%\n",
            "\n",
            "\n",
            "======================================================================================================================================================\n",
            "   COMPARATIVE ANALYSIS: GAP vs TPA vs TPA-Simple\n",
            "======================================================================================================================================================\n",
            "\n",
            "Noise Type                |   GAP Acc |  GAP Drop |   TPA Acc |  TPA Drop | Simple Acc | Simple Drop | Best Model  \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Clean                     |    91.79% |     0.00% |    92.91% |     0.00% |    95.22% |     0.00% | -           \n",
            "20% Gaussian              |    91.79% |     0.00% |    91.75% |     1.15% |    93.93% |     1.29% | GAP         \n",
            "40% Gaussian              |    91.69% |     0.10% |    91.55% |     1.36% |    92.98% |     2.24% | GAP         \n",
            "30% Temporal Mask         |    91.82% |    -0.03% |    92.57% |     0.34% |    94.91% |     0.31% | GAP         \n",
            "20% Channel Drift         |    91.55% |     0.24% |    92.67% |     0.24% |    93.89% |     1.32% | GAP         \n",
            "20% Channel Drop          |    87.24% |     4.55% |    89.96% |     2.95% |    91.69% |     3.53% | TPA         \n",
            "5% Spike Noise            |    91.58% |     0.20% |    91.52% |     1.39% |    93.72% |     1.49% | GAP         \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "AVERAGE (excl. Clean)     |         - |     0.84% |         - |     1.24% |         - |     1.70% | -           \n",
            "\n",
            "\n",
            "======================================================================================================================================================\n",
            "   SUMMARY STATISTICS\n",
            "======================================================================================================================================================\n",
            "\n",
            "Metric                         |    GAP Baseline |      TPA (Full) |      TPA-Simple | Best Model     \n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Clean Accuracy                 |          91.79% |          92.91% |          95.22% | TPA-Simple     \n",
            "Average Drop                   |           0.84% |           1.24% |           1.70% | GAP            \n",
            "Average Retention              |          99.08% |          98.67% |          98.22% | GAP            \n",
            "Robustness Score               |          96.89 |          96.94 |          97.32 | TPA-Simple     \n",
            "\n",
            "\n",
            "======================================================================================================================================================\n",
            "   IMPROVEMENT OVER GAP BASELINE\n",
            "======================================================================================================================================================\n",
            "\n",
            "Metric                         |           TPA (Full) |           TPA-Simple\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Clean Accuracy                 |               +1.12% |               +3.43%\n",
            "Average Drop Reduction         |               -0.40% |               -0.85%\n",
            "Average Retention Gain         |               -0.42% |               -0.86%\n",
            "Robustness Score Gain          |               +0.05 |               +0.42\n",
            "\n",
            "✓ Results saved to: /content/drive/MyDrive/AI_data/noise_robustness_study/three_models_comparison.csv\n",
            "\n",
            "\n",
            "======================================================================\n",
            "STUDY COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "Generated files:\n",
            "  1. /content/drive/MyDrive/AI_data/noise_robustness_study/three_models_comparison.csv\n",
            "  2. model_GAP_Baseline.pth\n",
            "  3. model_TPA.pth\n",
            "  4. model_TPA_Simple.pth\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TPA Noise Robustness Study: 3-Model Comparison\n",
        "Models: GAP (Baseline), TPA (Full), TPA-Simple (No Conv)\n",
        "Noise Types: Gaussian, Temporal Mask, Channel Drift, Channel Drop, Spike\n",
        "\"\"\"\n",
        "\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/noise_robustness_study\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    # Training augmentation (mixed noise)\n",
        "    train_augment_prob: float = 0.7\n",
        "    max_train_noise: float = 0.4\n",
        "\n",
        "    d_model: int = 128\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_seg_kernel: int = 9\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) Noise Augmentation Functions\n",
        "# ========================\n",
        "\n",
        "def add_gaussian_noise(X, noise_level):\n",
        "    \"\"\"Add Gaussian noise\"\"\"\n",
        "    if noise_level == 0:\n",
        "        return X\n",
        "    noise = np.random.normal(0, noise_level, X.shape).astype(np.float32)\n",
        "    return X + noise\n",
        "\n",
        "def add_temporal_mask(X, mask_ratio=0.3):\n",
        "    \"\"\"Add temporal masking\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    for i in range(B):\n",
        "        if np.random.rand() < 0.5:\n",
        "            mask_len = int(T * mask_ratio)\n",
        "            start = np.random.randint(0, T - mask_len)\n",
        "            X_aug[i, :, start:start+mask_len] = 0\n",
        "    return X_aug\n",
        "\n",
        "def add_channel_drift(X, drift_std=0.2):\n",
        "    \"\"\"Channel drift\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    for i in range(B):\n",
        "        for c in range(C):\n",
        "            if np.random.rand() < 0.3:\n",
        "                drift = np.random.normal(0, drift_std)\n",
        "                X_aug[i, c, :] += drift\n",
        "    return X_aug\n",
        "\n",
        "def add_channel_drop(X, drop_prob=0.2):\n",
        "    \"\"\"Channel drop\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    B, C, T = X.shape\n",
        "    for i in range(B):\n",
        "        drop_mask = np.random.rand(C) < drop_prob\n",
        "        X_aug[i, drop_mask, :] = 0\n",
        "    return X_aug\n",
        "\n",
        "def add_spike_noise(X, spike_prob=0.05, spike_magnitude=2.0):\n",
        "    \"\"\"Spike noise\"\"\"\n",
        "    X_aug = X.copy()\n",
        "    spike_mask = np.random.rand(*X.shape) < spike_prob\n",
        "    spikes = np.random.randn(*X.shape) * spike_magnitude\n",
        "    X_aug[spike_mask] += spikes[spike_mask]\n",
        "    return X_aug\n",
        "\n",
        "def apply_mixed_augmentation(X, max_noise=0.4):\n",
        "    \"\"\"Apply random combination of augmentations\"\"\"\n",
        "    aug_type = np.random.choice([\n",
        "        'gaussian', 'temporal', 'drift', 'drop', 'spike', 'mixed'\n",
        "    ])\n",
        "\n",
        "    if aug_type == 'gaussian':\n",
        "        noise_level = np.random.uniform(0, max_noise)\n",
        "        return add_gaussian_noise(X, noise_level)\n",
        "    elif aug_type == 'temporal':\n",
        "        mask_ratio = np.random.uniform(0.1, 0.4)\n",
        "        return add_temporal_mask(X, mask_ratio)\n",
        "    elif aug_type == 'drift':\n",
        "        drift_std = np.random.uniform(0.1, 0.3)\n",
        "        return add_channel_drift(X, drift_std)\n",
        "    elif aug_type == 'drop':\n",
        "        drop_prob = np.random.uniform(0.1, 0.3)\n",
        "        return add_channel_drop(X, drop_prob)\n",
        "    elif aug_type == 'spike':\n",
        "        spike_prob = np.random.uniform(0.02, 0.08)\n",
        "        return add_spike_noise(X, spike_prob)\n",
        "    else:  # mixed\n",
        "        X_aug = X.copy()\n",
        "        num_augs = np.random.randint(2, 4)\n",
        "        augs = np.random.choice(\n",
        "            ['gaussian', 'temporal', 'drift', 'spike'],\n",
        "            size=num_augs, replace=False\n",
        "        )\n",
        "        for aug in augs:\n",
        "            if aug == 'gaussian':\n",
        "                X_aug = add_gaussian_noise(X_aug, np.random.uniform(0, max_noise*0.5))\n",
        "            elif aug == 'temporal':\n",
        "                X_aug = add_temporal_mask(X_aug, np.random.uniform(0.1, 0.2))\n",
        "            elif aug == 'drift':\n",
        "                X_aug = add_channel_drift(X_aug, np.random.uniform(0.05, 0.15))\n",
        "            elif aug == 'spike':\n",
        "                X_aug = add_spike_noise(X_aug, np.random.uniform(0.02, 0.05))\n",
        "        return X_aug\n",
        "\n",
        "# ========================\n",
        "# 2) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/total_acc_z_\", \"txt\"), (\"Inertial Signals/body_acc_x_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None] for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None,\n",
        "                 augment: bool = False, max_noise: float = 0.4, augment_prob: float = 0.7):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "        self.augment = augment\n",
        "        self.max_noise = max_noise\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].copy()\n",
        "        y = self.y[idx]\n",
        "\n",
        "        if self.augment and np.random.rand() < self.augment_prob:\n",
        "            x = apply_mixed_augmentation(x[np.newaxis], self.max_noise)[0]\n",
        "\n",
        "        return torch.from_numpy(x).float(), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# ========================\n",
        "# 3) TPA Module\n",
        "# ========================\n",
        "class ProductionTPA(nn.Module):\n",
        "    \"\"\"Temporal Prototype Attention (Full version with conv layers)\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, seg_kernel=9, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        pad = (seg_kernel - 1) // 2\n",
        "        self.lowpass = nn.Conv1d(dim, dim, kernel_size=5, padding=2, groups=dim, bias=False)\n",
        "        self.dw = nn.Conv1d(dim, dim, kernel_size=seg_kernel, padding=pad, groups=dim, bias=False)\n",
        "        self.pw = nn.Conv1d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, return_confidence=False):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_filtered = self.lowpass(x.transpose(1, 2)).transpose(1, 2)\n",
        "        xloc = self.pw(self.dw(x_filtered.transpose(1, 2))).transpose(1, 2)\n",
        "        xloc = self.pre_norm(xloc) + x\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "\n",
        "class SimpleTPA(nn.Module):\n",
        "    \"\"\"Simplified TPA without lowpass, depthwise, pointwise convolutions\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, return_confidence=False):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        # Simply normalize without conv layers\n",
        "        xloc = self.pre_norm(x)\n",
        "\n",
        "        K = self.k_proj(xloc)\n",
        "        V = self.v_proj(xloc)\n",
        "\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)\n",
        "\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        z_gap = x.mean(dim=1)\n",
        "\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 4) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1)) for k in branches])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        pooled = features.mean(dim=1)\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "class TPAHead(nn.Module):\n",
        "    \"\"\"TPA: Temporal Prototype Attention\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, seg_kernel: int = 9,\n",
        "                 heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25,\n",
        "                 use_simple: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        if use_simple:\n",
        "            self.tpa = SimpleTPA(\n",
        "                dim=d_model,\n",
        "                num_prototypes=num_prototypes,\n",
        "                heads=heads,\n",
        "                dropout=dropout,\n",
        "                temperature=temperature,\n",
        "                topk_ratio=topk_ratio\n",
        "            )\n",
        "        else:\n",
        "            self.tpa = ProductionTPA(\n",
        "                dim=d_model,\n",
        "                num_prototypes=num_prototypes,\n",
        "                seg_kernel=seg_kernel,\n",
        "                heads=heads,\n",
        "                dropout=dropout,\n",
        "                temperature=temperature,\n",
        "                topk_ratio=topk_ratio\n",
        "            )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        \"\"\"\n",
        "        Fmap: [B, D, T]\n",
        "        \"\"\"\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        z = self.tpa(features, return_confidence=False)\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, model_type='gap', tpa_config=None, use_simple_tpa=False):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.model_type = model_type\n",
        "\n",
        "        if model_type == 'gap':\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "        else:  # tpa\n",
        "            self.head = TPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                seg_kernel=tpa_config.get('seg_kernel', 9),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25),\n",
        "                use_simple=use_simple_tpa\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = self.backbone(x)\n",
        "        return self.head(fmap)\n",
        "\n",
        "# ========================\n",
        "# 5) Train / Eval\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(cfg.device).float()\n",
        "        y = y.to(cfg.device)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(cfg.device)\n",
        "        y = y.to(cfg.device)\n",
        "\n",
        "        logits = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "# ========================\n",
        "# 6) Results Display Functions\n",
        "# ========================\n",
        "\n",
        "def print_model_results(model_name, results_list, clean_acc):\n",
        "    \"\"\"Print detailed results for a single model\"\"\"\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"   {model_name} - DETAILED RESULTS\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    print(f\"{'Noise Type':<25} | {'Accuracy':>10} | {'F1-Score':>10} | {'Drop':>10} | {'Retention':>12}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    total_drop = 0\n",
        "    total_retention = 0\n",
        "    noise_count = 0\n",
        "\n",
        "    for result in results_list:\n",
        "        noise_type = result['noise_type']\n",
        "        acc = result['accuracy']\n",
        "        f1 = result['f1']\n",
        "\n",
        "        if noise_type == 'Clean':\n",
        "            drop = 0.0\n",
        "            retention = 100.0\n",
        "        else:\n",
        "            drop = clean_acc - acc\n",
        "            retention = (acc / clean_acc) * 100\n",
        "            total_drop += drop\n",
        "            total_retention += retention\n",
        "            noise_count += 1\n",
        "\n",
        "        print(f\"{noise_type:<25} | {acc:9.2f}% | {f1:9.3f} | {drop:9.2f}% | {retention:11.2f}%\")\n",
        "\n",
        "    # Print averages\n",
        "    if noise_count > 0:\n",
        "        avg_drop = total_drop / noise_count\n",
        "        avg_retention = total_retention / noise_count\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"{'AVERAGE (excl. Clean)':<25} | {'-':>10} | {'-':>10} | {avg_drop:9.2f}% | {avg_retention:11.2f}%\")\n",
        "\n",
        "    print()\n",
        "\n",
        "def print_comparison_table(gap_results, tpa_results, tpa_simple_results):\n",
        "    \"\"\"Print side-by-side comparison of all three models\"\"\"\n",
        "    print(f\"\\n{'='*150}\")\n",
        "    print(f\"   COMPARATIVE ANALYSIS: GAP vs TPA vs TPA-Simple\")\n",
        "    print(f\"{'='*150}\\n\")\n",
        "\n",
        "    # Get clean accuracies\n",
        "    gap_clean = next(r['accuracy'] for r in gap_results if r['noise_type'] == 'Clean')\n",
        "    tpa_clean = next(r['accuracy'] for r in tpa_results if r['noise_type'] == 'Clean')\n",
        "    tpa_simple_clean = next(r['accuracy'] for r in tpa_simple_results if r['noise_type'] == 'Clean')\n",
        "\n",
        "    print(f\"{'Noise Type':<25} | {'GAP Acc':>9} | {'GAP Drop':>9} | {'TPA Acc':>9} | {'TPA Drop':>9} | \"\n",
        "          f\"{'Simple Acc':>9} | {'Simple Drop':>9} | {'Best Model':<12}\")\n",
        "    print(\"-\" * 150)\n",
        "\n",
        "    gap_total_drop = 0\n",
        "    tpa_total_drop = 0\n",
        "    tpa_simple_total_drop = 0\n",
        "    noise_count = 0\n",
        "\n",
        "    for gap_r, tpa_r, tpa_simple_r in zip(gap_results, tpa_results, tpa_simple_results):\n",
        "        noise_type = gap_r['noise_type']\n",
        "        gap_acc = gap_r['accuracy']\n",
        "        tpa_acc = tpa_r['accuracy']\n",
        "        tpa_simple_acc = tpa_simple_r['accuracy']\n",
        "\n",
        "        if noise_type == 'Clean':\n",
        "            gap_drop = 0.0\n",
        "            tpa_drop = 0.0\n",
        "            tpa_simple_drop = 0.0\n",
        "            best_model = \"-\"\n",
        "        else:\n",
        "            gap_drop = gap_clean - gap_acc\n",
        "            tpa_drop = tpa_clean - tpa_acc\n",
        "            tpa_simple_drop = tpa_simple_clean - tpa_simple_acc\n",
        "\n",
        "            gap_total_drop += gap_drop\n",
        "            tpa_total_drop += tpa_drop\n",
        "            tpa_simple_total_drop += tpa_simple_drop\n",
        "            noise_count += 1\n",
        "\n",
        "            # Find best (lowest drop)\n",
        "            drops = {'GAP': gap_drop, 'TPA': tpa_drop, 'TPA-Simple': tpa_simple_drop}\n",
        "            best_model = min(drops, key=drops.get)\n",
        "\n",
        "        print(f\"{noise_type:<25} | {gap_acc:8.2f}% | {gap_drop:8.2f}% | {tpa_acc:8.2f}% | {tpa_drop:8.2f}% | \"\n",
        "              f\"{tpa_simple_acc:8.2f}% | {tpa_simple_drop:8.2f}% | {best_model:<12}\")\n",
        "\n",
        "    # Print averages\n",
        "    if noise_count > 0:\n",
        "        gap_avg_drop = gap_total_drop / noise_count\n",
        "        tpa_avg_drop = tpa_total_drop / noise_count\n",
        "        tpa_simple_avg_drop = tpa_simple_total_drop / noise_count\n",
        "\n",
        "        print(\"-\" * 150)\n",
        "        print(f\"{'AVERAGE (excl. Clean)':<25} | {'-':>9} | {gap_avg_drop:8.2f}% | {'-':>9} | {tpa_avg_drop:8.2f}% | \"\n",
        "              f\"{'-':>9} | {tpa_simple_avg_drop:8.2f}% | {'-':<12}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\n{'='*150}\")\n",
        "    print(f\"   SUMMARY STATISTICS\")\n",
        "    print(f\"{'='*150}\\n\")\n",
        "\n",
        "    gap_avg_retention = ((gap_clean - gap_avg_drop) / gap_clean) * 100\n",
        "    tpa_avg_retention = ((tpa_clean - tpa_avg_drop) / tpa_clean) * 100\n",
        "    tpa_simple_avg_retention = ((tpa_simple_clean - tpa_simple_avg_drop) / tpa_simple_clean) * 100\n",
        "\n",
        "    print(f\"{'Metric':<30} | {'GAP Baseline':>15} | {'TPA (Full)':>15} | {'TPA-Simple':>15} | {'Best Model':<15}\")\n",
        "    print(\"-\" * 150)\n",
        "\n",
        "    # Clean Accuracy\n",
        "    clean_best = max([('GAP', gap_clean), ('TPA', tpa_clean), ('TPA-Simple', tpa_simple_clean)], key=lambda x: x[1])[0]\n",
        "    print(f\"{'Clean Accuracy':<30} | {gap_clean:14.2f}% | {tpa_clean:14.2f}% | {tpa_simple_clean:14.2f}% | {clean_best:<15}\")\n",
        "\n",
        "    # Average Drop\n",
        "    drop_best = min([('GAP', gap_avg_drop), ('TPA', tpa_avg_drop), ('TPA-Simple', tpa_simple_avg_drop)], key=lambda x: x[1])[0]\n",
        "    print(f\"{'Average Drop':<30} | {gap_avg_drop:14.2f}% | {tpa_avg_drop:14.2f}% | {tpa_simple_avg_drop:14.2f}% | {drop_best:<15}\")\n",
        "\n",
        "    # Average Retention\n",
        "    retention_best = max([('GAP', gap_avg_retention), ('TPA', tpa_avg_retention), ('TPA-Simple', tpa_simple_avg_retention)], key=lambda x: x[1])[0]\n",
        "    print(f\"{'Average Retention':<30} | {gap_avg_retention:14.2f}% | {tpa_avg_retention:14.2f}% | {tpa_simple_avg_retention:14.2f}% | {retention_best:<15}\")\n",
        "\n",
        "    # Robustness score\n",
        "    gap_robustness = gap_clean * 0.3 + gap_avg_retention * 0.7\n",
        "    tpa_robustness = tpa_clean * 0.3 + tpa_avg_retention * 0.7\n",
        "    tpa_simple_robustness = tpa_simple_clean * 0.3 + tpa_simple_avg_retention * 0.7\n",
        "\n",
        "    robustness_best = max([('GAP', gap_robustness), ('TPA', tpa_robustness), ('TPA-Simple', tpa_simple_robustness)], key=lambda x: x[1])[0]\n",
        "    print(f\"{'Robustness Score':<30} | {gap_robustness:14.2f} | {tpa_robustness:14.2f} | {tpa_simple_robustness:14.2f} | {robustness_best:<15}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Improvement analysis\n",
        "    print(f\"\\n{'='*150}\")\n",
        "    print(f\"   IMPROVEMENT OVER GAP BASELINE\")\n",
        "    print(f\"{'='*150}\\n\")\n",
        "\n",
        "    print(f\"{'Metric':<30} | {'TPA (Full)':>20} | {'TPA-Simple':>20}\")\n",
        "    print(\"-\" * 150)\n",
        "    print(f\"{'Clean Accuracy':<30} | {tpa_clean - gap_clean:+19.2f}% | {tpa_simple_clean - gap_clean:+19.2f}%\")\n",
        "    print(f\"{'Average Drop Reduction':<30} | {gap_avg_drop - tpa_avg_drop:+19.2f}% | {gap_avg_drop - tpa_simple_avg_drop:+19.2f}%\")\n",
        "    print(f\"{'Average Retention Gain':<30} | {tpa_avg_retention - gap_avg_retention:+19.2f}% | {tpa_simple_avg_retention - gap_avg_retention:+19.2f}%\")\n",
        "    print(f\"{'Robustness Score Gain':<30} | {tpa_robustness - gap_robustness:+19.2f} | {tpa_simple_robustness - gap_robustness:+19.2f}\")\n",
        "    print()\n",
        "\n",
        "# ========================\n",
        "# 7) Noise Robustness Study\n",
        "# ========================\n",
        "def run_noise_robustness_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)\n",
        "\n",
        "    indices = np.arange(len(X_full))\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_full\n",
        "    )\n",
        "\n",
        "    print(f\"Total samples: {len(X_full)}\")\n",
        "    print(f\"  → Train: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices,\n",
        "        augment=True,\n",
        "        max_noise=cfg.max_train_noise,\n",
        "        augment_prob=cfg.train_augment_prob\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std, augment=False)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    # Define noise configurations for testing\n",
        "    noise_configs = [\n",
        "        {'name': 'Clean', 'type': 'none', 'level': 0.0},\n",
        "        {'name': '20% Gaussian', 'type': 'gaussian', 'level': 0.2},\n",
        "        {'name': '40% Gaussian', 'type': 'gaussian', 'level': 0.4},\n",
        "        {'name': '30% Temporal Mask', 'type': 'temporal', 'level': 0.3},\n",
        "        {'name': '20% Channel Drift', 'type': 'drift', 'level': 0.2},\n",
        "        {'name': '20% Channel Drop', 'type': 'drop', 'level': 0.2},\n",
        "        {'name': '5% Spike Noise', 'type': 'spike', 'level': 0.05},\n",
        "    ]\n",
        "\n",
        "    # Define 3 model configurations\n",
        "    model_configs = [\n",
        "        {\"name\": \"GAP_Baseline\", \"model_type\": \"gap\", \"use_simple\": False},\n",
        "        {\"name\": \"TPA\", \"model_type\": \"tpa\", \"use_simple\": False},\n",
        "        {\"name\": \"TPA_Simple\", \"model_type\": \"tpa\", \"use_simple\": True},\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    NOISE ROBUSTNESS STUDY: 3-Model Comparison\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Models: GAP (Baseline), TPA (Full), TPA-Simple (No Conv)\")\n",
        "    print(f\"   Training: Mixed noise augmentation (prob={cfg.train_augment_prob}, max={cfg.max_train_noise})\")\n",
        "    print(f\"   Testing: 7 noise scenarios\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Store results for each model\n",
        "    gap_results = []\n",
        "    tpa_results = []\n",
        "    tpa_simple_results = []\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for model_cfg in model_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   MODEL: {model_cfg['name']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'seg_kernel': cfg.tpa_seg_kernel,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            model_type=model_cfg[\"model_type\"],\n",
        "            tpa_config=tpa_config,\n",
        "            use_simple_tpa=model_cfg.get(\"use_simple\", False)\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True,\n",
        "                                 num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {model_cfg['name']} for up to {cfg.epochs} epochs (patience={cfg.patience})...\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg)\n",
        "            val_acc, val_f1 = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "            log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "            if improved:\n",
        "                log_str += \" ✓\"\n",
        "\n",
        "            if epoch % 10 == 0 or epoch == 1:\n",
        "                print(log_str)\n",
        "\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            model_path = os.path.join(cfg.save_dir, f\"model_{model_cfg['name']}.pth\")\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Test on all noise scenarios\n",
        "        print(f\"\\n   Testing on {len(noise_configs)} noise scenarios...\")\n",
        "\n",
        "        model_results = []\n",
        "\n",
        "        for noise_cfg in noise_configs:\n",
        "            X_test = test_set_orig.X.copy()\n",
        "\n",
        "            # Apply noise\n",
        "            if noise_cfg['type'] == 'gaussian':\n",
        "                X_test = add_gaussian_noise(X_test, noise_cfg['level'])\n",
        "            elif noise_cfg['type'] == 'temporal':\n",
        "                X_test = add_temporal_mask(X_test, noise_cfg['level'])\n",
        "            elif noise_cfg['type'] == 'drift':\n",
        "                X_test = add_channel_drift(X_test, noise_cfg['level'])\n",
        "            elif noise_cfg['type'] == 'drop':\n",
        "                X_test = add_channel_drop(X_test, noise_cfg['level'])\n",
        "            elif noise_cfg['type'] == 'spike':\n",
        "                X_test = add_spike_noise(X_test, noise_cfg['level'])\n",
        "\n",
        "            # Create test dataset\n",
        "            test_ds_noisy = UCIHARInertial(\n",
        "                cfg.data_dir, \"test\",\n",
        "                mean=mean, std=std,\n",
        "                preloaded_data=(X_test, test_set_orig.y),\n",
        "                augment=False\n",
        "            )\n",
        "            test_loader_noisy = DataLoader(test_ds_noisy, cfg.batch_size,\n",
        "                                          num_workers=cfg.num_workers)\n",
        "\n",
        "            acc, f1 = evaluate(model, test_loader_noisy, cfg)\n",
        "\n",
        "            model_results.append({\n",
        "                'noise_type': noise_cfg['name'],\n",
        "                'accuracy': acc * 100,\n",
        "                'f1': f1\n",
        "            })\n",
        "\n",
        "        # Store results\n",
        "        if model_cfg['name'] == 'GAP_Baseline':\n",
        "            gap_results = model_results\n",
        "        elif model_cfg['name'] == 'TPA':\n",
        "            tpa_results = model_results\n",
        "        else:\n",
        "            tpa_simple_results = model_results\n",
        "\n",
        "    # ========================\n",
        "    # Display Results\n",
        "    # ========================\n",
        "\n",
        "    # Get clean accuracies\n",
        "    gap_clean = next(r['accuracy'] for r in gap_results if r['noise_type'] == 'Clean')\n",
        "    tpa_clean = next(r['accuracy'] for r in tpa_results if r['noise_type'] == 'Clean')\n",
        "    tpa_simple_clean = next(r['accuracy'] for r in tpa_simple_results if r['noise_type'] == 'Clean')\n",
        "\n",
        "    # Print individual model results\n",
        "    print_model_results(\"GAP BASELINE\", gap_results, gap_clean)\n",
        "    print_model_results(\"TPA (Full)\", tpa_results, tpa_clean)\n",
        "    print_model_results(\"TPA-Simple (No Conv)\", tpa_simple_results, tpa_simple_clean)\n",
        "\n",
        "    # Print comparison table\n",
        "    print_comparison_table(gap_results, tpa_results, tpa_simple_results)\n",
        "\n",
        "    # Save results to CSV\n",
        "    all_results = []\n",
        "    for gap_r, tpa_r, tpa_simple_r in zip(gap_results, tpa_results, tpa_simple_results):\n",
        "        noise_type = gap_r['noise_type']\n",
        "\n",
        "        # GAP metrics\n",
        "        gap_acc = gap_r['accuracy']\n",
        "        gap_f1 = gap_r['f1']\n",
        "        gap_drop = 0.0 if noise_type == 'Clean' else gap_clean - gap_acc\n",
        "        gap_retention = 100.0 if noise_type == 'Clean' else (gap_acc / gap_clean) * 100\n",
        "\n",
        "        # TPA metrics\n",
        "        tpa_acc = tpa_r['accuracy']\n",
        "        tpa_f1 = tpa_r['f1']\n",
        "        tpa_drop = 0.0 if noise_type == 'Clean' else tpa_clean - tpa_acc\n",
        "        tpa_retention = 100.0 if noise_type == 'Clean' else (tpa_acc / tpa_clean) * 100\n",
        "\n",
        "        # TPA-Simple metrics\n",
        "        tpa_simple_acc = tpa_simple_r['accuracy']\n",
        "        tpa_simple_f1 = tpa_simple_r['f1']\n",
        "        tpa_simple_drop = 0.0 if noise_type == 'Clean' else tpa_simple_clean - tpa_simple_acc\n",
        "        tpa_simple_retention = 100.0 if noise_type == 'Clean' else (tpa_simple_acc / tpa_simple_clean) * 100\n",
        "\n",
        "        all_results.append({\n",
        "            'Noise Type': noise_type,\n",
        "            'GAP Accuracy (%)': gap_acc,\n",
        "            'GAP F1-Score': gap_f1,\n",
        "            'GAP Drop (%)': gap_drop,\n",
        "            'GAP Retention (%)': gap_retention,\n",
        "            'TPA Accuracy (%)': tpa_acc,\n",
        "            'TPA F1-Score': tpa_f1,\n",
        "            'TPA Drop (%)': tpa_drop,\n",
        "            'TPA Retention (%)': tpa_retention,\n",
        "            'TPA-Simple Accuracy (%)': tpa_simple_acc,\n",
        "            'TPA-Simple F1-Score': tpa_simple_f1,\n",
        "            'TPA-Simple Drop (%)': tpa_simple_drop,\n",
        "            'TPA-Simple Retention (%)': tpa_simple_retention\n",
        "        })\n",
        "\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "    csv_path = os.path.join(cfg.save_dir, 'three_models_comparison.csv')\n",
        "    df_results.to_csv(csv_path, index=False)\n",
        "    print(f\"✓ Results saved to: {csv_path}\\n\")\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# ========================\n",
        "# 8) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    # Training augmentation settings\n",
        "    config.train_augment_prob = 0.7\n",
        "    config.max_train_noise = 0.4\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_seg_kernel = 9\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    NOISE ROBUSTNESS STUDY: 3-Model Comparison\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"Training Augmentation: prob={config.train_augment_prob}, max_noise={config.max_train_noise}\")\n",
        "    print(f\"\\n3 Models to Compare:\")\n",
        "    print(f\"  1) GAP Baseline:  Standard Global Average Pooling\")\n",
        "    print(f\"  2) TPA (Full):    Temporal Prototype Attention with Conv layers\")\n",
        "    print(f\"  3) TPA-Simple:    TPA without lowpass/depthwise/pointwise conv\")\n",
        "    print(f\"\\n7 Noise Scenarios:\")\n",
        "    print(f\"  - Clean (no noise)\")\n",
        "    print(f\"  - Gaussian noise (20%, 40%)\")\n",
        "    print(f\"  - Temporal mask (30%)\")\n",
        "    print(f\"  - Channel drift (20%)\")\n",
        "    print(f\"  - Channel drop (20%)\")\n",
        "    print(f\"  - Spike noise (5%)\")\n",
        "    print(f\"\\nTPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Run study\n",
        "    df_results = run_noise_robustness_study(config)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STUDY COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(f\"  1. {os.path.join(config.save_dir, 'three_models_comparison.csv')}\")\n",
        "    print(f\"  2. model_GAP_Baseline.pth\")\n",
        "    print(f\"  3. model_TPA.pth\")\n",
        "    print(f\"  4. model_TPA_Simple.pth\")\n",
        "    print(\"=\"*70 + \"\\n\")"
      ]
    }
  ]
}