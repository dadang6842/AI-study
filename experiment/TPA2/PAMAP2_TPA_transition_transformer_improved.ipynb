{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "supE8VPD0o0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08443041-22f1-47d1-be6e-638c2c354e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "================================================================================\n",
            "SIMPLIFIED MODEL COMPARISON: GAP vs TPA vs Gated-TPA\n",
            "================================================================================\n",
            "\n",
            "개선사항:\n",
            "  1. TPA Top-k 마스킹 적용\n",
            "  2. 프로토타입 다양성 페널티\n",
            "  3. 로짓 수준 MoE 융합 (별도 분류기)\n",
            "================================================================================\n",
            "\n",
            "Total datasets to test: 57\n",
            "  - transitions: 58\n",
            "\n",
            "[Progress: 1/57]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: ORIGINAL\n",
            "================================================================================\n",
            "\n",
            "Loading ORIGINAL...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets/ORIGINAL\n",
            "  Train: (31084, 100, 27), Test: (7772, 100, 27)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 24867, Val: 6217, Test: 7772\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPLEXITY COMPARISON\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPLEXITY ANALYSIS: GAP\n",
            "================================================================================\n",
            "Total Parameters: 270,348\n",
            "Trainable Parameters: 270,348\n",
            "Model Size: 1.03 MB (float32)\n",
            "FLOPs: 272,128 (0.27 MFLOPs)\n",
            "Inference Time: 0.885 ms (avg over 100 runs)\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPLEXITY ANALYSIS: TPA\n",
            "================================================================================\n",
            "Total Parameters: 354,828\n",
            "Trainable Parameters: 354,828\n",
            "Model Size: 1.35 MB (float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 927,488 (0.93 MFLOPs)\n",
            "Inference Time: 2.187 ms (avg over 100 runs)\n",
            "\n",
            "================================================================================\n",
            "MODEL COMPLEXITY ANALYSIS: Gated-TPA\n",
            "================================================================================\n",
            "Total Parameters: 359,460\n",
            "Trainable Parameters: 359,460\n",
            "Model Size: 1.37 MB (float32)\n",
            "FLOPs: 936,704 (0.94 MFLOPs)\n",
            "Inference Time: 2.093 ms (avg over 100 runs)\n",
            "\n",
            "[Training GAP]\n",
            "  Epoch  10: Train Acc=0.9219, Loss=0.5723, Val Acc=0.9213, F1=0.9125\n",
            "  Epoch  20: Train Acc=0.9579, Loss=0.4494, Val Acc=0.9472, F1=0.9408\n",
            "  Epoch  30: Train Acc=0.9745, Loss=0.4010, Val Acc=0.9590, F1=0.9540\n",
            "  Epoch  40: Train Acc=0.9848, Loss=0.3714, Val Acc=0.9633, F1=0.9596\n",
            "  Epoch  50: Train Acc=0.9901, Loss=0.3530, Val Acc=0.9691, F1=0.9657\n",
            "  Epoch  60: Train Acc=0.9926, Loss=0.3414, Val Acc=0.9743, F1=0.9717\n",
            "  Epoch  70: Train Acc=0.9945, Loss=0.3324, Val Acc=0.9768, F1=0.9741\n",
            "  Epoch  80: Train Acc=0.9955, Loss=0.3263, Val Acc=0.9796, F1=0.9774\n",
            "  Epoch  90: Train Acc=0.9962, Loss=0.3222, Val Acc=0.9802, F1=0.9774\n",
            "  Epoch 100: Train Acc=0.9967, Loss=0.3186, Val Acc=0.9807, F1=0.9787\n",
            "  Best Val Acc: 0.9818\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9818\n",
            "  Test Acc: 0.9785, F1: 0.9764\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9254, CE=0.5463, Div=0.0684, Val Acc=0.9178, F1=0.9069\n",
            "  Epoch  20: Train Acc=0.9580, CE=0.4487, Div=0.0168, Val Acc=0.9447, F1=0.9378\n",
            "  Epoch  30: Train Acc=0.9719, CE=0.4072, Div=0.0081, Val Acc=0.9545, F1=0.9481\n",
            "  Epoch  40: Train Acc=0.9828, CE=0.3793, Div=0.0053, Val Acc=0.9664, F1=0.9623\n",
            "  Epoch  50: Train Acc=0.9888, CE=0.3609, Div=0.0027, Val Acc=0.9699, F1=0.9672\n",
            "  Epoch  60: Train Acc=0.9923, CE=0.3479, Div=0.0018, Val Acc=0.9736, F1=0.9707\n",
            "  Epoch  70: Train Acc=0.9939, CE=0.3395, Div=0.0012, Val Acc=0.9749, F1=0.9731\n",
            "  Epoch  80: Train Acc=0.9953, CE=0.3333, Div=0.0005, Val Acc=0.9751, F1=0.9728\n",
            "  Epoch  90: Train Acc=0.9960, CE=0.3296, Div=0.0003, Val Acc=0.9788, F1=0.9779\n",
            "  Epoch 100: Train Acc=0.9961, CE=0.3256, Div=0.0002, Val Acc=0.9762, F1=0.9747\n",
            "  Best Val Acc: 0.9797\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9797\n",
            "  Test Acc: 0.9811, F1: 0.9796\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9241, CE=0.5405, Div=0.0434, Val Acc=0.9207, F1=0.9122\n",
            "  Epoch  20: Train Acc=0.9550, CE=0.4414, Div=0.0083, Val Acc=0.9474, F1=0.9406\n",
            "  Epoch  30: Train Acc=0.9745, CE=0.3936, Div=0.0029, Val Acc=0.9543, F1=0.9490\n",
            "  Epoch  40: Train Acc=0.9847, CE=0.3653, Div=0.0017, Val Acc=0.9665, F1=0.9627\n",
            "  Epoch  50: Train Acc=0.9902, CE=0.3482, Div=0.0007, Val Acc=0.9693, F1=0.9661\n",
            "  Epoch  60: Train Acc=0.9934, CE=0.3362, Div=0.0008, Val Acc=0.9715, F1=0.9685\n",
            "  Epoch  70: Train Acc=0.9943, CE=0.3291, Div=0.0003, Val Acc=0.9722, F1=0.9694\n",
            "  Epoch  80: Train Acc=0.9950, CE=0.3233, Div=0.0002, Val Acc=0.9746, F1=0.9727\n",
            "  Epoch  90: Train Acc=0.9961, CE=0.3185, Div=0.0001, Val Acc=0.9727, F1=0.9697\n",
            "  Epoch 100: Train Acc=0.9965, CE=0.3156, Div=0.0001, Val Acc=0.9762, F1=0.9739\n",
            "  Best Val Acc: 0.9775\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9775\n",
            "  Test Acc: 0.9789, F1: 0.9767\n",
            "\n",
            "[Progress: 2/57]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: Standing_TO_Sitting_10PCT\n",
            "================================================================================\n",
            "\n",
            "Loading Standing_TO_Sitting_10PCT...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets/Standing_TO_Sitting_10PCT\n",
            "  Train: (34192, 100, 27), Test: (8549, 100, 27)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 27353, Val: 6839, Test: 8549\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9295, Loss=0.5523, Val Acc=0.9275, F1=0.9184\n",
            "  Epoch  20: Train Acc=0.9588, Loss=0.4446, Val Acc=0.9466, F1=0.9413\n",
            "  Epoch  30: Train Acc=0.9763, Loss=0.3961, Val Acc=0.9639, F1=0.9592\n",
            "  Epoch  40: Train Acc=0.9853, Loss=0.3672, Val Acc=0.9689, F1=0.9643\n",
            "  Epoch  50: Train Acc=0.9901, Loss=0.3500, Val Acc=0.9734, F1=0.9697\n",
            "  Epoch  60: Train Acc=0.9929, Loss=0.3386, Val Acc=0.9773, F1=0.9759\n",
            "  Epoch  70: Train Acc=0.9948, Loss=0.3304, Val Acc=0.9784, F1=0.9762\n",
            "  Epoch  80: Train Acc=0.9948, Loss=0.3252, Val Acc=0.9804, F1=0.9778\n",
            "  Epoch  90: Train Acc=0.9965, Loss=0.3200, Val Acc=0.9827, F1=0.9809\n",
            "  Epoch 100: Train Acc=0.9965, Loss=0.3176, Val Acc=0.9838, F1=0.9820\n",
            "  Best Val Acc: 0.9838\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9838\n",
            "  Test Acc: 0.9820, F1: 0.9805\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9292, CE=0.5433, Div=0.0410, Val Acc=0.9251, F1=0.9146\n",
            "  Epoch  20: Train Acc=0.9592, CE=0.4437, Div=0.0070, Val Acc=0.9469, F1=0.9397\n",
            "  Epoch  30: Train Acc=0.9748, CE=0.4000, Div=0.0031, Val Acc=0.9574, F1=0.9511\n",
            "  Epoch  40: Train Acc=0.9834, CE=0.3736, Div=0.0014, Val Acc=0.9656, F1=0.9610\n",
            "  Epoch  50: Train Acc=0.9886, CE=0.3574, Div=0.0007, Val Acc=0.9741, F1=0.9711\n",
            "  Epoch  60: Train Acc=0.9918, CE=0.3458, Div=0.0006, Val Acc=0.9749, F1=0.9719\n",
            "  Epoch  70: Train Acc=0.9939, CE=0.3381, Div=0.0006, Val Acc=0.9789, F1=0.9761\n",
            "  Epoch  80: Train Acc=0.9952, CE=0.3321, Div=0.0004, Val Acc=0.9788, F1=0.9763\n",
            "  Epoch  90: Train Acc=0.9958, CE=0.3276, Div=0.0002, Val Acc=0.9798, F1=0.9773\n",
            "  Epoch 100: Train Acc=0.9966, CE=0.3246, Div=0.0002, Val Acc=0.9825, F1=0.9800\n",
            "  Best Val Acc: 0.9827\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9827\n",
            "  Test Acc: 0.9807, F1: 0.9788\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9299, CE=0.5241, Div=0.0283, Val Acc=0.9300, F1=0.9214\n",
            "  Epoch  20: Train Acc=0.9618, CE=0.4275, Div=0.0060, Val Acc=0.9498, F1=0.9445\n",
            "  Epoch  30: Train Acc=0.9778, CE=0.3866, Div=0.0020, Val Acc=0.9637, F1=0.9600\n",
            "  Epoch  40: Train Acc=0.9859, CE=0.3595, Div=0.0010, Val Acc=0.9703, F1=0.9671\n",
            "  Epoch  50: Train Acc=0.9910, CE=0.3439, Div=0.0008, Val Acc=0.9749, F1=0.9724\n",
            "  Epoch  60: Train Acc=0.9932, CE=0.3328, Div=0.0008, Val Acc=0.9765, F1=0.9744\n",
            "  Epoch  70: Train Acc=0.9945, CE=0.3255, Div=0.0003, Val Acc=0.9787, F1=0.9772\n",
            "  Epoch  80: Train Acc=0.9954, CE=0.3196, Div=0.0000, Val Acc=0.9795, F1=0.9784\n",
            "  Epoch  90: Train Acc=0.9961, CE=0.3161, Div=0.0001, Val Acc=0.9806, F1=0.9793\n",
            "  Epoch 100: Train Acc=0.9966, CE=0.3132, Div=0.0000, Val Acc=0.9797, F1=0.9784\n",
            "  Best Val Acc: 0.9816\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9816\n",
            "  Test Acc: 0.9823, F1: 0.9808\n",
            "\n",
            "[Progress: 3/57]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: Standing_TO_Sitting_20PCT\n",
            "================================================================================\n",
            "\n",
            "Loading Standing_TO_Sitting_20PCT...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets/Standing_TO_Sitting_20PCT\n",
            "  Train: (34192, 100, 27), Test: (8549, 100, 27)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 27353, Val: 6839, Test: 8549\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9250, Loss=0.5563, Val Acc=0.9226, F1=0.9150\n",
            "  Epoch  20: Train Acc=0.9578, Loss=0.4456, Val Acc=0.9456, F1=0.9393\n",
            "  Epoch  30: Train Acc=0.9755, Loss=0.3943, Val Acc=0.9592, F1=0.9550\n",
            "  Epoch  40: Train Acc=0.9862, Loss=0.3662, Val Acc=0.9696, F1=0.9666\n",
            "  Epoch  50: Train Acc=0.9912, Loss=0.3481, Val Acc=0.9735, F1=0.9712\n",
            "  Epoch  60: Train Acc=0.9940, Loss=0.3366, Val Acc=0.9763, F1=0.9743\n",
            "  Epoch  70: Train Acc=0.9947, Loss=0.3300, Val Acc=0.9770, F1=0.9751\n",
            "  Epoch  80: Train Acc=0.9966, Loss=0.3232, Val Acc=0.9772, F1=0.9757\n",
            "  Epoch  90: Train Acc=0.9968, Loss=0.3196, Val Acc=0.9785, F1=0.9767\n",
            "  Epoch 100: Train Acc=0.9971, Loss=0.3162, Val Acc=0.9792, F1=0.9776\n",
            "  Best Val Acc: 0.9806\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9806\n",
            "  Test Acc: 0.9786, F1: 0.9775\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9274, CE=0.5402, Div=0.0348, Val Acc=0.9266, F1=0.9177\n",
            "  Epoch  20: Train Acc=0.9596, CE=0.4429, Div=0.0081, Val Acc=0.9477, F1=0.9417\n",
            "  Epoch  30: Train Acc=0.9752, CE=0.3984, Div=0.0046, Val Acc=0.9633, F1=0.9593\n",
            "  Epoch  40: Train Acc=0.9849, CE=0.3721, Div=0.0022, Val Acc=0.9683, F1=0.9654\n",
            "  Epoch  50: Train Acc=0.9906, CE=0.3551, Div=0.0015, Val Acc=0.9741, F1=0.9721\n",
            "  Epoch  60: Train Acc=0.9933, CE=0.3444, Div=0.0009, Val Acc=0.9727, F1=0.9711\n",
            "  Epoch  70: Train Acc=0.9948, CE=0.3372, Div=0.0005, Val Acc=0.9779, F1=0.9772\n",
            "  Epoch  80: Train Acc=0.9952, CE=0.3315, Div=0.0005, Val Acc=0.9763, F1=0.9759\n",
            "  Epoch  90: Train Acc=0.9968, CE=0.3262, Div=0.0003, Val Acc=0.9800, F1=0.9794\n",
            "  Epoch 100: Train Acc=0.9973, CE=0.3228, Div=0.0001, Val Acc=0.9797, F1=0.9785\n",
            "  Best Val Acc: 0.9810\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9810\n",
            "  Test Acc: 0.9791, F1: 0.9770\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9302, CE=0.5232, Div=0.0318, Val Acc=0.9254, F1=0.9178\n",
            "  Epoch  20: Train Acc=0.9611, CE=0.4284, Div=0.0069, Val Acc=0.9512, F1=0.9445\n",
            "  Epoch  30: Train Acc=0.9778, CE=0.3839, Div=0.0023, Val Acc=0.9640, F1=0.9593\n",
            "  Epoch  40: Train Acc=0.9867, CE=0.3586, Div=0.0016, Val Acc=0.9706, F1=0.9676\n",
            "  Epoch  50: Train Acc=0.9919, CE=0.3412, Div=0.0016, Val Acc=0.9746, F1=0.9722\n",
            "  Epoch  60: Train Acc=0.9935, CE=0.3306, Div=0.0012, Val Acc=0.9757, F1=0.9742\n",
            "  Epoch  70: Train Acc=0.9951, CE=0.3241, Div=0.0004, Val Acc=0.9763, F1=0.9751\n",
            "  Epoch  80: Train Acc=0.9965, CE=0.3185, Div=0.0003, Val Acc=0.9789, F1=0.9774\n",
            "  Epoch  90: Train Acc=0.9969, CE=0.3152, Div=0.0001, Val Acc=0.9785, F1=0.9777\n",
            "  Epoch 100: Train Acc=0.9970, CE=0.3130, Div=0.0001, Val Acc=0.9795, F1=0.9781\n",
            "  Best Val Acc: 0.9808\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9808\n",
            "  Test Acc: 0.9777, F1: 0.9763\n",
            "\n",
            "[Progress: 4/57]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: Standing_TO_Sitting_30PCT\n",
            "================================================================================\n",
            "\n",
            "Loading Standing_TO_Sitting_30PCT...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets/Standing_TO_Sitting_30PCT\n",
            "  Train: (34192, 100, 27), Test: (8549, 100, 27)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 27353, Val: 6839, Test: 8549\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9258, Loss=0.5586, Val Acc=0.9256, F1=0.9175\n",
            "  Epoch  20: Train Acc=0.9589, Loss=0.4457, Val Acc=0.9510, F1=0.9442\n",
            "  Epoch  30: Train Acc=0.9761, Loss=0.3965, Val Acc=0.9629, F1=0.9576\n",
            "  Epoch  40: Train Acc=0.9855, Loss=0.3676, Val Acc=0.9703, F1=0.9672\n",
            "  Epoch  50: Train Acc=0.9903, Loss=0.3506, Val Acc=0.9744, F1=0.9716\n",
            "  Epoch  60: Train Acc=0.9934, Loss=0.3380, Val Acc=0.9779, F1=0.9758\n",
            "  Epoch  70: Train Acc=0.9953, Loss=0.3297, Val Acc=0.9806, F1=0.9793\n",
            "  Epoch  80: Train Acc=0.9961, Loss=0.3242, Val Acc=0.9810, F1=0.9792\n",
            "  Epoch  90: Train Acc=0.9969, Loss=0.3195, Val Acc=0.9800, F1=0.9782\n",
            "  Epoch 100: Train Acc=0.9971, Loss=0.3161, Val Acc=0.9801, F1=0.9782\n",
            "  Early stopping at epoch 100\n",
            "  Best Val Acc: 0.9810\n",
            "\n",
            "[GAP Results]\n",
            "  Val Acc: 0.9810\n",
            "  Test Acc: 0.9778, F1: 0.9741\n",
            "\n",
            "[Training TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9274, CE=0.5411, Div=0.0372, Val Acc=0.9259, F1=0.9185\n",
            "  Epoch  20: Train Acc=0.9594, CE=0.4427, Div=0.0080, Val Acc=0.9460, F1=0.9404\n",
            "  Epoch  30: Train Acc=0.9746, CE=0.4007, Div=0.0034, Val Acc=0.9618, F1=0.9577\n",
            "  Epoch  40: Train Acc=0.9840, CE=0.3741, Div=0.0022, Val Acc=0.9687, F1=0.9663\n",
            "  Epoch  50: Train Acc=0.9896, CE=0.3565, Div=0.0013, Val Acc=0.9699, F1=0.9669\n",
            "  Epoch  60: Train Acc=0.9928, CE=0.3441, Div=0.0010, Val Acc=0.9760, F1=0.9734\n",
            "  Epoch  70: Train Acc=0.9946, CE=0.3371, Div=0.0010, Val Acc=0.9747, F1=0.9727\n",
            "  Epoch  80: Train Acc=0.9950, CE=0.3320, Div=0.0006, Val Acc=0.9791, F1=0.9770\n",
            "  Epoch  90: Train Acc=0.9962, CE=0.3268, Div=0.0002, Val Acc=0.9792, F1=0.9770\n",
            "  Epoch 100: Train Acc=0.9967, CE=0.3238, Div=0.0002, Val Acc=0.9798, F1=0.9782\n",
            "  Best Val Acc: 0.9807\n",
            "\n",
            "[TPA Results]\n",
            "  Val Acc: 0.9807\n",
            "  Test Acc: 0.9784, F1: 0.9755\n",
            "\n",
            "[Training Gated-TPA]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9294, CE=0.5283, Div=0.0345, Val Acc=0.9244, F1=0.9187\n",
            "  Epoch  20: Train Acc=0.9608, CE=0.4309, Div=0.0080, Val Acc=0.9425, F1=0.9372\n",
            "  Epoch  30: Train Acc=0.9765, CE=0.3891, Div=0.0030, Val Acc=0.9613, F1=0.9571\n",
            "  Epoch  40: Train Acc=0.9863, CE=0.3614, Div=0.0019, Val Acc=0.9718, F1=0.9697\n",
            "  Epoch  50: Train Acc=0.9905, CE=0.3441, Div=0.0011, Val Acc=0.9754, F1=0.9731\n",
            "  Epoch  60: Train Acc=0.9935, CE=0.3319, Div=0.0010, Val Acc=0.9782, F1=0.9767\n",
            "  Epoch  70: Train Acc=0.9951, CE=0.3244, Div=0.0004, Val Acc=0.9791, F1=0.9775\n",
            "  Epoch  80: Train Acc=0.9961, CE=0.3195, Div=0.0002, Val Acc=0.9803, F1=0.9786\n",
            "  Epoch  90: Train Acc=0.9969, CE=0.3151, Div=0.0002, Val Acc=0.9820, F1=0.9805\n",
            "  Epoch 100: Train Acc=0.9972, CE=0.3125, Div=0.0001, Val Acc=0.9820, F1=0.9808\n",
            "  Best Val Acc: 0.9820\n",
            "\n",
            "[Gated-TPA Results]\n",
            "  Val Acc: 0.9820\n",
            "  Test Acc: 0.9792, F1: 0.9759\n",
            "\n",
            "[Progress: 5/57]\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT: Standing_TO_Sitting_40PCT\n",
            "================================================================================\n",
            "\n",
            "Loading Standing_TO_Sitting_40PCT...\n",
            "  Path: /content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets/Standing_TO_Sitting_40PCT\n",
            "  Train: (34192, 100, 27), Test: (8549, 100, 27)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 27353, Val: 6839, Test: 8549\n",
            "\n",
            "[Training GAP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch  10: Train Acc=0.9251, Loss=0.5621, Val Acc=0.9210, F1=0.9117\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Simplified Model Comparison: GAP, TPA, Gated-TPA\n",
        "- TPA Top-k 마스킹 적용\n",
        "- 프로토타입 다양성 페널티\n",
        "- 로짓 수준 MoE 융합 (별도 분류기)\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ========================\n",
        "# Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/TPA2/pamap2_transition_datasets\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/TPA2\"\n",
        "\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    d_model: int = 128\n",
        "\n",
        "    # Transformer hyperparameters\n",
        "    num_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    ff_dim: int = 256\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    # 새로운 하이퍼파라미터\n",
        "    diversity_weight: float = 5e-3  # 프로토타입 다양성 페널티 가중치\n",
        "    use_logit_fusion: bool = True    # 로짓 수준 융합 사용 여부\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ========================\n",
        "# Dataset Class\n",
        "# ========================\n",
        "class PreloadedDataset(Dataset):\n",
        "    \"\"\"Dataset for pre-loaded numpy arrays\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        super().__init__()\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "\n",
        "        # Label 범위 확인 및 조정 (1-6 -> 0-5)\n",
        "        if y.min() >= 1:\n",
        "            y = y - 1\n",
        "\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# ========================\n",
        "# Data Loading Functions\n",
        "# ========================\n",
        "def load_dataset(base_dir: str, dataset_name: str):\n",
        "    \"\"\"\n",
        "    Load pre-augmented dataset\n",
        "    Args:\n",
        "        base_dir: base directory containing all datasets\n",
        "        dataset_name: e.g., \"ORIGINAL\", \"STANDING_TO_SITTING_10pct\", etc.\n",
        "    Returns:\n",
        "        train_dataset, test_dataset\n",
        "    \"\"\"\n",
        "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
        "\n",
        "    print(f\"\\nLoading {dataset_name}...\")\n",
        "    print(f\"  Path: {dataset_dir}\")\n",
        "\n",
        "    # Load data\n",
        "    X_train = np.load(os.path.join(dataset_dir, \"X_train.npy\"))\n",
        "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
        "    X_test = np.load(os.path.join(dataset_dir, \"X_test.npy\"))\n",
        "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
        "\n",
        "    print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "    train_dataset = PreloadedDataset(X_train, y_train)\n",
        "    test_dataset = PreloadedDataset(X_test, y_test)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# ========================\n",
        "# Transformer Backbone Components\n",
        "# ========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal Positional Encoding\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, D]\n",
        "        Returns:\n",
        "            [B, T, D]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight Transformer Encoder Backbone\n",
        "    - 2 layers\n",
        "    - d_model=128\n",
        "    - n_heads=4\n",
        "    - ff_dim=256\n",
        "    - Dropout=0.1\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_seq_len: int = 200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection: [B, C, T] -> [B, T, D]\n",
        "        self.input_projection = nn.Linear(in_channels, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-LN for better stability\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Output normalization\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, C, T] - input sensor data\n",
        "        Returns:\n",
        "            [B, T, D] - transformed sequence\n",
        "        \"\"\"\n",
        "        # [B, C, T] -> [B, T, C]\n",
        "        # x = x.transpose(1, 2)\n",
        "\n",
        "        # Project to d_model: [B, T, C] -> [B, T, D]\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding: [B, T, D]\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Transformer encoding: [B, T, D]\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Final normalization: [B, T, D]\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ========================\n",
        "# GAP Model\n",
        "# ========================\n",
        "class GAPModel(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling with Transformer Backbone\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        pooled = features.mean(dim=1)  # [B, D]\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Improved TPA with Top-k\n",
        "# ========================\n",
        "class ImprovedTPA(nn.Module):\n",
        "    \"\"\"개선된 TPA: Top-k 마스킹 + 다양성 정규화\"\"\"\n",
        "    def __init__(self, dim, num_prototypes=16, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, T, D]\n",
        "        Returns:\n",
        "            z_tpa: [B, D]\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        x_norm = self.pre_norm(x)\n",
        "\n",
        "        K = self.k_proj(x_norm)\n",
        "        V = self.v_proj(x_norm)\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Qh = split_heads(Qp, P)  # [B, H, P, d]\n",
        "        Kh = split_heads(K, T)    # [B, H, T, d]\n",
        "        Vh = split_heads(V, T)    # [B, H, T, d]\n",
        "\n",
        "        # Qh = F.normalize(Qh, dim=-1)\n",
        "        # Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn = F.softmax(scores, dim=-1)  # [B, H, P, T]\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "\n",
        "        # ==================\n",
        "        # Top-k 마스킹\n",
        "        # ==================\n",
        "        k = max(1, int(self.topk_ratio * T))\n",
        "        vals, idx = attn.topk(k, dim=-1)  # [B, H, P, k]\n",
        "        mask = torch.zeros_like(attn).scatter_(-1, idx, 1.0)\n",
        "        attn = attn * mask\n",
        "        # 재정규화\n",
        "        attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        proto_tokens = torch.matmul(attn, Vh)  # [B, H, P, d]\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        z_tpa = proto_tokens.mean(dim=1)  # [B, D]\n",
        "\n",
        "        z = self.fuse(z_tpa)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def compute_diversity_loss(self):\n",
        "        \"\"\"\n",
        "        프로토타입 다양성 페널티\n",
        "        Returns:\n",
        "            diversity_loss: scalar\n",
        "        \"\"\"\n",
        "        proto_norm = F.normalize(self.proto, dim=-1)  # [P, D]\n",
        "        sim = proto_norm @ proto_norm.t()  # [P, P]\n",
        "        # 대각선 제외하고 유사도를 최소화\n",
        "        div_loss = (sim - torch.eye(sim.size(0), device=sim.device)).pow(2).mean()\n",
        "        return div_loss\n",
        "\n",
        "class TPAModel(nn.Module):\n",
        "    \"\"\"개선된 TPA 모델\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12,\n",
        "                 tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.tpa = ImprovedTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "        z = self.tpa(features)  # [B, D]\n",
        "        logits = self.classifier(z)\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Improved Gated-TPA with Logit-level Fusion\n",
        "# ========================\n",
        "class ImprovedGatedTPAModel(nn.Module):\n",
        "    \"\"\"개선된 Gated-TPA: 로짓 수준 융합 + 별도 분류기\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels: int = 27,\n",
        "                 d_model: int = 128,\n",
        "                 num_layers: int = 2,\n",
        "                 n_heads: int = 4,\n",
        "                 ff_dim: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 num_classes: int = 12,\n",
        "                 tpa_config=None,\n",
        "                 use_logit_fusion=True):\n",
        "        super().__init__()\n",
        "        self.use_logit_fusion = use_logit_fusion\n",
        "        self.backbone = TransformerBackbone(\n",
        "            in_channels=in_channels,\n",
        "            d_model=d_model,\n",
        "            num_layers=num_layers,\n",
        "            n_heads=n_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.tpa = ImprovedTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=tpa_config['num_prototypes'],\n",
        "            heads=tpa_config['heads'],\n",
        "            dropout=tpa_config['dropout'],\n",
        "            temperature=tpa_config['temperature'],\n",
        "            topk_ratio=tpa_config['topk_ratio']\n",
        "        )\n",
        "\n",
        "        if use_logit_fusion:\n",
        "            # 로짓 수준 융합 (별도 분류기)\n",
        "            self.cls_gap = nn.Linear(d_model, num_classes)\n",
        "            self.cls_tpa = nn.Linear(d_model, num_classes)\n",
        "\n",
        "            # Gating mechanism\n",
        "            self.gate = nn.Sequential(\n",
        "                nn.Linear(d_model * 2, num_classes),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else:\n",
        "            # 기존 feature 수준 융합\n",
        "            self.gate = nn.Sequential(\n",
        "                nn.Linear(d_model * 2, d_model),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "            self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)  # [B, T, D]\n",
        "\n",
        "        # GAP branch\n",
        "        z_gap = features.mean(dim=1)  # [B, D]\n",
        "\n",
        "        # TPA branch\n",
        "        z_tpa = self.tpa(features)  # [B, D]\n",
        "\n",
        "        if self.use_logit_fusion:\n",
        "            # 로짓 수준 융합\n",
        "            logits_gap = self.cls_gap(z_gap)  # [B, C]\n",
        "            logits_tpa = self.cls_tpa(z_tpa)  # [B, C]\n",
        "\n",
        "            # 게이팅 (클래스별 게이트)\n",
        "            gate_input = torch.cat([z_gap, z_tpa], dim=-1)\n",
        "            g = self.gate(gate_input)  # [B, C]\n",
        "\n",
        "            # 가중 융합\n",
        "            logits = g * logits_gap + (1 - g) * logits_tpa\n",
        "        else:\n",
        "            # 기존 feature 수준 융합\n",
        "            gate_input = torch.cat([z_gap, z_tpa], dim=-1)\n",
        "            g = self.gate(gate_input)  # [B, D]\n",
        "            z = g * z_gap + (1 - g) * z_tpa\n",
        "            logits = self.classifier(z)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# Training & Evaluation\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config, compute_diversity=True):\n",
        "    \"\"\"\n",
        "    개선: 다양성 페널티 추가\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total, correct, loss_sum, ce_loss_sum, div_loss_sum = 0, 0, 0.0, 0.0, 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device).float(), y.to(cfg.device)\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "\n",
        "        # Cross-entropy loss\n",
        "        ce_loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "\n",
        "        # Diversity loss (TPA 모델만)\n",
        "        div_loss = torch.tensor(0.0, device=cfg.device)\n",
        "        if compute_diversity and hasattr(model, 'tpa'):\n",
        "            div_loss = model.tpa.compute_diversity_loss()\n",
        "\n",
        "        # Total loss\n",
        "        loss = ce_loss + cfg.diversity_weight * div_loss\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            ce_loss_sum += ce_loss.item() * y.size(0)\n",
        "            div_loss_sum += div_loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"ce_loss\": ce_loss_sum / total if total > 0 else 0,\n",
        "        \"div_loss\": div_loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "# ========================\n",
        "# Model Complexity Analysis\n",
        "# ========================\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count total and trainable parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total_params, trainable_params\n",
        "\n",
        "def estimate_flops(model, input_shape=(1, 100, 27), device='cuda'):\n",
        "    \"\"\"\n",
        "    Estimate FLOPs using manual calculation\n",
        "    For Conv1d: FLOPs = 2 * C_in * C_out * K * L_out\n",
        "    For Linear: FLOPs = 2 * in_features * out_features\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_flops = 0\n",
        "\n",
        "    def conv1d_flops(module, input, output):\n",
        "        batch_size, out_channels, out_length = output.shape\n",
        "        kernel_size = module.kernel_size[0]\n",
        "        in_channels = module.in_channels\n",
        "        groups = module.groups\n",
        "\n",
        "        flops_per_element = 2 * (in_channels // groups) * kernel_size\n",
        "        total = flops_per_element * out_channels * out_length * batch_size\n",
        "\n",
        "        nonlocal total_flops\n",
        "        total_flops += total\n",
        "\n",
        "    def linear_flops(module, input, output):\n",
        "        batch_size = input[0].shape[0]\n",
        "        in_features = module.in_features\n",
        "        out_features = module.out_features\n",
        "\n",
        "        total = 2 * in_features * out_features * batch_size\n",
        "\n",
        "        nonlocal total_flops\n",
        "        total_flops += total\n",
        "\n",
        "    # Register hooks\n",
        "    hooks = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            hooks.append(module.register_forward_hook(conv1d_flops))\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            hooks.append(module.register_forward_hook(linear_flops))\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        x = torch.randn(input_shape).to(device)\n",
        "        model(x)\n",
        "\n",
        "    # Remove hooks\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "def measure_inference_time(model, input_shape=(1, 100, 27), device='cuda', n_runs=100):\n",
        "    \"\"\"\n",
        "    Measure average inference time over multiple runs\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = torch.randn(input_shape).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(x)\n",
        "\n",
        "    # Synchronize GPU\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Measure\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_runs):\n",
        "            _ = model(x)\n",
        "            if device == 'cuda':\n",
        "                torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    avg_time = (end - start) / n_runs * 1000  # Convert to ms\n",
        "    return avg_time\n",
        "\n",
        "def analyze_model_complexity(model, model_name, cfg: Config, input_shape=(1, 100, 27)):\n",
        "    \"\"\"\n",
        "    Complete model complexity analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL COMPLEXITY ANALYSIS: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Parameters\n",
        "    total_params, trainable_params = count_parameters(model)\n",
        "    print(f\"Total Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "    print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
        "\n",
        "    # FLOPs\n",
        "    flops = estimate_flops(model, input_shape, cfg.device)\n",
        "    print(f\"FLOPs: {flops:,} ({flops / 1e6:.2f} MFLOPs)\")\n",
        "\n",
        "    # Inference time\n",
        "    inference_time = measure_inference_time(model, input_shape, cfg.device, n_runs=100)\n",
        "    print(f\"Inference Time: {inference_time:.3f} ms (avg over 100 runs)\")\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'total_params': total_params,\n",
        "        'trainable_params': trainable_params,\n",
        "        'model_size_mb': total_params * 4 / 1024 / 1024,\n",
        "        'flops': flops,\n",
        "        'mflops': flops / 1e6,\n",
        "        'inference_time_ms': inference_time\n",
        "    }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, cfg: Config, model_name: str):\n",
        "    \"\"\"Train a single model\"\"\"\n",
        "    print(f\"\\n[Training {model_name}]\")\n",
        "\n",
        "    # TPA 모델만 diversity loss 계산\n",
        "    compute_diversity = 'TPA' in model_name\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    best_acc, best_wts = 0.0, None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, cfg.epochs + 1):\n",
        "        stats = train_one_epoch(model, train_loader, opt, cfg, compute_diversity)\n",
        "        val_acc, val_f1 = evaluate(model, val_loader, cfg)\n",
        "\n",
        "        if val_acc > best_acc + cfg.min_delta:\n",
        "            best_acc = val_acc\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            if compute_diversity:\n",
        "                print(f\"  Epoch {epoch:3d}: Train Acc={stats['acc']:.4f}, \"\n",
        "                      f\"CE={stats['ce_loss']:.4f}, Div={stats['div_loss']:.4f}, \"\n",
        "                      f\"Val Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
        "            else:\n",
        "                print(f\"  Epoch {epoch:3d}: Train Acc={stats['acc']:.4f}, \"\n",
        "                      f\"Loss={stats['loss']:.4f}, Val Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
        "\n",
        "        if patience_counter >= cfg.patience:\n",
        "            print(f\"  Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_wts:\n",
        "        model.load_state_dict(best_wts)\n",
        "\n",
        "    print(f\"  Best Val Acc: {best_acc:.4f}\")\n",
        "    return best_acc\n",
        "\n",
        "def create_model(model_name: str, cfg: Config):\n",
        "    \"\"\"Create model by name\"\"\"\n",
        "    tpa_config = {\n",
        "        'num_prototypes': cfg.tpa_num_prototypes,\n",
        "        'heads': cfg.tpa_heads,\n",
        "        'dropout': cfg.tpa_dropout,\n",
        "        'temperature': cfg.tpa_temperature,\n",
        "        'topk_ratio': cfg.tpa_topk_ratio\n",
        "    }\n",
        "\n",
        "    if model_name == \"GAP\":\n",
        "        return GAPModel(d_model=cfg.d_model).to(cfg.device).float()\n",
        "    elif model_name == \"TPA\":\n",
        "        return TPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "    elif model_name == \"Gated-TPA\":\n",
        "        return ImprovedGatedTPAModel(\n",
        "            d_model=cfg.d_model,\n",
        "            tpa_config=tpa_config,\n",
        "            use_logit_fusion=cfg.use_logit_fusion\n",
        "        ).to(cfg.device).float()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "# ========================\n",
        "# Main Experiment\n",
        "# ========================\n",
        "def run_experiment(dataset_name: str, cfg: Config):\n",
        "    \"\"\"Run complete experiment for one dataset\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EXPERIMENT: {dataset_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load data\n",
        "    train_dataset, test_dataset = load_dataset(cfg.data_dir, dataset_name)\n",
        "\n",
        "    # Split train into train/val using indices\n",
        "    n_total = len(train_dataset)\n",
        "    indices = np.arange(n_total)\n",
        "\n",
        "    # Get labels for stratification\n",
        "    y_labels = train_dataset.y.numpy()\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=y_labels\n",
        "    )\n",
        "\n",
        "    # Create subsets using Subset\n",
        "    from torch.utils.data import Subset\n",
        "    train_subset = Subset(train_dataset, train_indices)\n",
        "    val_subset = Subset(train_dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_subset, cfg.batch_size, shuffle=True,\n",
        "                              num_workers=cfg.num_workers, generator=g)\n",
        "    val_loader = DataLoader(val_subset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    print(f\"\\nDataset splits:\")\n",
        "    print(f\"  Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Train and evaluate all models\n",
        "    results = []\n",
        "    complexity_results = []\n",
        "    model_names = [\"GAP\", \"TPA\", \"Gated-TPA\"]\n",
        "\n",
        "    # First, analyze model complexity (only once, use first dataset)\n",
        "    if dataset_name == \"ORIGINAL\":\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"MODEL COMPLEXITY COMPARISON\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        for model_name in model_names:\n",
        "            random.seed(SEED)\n",
        "            np.random.seed(SEED)\n",
        "            torch.manual_seed(SEED)\n",
        "\n",
        "            model = create_model(model_name, cfg)\n",
        "            complexity = analyze_model_complexity(model, model_name, cfg)\n",
        "            complexity_results.append(complexity)\n",
        "\n",
        "    for model_name in model_names:\n",
        "        # Reset seed for each model\n",
        "        random.seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "\n",
        "        # Create and train model\n",
        "        model = create_model(model_name, cfg)\n",
        "        best_val_acc = train_model(model, train_loader, val_loader, cfg, model_name)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_acc, test_f1 = evaluate(model, test_loader, cfg)\n",
        "\n",
        "        print(f\"\\n[{model_name} Results]\")\n",
        "        print(f\"  Val Acc: {best_val_acc:.4f}\")\n",
        "        print(f\"  Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Dataset': dataset_name,\n",
        "            'Val_Accuracy': float(best_val_acc),\n",
        "            'Test_Accuracy': float(test_acc),\n",
        "            'Test_F1_Score': float(test_f1)\n",
        "        })\n",
        "\n",
        "    return results, complexity_results\n",
        "\n",
        "# ========================\n",
        "# Run All Experiments\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SIMPLIFIED MODEL COMPARISON: GAP vs TPA vs Gated-TPA\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n개선사항:\")\n",
        "    print(\"  1. TPA Top-k 마스킹 적용\")\n",
        "    print(\"  2. 프로토타입 다양성 페널티\")\n",
        "    print(\"  3. 로짓 수준 MoE 융합 (별도 분류기)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    datasets = [\"ORIGINAL\"]\n",
        "\n",
        "    transitions = [\n",
        "        'Standing_TO_Sitting',\n",
        "        'Sitting_TO_Standing',\n",
        "        'Sitting_TO_Lying',\n",
        "        'Lying_TO_Sitting',\n",
        "        'Standing_TO_Lying',\n",
        "        'Lying_TO_Standing',\n",
        "        'Standing_TO_Walking',\n",
        "        'Walking_TO_Standing',\n",
        "        'Walking_TO_Running',\n",
        "        'Running_TO_Walking',\n",
        "        'Walking_TO_Ascending_stairs',\n",
        "        'Walking_TO_Descending_stairs',\n",
        "        'Ascending_stairs_TO_Walking',\n",
        "        'Descending_stairs_TO_Walking'\n",
        "    ]\n",
        "\n",
        "    # 모든 전이에 대해 10%, 20%, 30%, 40% 추가\n",
        "    mix_pcts = [10, 20, 30, 40]\n",
        "\n",
        "    for transition in transitions:\n",
        "        for pct in mix_pcts:\n",
        "            datasets.append(f\"{transition}_{pct}PCT\")\n",
        "\n",
        "    print(f\"\\nTotal datasets to test: {len(datasets)}\")\n",
        "    print(f\"  - transitions: {len(transitions) * len(mix_pcts) + 2}\")\n",
        "\n",
        "    all_results = []\n",
        "    all_complexity = []\n",
        "\n",
        "    # Run experiments\n",
        "    for i, dataset_name in enumerate(datasets, 1):\n",
        "        print(f\"\\n[Progress: {i}/{len(datasets)}]\")\n",
        "\n",
        "        results, complexity = run_experiment(dataset_name, cfg)\n",
        "        all_results.extend(results)\n",
        "        if complexity:  # 첫 번째 데이터셋에서만 반환됨\n",
        "            all_complexity = complexity\n",
        "\n",
        "    # Save all results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SAVING RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    results_dict = {\n",
        "        'experiment_info': {\n",
        "            'date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'version': 'simplified_v1',\n",
        "            'improvements': [\n",
        "                'TPA Top-k masking',\n",
        "                'Prototype diversity penalty',\n",
        "                'Logit-level MoE fusion'\n",
        "            ],\n",
        "            'models': ['GAP', 'TPA', 'Gated-TPA'],\n",
        "            'total_datasets': len(datasets),\n",
        "            'datasets': datasets,\n",
        "            'config': {\n",
        "                'epochs': cfg.epochs,\n",
        "                'batch_size': cfg.batch_size,\n",
        "                'lr': cfg.lr,\n",
        "                'd_model': cfg.d_model,\n",
        "                'tpa_num_prototypes': cfg.tpa_num_prototypes,\n",
        "                'tpa_heads': cfg.tpa_heads,\n",
        "                'tpa_temperature': cfg.tpa_temperature,\n",
        "                'tpa_topk_ratio': cfg.tpa_topk_ratio,\n",
        "                'diversity_weight': cfg.diversity_weight,\n",
        "                'use_logit_fusion': cfg.use_logit_fusion\n",
        "            }\n",
        "        },\n",
        "        'model_complexity': all_complexity,\n",
        "        'results': all_results\n",
        "    }\n",
        "\n",
        "    # Save to JSON\n",
        "    json_path = os.path.join(cfg.save_dir, \"pamap2_tpa_transition_cnn_simplified.json\")\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_dict, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved to: {json_path}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total experiments: {len(all_results)}\")\n",
        "    print(f\"Total datasets tested: {len(datasets)}\")\n",
        "    print(f\"Models compared: 3 (GAP, TPA, Gated-TPA)\")\n",
        "\n",
        "    # Calculate average performance per model\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"AVERAGE PERFORMANCE (All Datasets)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for model_name in ['GAP', 'TPA', 'Gated-TPA']:\n",
        "        model_results = [r for r in all_results if r['Model'] == model_name]\n",
        "        avg_acc = np.mean([r['Test_Accuracy'] for r in model_results])\n",
        "        avg_f1 = np.mean([r['Test_F1_Score'] for r in model_results])\n",
        "        print(f\"{model_name:12s}: Acc={avg_acc:.4f}, F1={avg_f1:.4f}\")\n",
        "\n",
        "    # Print model complexity table\n",
        "    if all_complexity:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"MODEL COMPLEXITY COMPARISON\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"{'Model':<12} {'Params':<12} {'Size(MB)':<10} {'MFLOPs':<10} {'Time(ms)':<10}\")\n",
        "        print(\"-\" * 80)\n",
        "        for comp in all_complexity:\n",
        "            print(f\"{comp['model']:<12} {comp['total_params']:<12,} \"\n",
        "                  f\"{comp['model_size_mb']:<10.2f} {comp['mflops']:<10.2f} \"\n",
        "                  f\"{comp['inference_time_ms']:<10.3f}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n"
      ]
    }
  ]
}