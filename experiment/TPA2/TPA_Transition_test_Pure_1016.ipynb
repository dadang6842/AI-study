{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt7vLSNgh3ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e841d5-51ef-41df-ea00-84e129c109fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "UCI-HAR Ablation Study: GAP vs Pure TPA (No Conv layers)\n",
        "\"\"\"\n",
        "import os, random, math, sys, time, copy, json\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# ========================\n",
        "# 0) Config & Reproducibility\n",
        "# ========================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str = \"ablation\"\n",
        "    data_dir: str = \"/content/drive/MyDrive/AI_data/UCI_HAR_Dataset/UCI HAR Dataset\"\n",
        "    save_dir: str = \"/content/drive/MyDrive/AI_data/ablation_pure_tpa\"\n",
        "\n",
        "    epochs: int = 25\n",
        "    batch_size: int = 128\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: float = 1.0\n",
        "    label_smoothing: float = 0.05\n",
        "\n",
        "    # Early stopping\n",
        "    patience: int = 20\n",
        "    min_delta: float = 0.0001\n",
        "    val_split: float = 0.2\n",
        "\n",
        "    train_augment_prob: float = 0.25\n",
        "    train_augment_mix: float = 0.35\n",
        "\n",
        "    d_model: int = 128\n",
        "    use_tpa: bool = False\n",
        "\n",
        "    # TPA hyperparameters\n",
        "    tpa_num_prototypes: int = 16\n",
        "    tpa_heads: int = 4\n",
        "    tpa_dropout: float = 0.1\n",
        "    tpa_temperature: float = 0.07\n",
        "    tpa_topk_ratio: float = 0.25\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    num_workers: int = 2\n",
        "\n",
        "# ========================\n",
        "# 1) UCI-HAR Data Loader\n",
        "# ========================\n",
        "_RAW_CHANNELS = [\n",
        "    (\"Inertial Signals/total_acc_x_\", \"txt\"), (\"Inertial Signals/total_acc_y_\", \"txt\"), (\"Inertial Signals/total_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_acc_x_\", \"txt\"), (\"Inertial Signals/body_acc_y_\", \"txt\"), (\"Inertial Signals/body_acc_z_\", \"txt\"),\n",
        "    (\"Inertial Signals/body_gyro_x_\", \"txt\"), (\"Inertial Signals/body_gyro_y_\", \"txt\"), (\"Inertial Signals/body_gyro_z_\", \"txt\"),\n",
        "]\n",
        "_LABEL_MAP = {1:\"WALKING\", 2:\"WALKING_UPSTAIRS\", 3:\"WALKING_DOWNSTAIRS\", 4:\"SITTING\", 5:\"STANDING\", 6:\"LAYING\"}\n",
        "_CODE_TO_LABEL_NAME = {i-1: _LABEL_MAP[i] for i in _LABEL_MAP}\n",
        "_LABEL_NAME_TO_CODE = {v: k for k, v in _CODE_TO_LABEL_NAME.items()}\n",
        "\n",
        "def _load_split_raw(root: str, split: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    assert split in (\"train\", \"test\")\n",
        "    X_list = [np.loadtxt(os.path.join(root, split, p + split + \".\" + e))[..., None] for p, e in _RAW_CHANNELS]\n",
        "    X = np.concatenate(X_list, axis=-1).transpose(0, 2, 1)\n",
        "    y = np.loadtxt(os.path.join(root, split, f\"y_{split}.txt\")).astype(int)\n",
        "    return X, y\n",
        "\n",
        "class UCIHARInertial(Dataset):\n",
        "    def __init__(self, root: str, split: str, mean=None, std=None,\n",
        "                 preloaded_data: Tuple[np.ndarray, np.ndarray] | None = None,\n",
        "                 indices: np.ndarray | None = None):\n",
        "        super().__init__()\n",
        "\n",
        "        if preloaded_data is not None:\n",
        "            X, y = preloaded_data\n",
        "        else:\n",
        "            X, y = _load_split_raw(root, split)\n",
        "\n",
        "        if indices is not None:\n",
        "            X = X[indices]\n",
        "            y = y[indices]\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64) if y.min() >= 1 else y.astype(np.int64)\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean, self.std = mean, std\n",
        "            if preloaded_data is None:\n",
        "                self.X = (self.X - self.mean) / self.std\n",
        "        else:\n",
        "            self.mean = self.X.mean(axis=(0,2), keepdims=True).astype(np.float32)\n",
        "            self.std = (self.X.std(axis=(0,2), keepdims=True) + 1e-6).astype(np.float32)\n",
        "            self.X = ((self.X - self.mean) / self.std).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# ========================\n",
        "# 2) Online Transition Augmentation\n",
        "# ========================\n",
        "def apply_transition_augmentation(x: torch.Tensor, y: torch.Tensor, mix_ratio: float = 0.25) -> torch.Tensor:\n",
        "    B, C, T = x.shape\n",
        "\n",
        "    mix_pts = int(T * mix_ratio)\n",
        "\n",
        "    for i in range(B):\n",
        "        if random.random() < 0.5:\n",
        "            other_class_indices = (y != y[i]).nonzero(as_tuple=True)[0]\n",
        "            if len(other_class_indices) > 0:\n",
        "                j = other_class_indices[random.randint(0, len(other_class_indices)-1)]\n",
        "                x[i, :, -mix_pts:] = x[j, :, :mix_pts].clone()\n",
        "\n",
        "    return x\n",
        "\n",
        "# ========================\n",
        "# 3) Pure TPA Module (No Conv layers)\n",
        "# ========================\n",
        "class PureTPA(nn.Module):\n",
        "    \"\"\"Pure Temporal Prototype Attention - No convolutional preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_prototypes=16, heads=4, dropout=0.1,\n",
        "                 temperature=0.07, topk_ratio=0.25):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.head_dim = dim // heads\n",
        "        self.num_prototypes = num_prototypes\n",
        "        self.temperature = temperature\n",
        "        self.topk_ratio = topk_ratio\n",
        "\n",
        "        # Learnable prototypes\n",
        "        self.proto = nn.Parameter(torch.randn(num_prototypes, dim) * 0.02)\n",
        "\n",
        "        # Simple normalization (replace conv-based preprocessing)\n",
        "        self.pre_norm = nn.LayerNorm(dim)\n",
        "\n",
        "        # Q, K, V projections for multi-head attention\n",
        "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.out_proj = nn.Linear(dim, dim, bias=False)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "        # Confidence predictor\n",
        "        self.conf_head = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim // 4, 1)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, return_confidence=False):\n",
        "        \"\"\"\n",
        "        x: [B, T, D]\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        P = self.num_prototypes\n",
        "\n",
        "        # Simple normalization instead of conv preprocessing\n",
        "        x_norm = self.pre_norm(x)\n",
        "\n",
        "        # Generate K, V from normalized input\n",
        "        K = self.k_proj(x_norm)\n",
        "        V = self.v_proj(x_norm)\n",
        "\n",
        "        # Generate Q from prototypes\n",
        "        Qp = self.q_proj(self.proto).unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        def split_heads(t, length):\n",
        "            return t.view(B, length, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Multi-head attention: [B, H, P/T, D/H]\n",
        "        Qh = split_heads(Qp, P)\n",
        "        Kh = split_heads(K, T)\n",
        "        Vh = split_heads(V, T)\n",
        "\n",
        "        # Normalize for stable attention\n",
        "        Qh = F.normalize(Qh, dim=-1)\n",
        "        Kh = F.normalize(Kh, dim=-1)\n",
        "\n",
        "        # Attention scores: [B, H, P, T]\n",
        "        scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        # Attention weights\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn, nan=0.0)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Aggregate temporal features to prototypes: [B, H, P, D/H]\n",
        "        proto_tokens = torch.matmul(attn, Vh)\n",
        "        proto_tokens = proto_tokens.transpose(1, 2).contiguous().view(B, P, D)\n",
        "\n",
        "        # Select top-k prototypes\n",
        "        topk = max(1, int(P * self.topk_ratio))\n",
        "        vals, _ = torch.topk(proto_tokens, k=topk, dim=1)\n",
        "        z_tpa = vals.mean(dim=1)  # [B, D]\n",
        "\n",
        "        # Refine with fusion layer\n",
        "        z_tpa = self.fuse(z_tpa)\n",
        "        z_tpa = self.out_proj(z_tpa)\n",
        "\n",
        "        # Compute GAP for fallback\n",
        "        z_gap = x.mean(dim=1)\n",
        "\n",
        "        # Confidence-weighted combination\n",
        "        confidence = torch.sigmoid(self.conf_head(z_tpa))\n",
        "        z = confidence * z_tpa + (1 - confidence) * z_gap\n",
        "\n",
        "        if return_confidence:\n",
        "            return z, confidence\n",
        "        return z\n",
        "\n",
        "# ========================\n",
        "# 4) Model Definitions\n",
        "# ========================\n",
        "class ConvBNAct(nn.Module):\n",
        "    def __init__(self, c_in, c_out, k, s=1, p=None, g=1):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv1d(c_in, c_out, k, s, k//2 if p is None else p, groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm1d(c_out)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.c(x)))\n",
        "\n",
        "class MultiPathCNN(nn.Module):\n",
        "    def __init__(self, in_ch=9, d_model=128, branches=(3,5,9,15), stride=2):\n",
        "        super().__init__()\n",
        "        h = d_model // 2\n",
        "        self.pre = ConvBNAct(in_ch, h, 1)\n",
        "        self.branches = nn.ModuleList([nn.Sequential(ConvBNAct(h, h, k, stride, g=h), ConvBNAct(h, h, 1)) for k in branches])\n",
        "        self.post = ConvBNAct(len(branches)*h, d_model, 1)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post(torch.cat([b(self.pre(x)) for b in self.branches], dim=1))\n",
        "\n",
        "class SimpleGAPHead(nn.Module):\n",
        "    \"\"\"Baseline: Global Average Pooling\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        # [B, D, T] -> [B, T, D]\n",
        "        features = Fmap.transpose(1, 2)\n",
        "        pooled = features.mean(dim=1)\n",
        "        logits = self.fc(pooled)\n",
        "        aux = {\"confidence\": None}\n",
        "        return logits, aux\n",
        "\n",
        "class PureTPAHead(nn.Module):\n",
        "    \"\"\"Pure TPA: Temporal Prototype Attention (No Conv preprocessing)\"\"\"\n",
        "    def __init__(self, d_model: int, num_classes: int,\n",
        "                 num_prototypes: int = 16, heads: int = 4, dropout: float = 0.1,\n",
        "                 temperature: float = 0.07, topk_ratio: float = 0.25):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tpa = PureTPA(\n",
        "            dim=d_model,\n",
        "            num_prototypes=num_prototypes,\n",
        "            heads=heads,\n",
        "            dropout=dropout,\n",
        "            temperature=temperature,\n",
        "            topk_ratio=topk_ratio\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, Fmap):\n",
        "        \"\"\"\n",
        "        Fmap: [B, D, T]\n",
        "        \"\"\"\n",
        "        features = Fmap.transpose(1, 2)  # [B, T, D]\n",
        "        z, confidence = self.tpa(features, return_confidence=True)\n",
        "        logits = self.classifier(z)\n",
        "        aux = {\"confidence\": confidence.mean().item()}\n",
        "        return logits, aux\n",
        "\n",
        "class HAR_Model(nn.Module):\n",
        "    def __init__(self, d_model=128, num_classes=6, use_tpa=False, tpa_config=None):\n",
        "        super().__init__()\n",
        "        self.backbone = MultiPathCNN(d_model=d_model)\n",
        "        self.use_tpa = use_tpa\n",
        "\n",
        "        if use_tpa:\n",
        "            self.head = PureTPAHead(\n",
        "                d_model=d_model,\n",
        "                num_classes=num_classes,\n",
        "                num_prototypes=tpa_config.get('num_prototypes', 16),\n",
        "                heads=tpa_config.get('heads', 4),\n",
        "                dropout=tpa_config.get('dropout', 0.1),\n",
        "                temperature=tpa_config.get('temperature', 0.07),\n",
        "                topk_ratio=tpa_config.get('topk_ratio', 0.25)\n",
        "            )\n",
        "        else:\n",
        "            self.head = SimpleGAPHead(d_model=d_model, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = self.backbone(x)\n",
        "        return self.head(fmap)\n",
        "\n",
        "# ========================\n",
        "# 5) Train / Eval\n",
        "# ========================\n",
        "def train_one_epoch(model, loader, opt, cfg: Config, verbose_epoch: bool = False):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    aug_count = 0\n",
        "    confidence_vals = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device).float(), y.to(cfg.device)\n",
        "\n",
        "        if random.random() < cfg.train_augment_prob:\n",
        "            x = apply_transition_augmentation(x, y, cfg.train_augment_mix)\n",
        "            aug_count += 1\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits, aux = model(x)\n",
        "\n",
        "        cls_loss = F.cross_entropy(logits, y, label_smoothing=cfg.label_smoothing)\n",
        "        loss = cls_loss\n",
        "        if torch.isnan(loss):\n",
        "            if verbose_epoch:\n",
        "                print(\"  Warning: NaN loss detected, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            loss_sum += loss.item() * y.size(0)\n",
        "            if aux[\"confidence\"] is not None:\n",
        "                confidence_vals.append(aux[\"confidence\"])\n",
        "\n",
        "    stats = {\n",
        "        \"loss\": loss_sum / total if total > 0 else 0,\n",
        "        \"acc\": correct / total if total > 0 else 0,\n",
        "        \"aug_count\": aug_count,\n",
        "        \"avg_confidence\": np.mean(confidence_vals) if confidence_vals else None\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, cfg: Config, classes=6):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits, _ = model(x)\n",
        "        ps.append(logits.argmax(dim=-1).cpu().numpy())\n",
        "        ys.append(y.cpu().numpy())\n",
        "\n",
        "    y_true, y_pred = np.concatenate(ys), np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(classes)))\n",
        "    report = classification_report(y_true, y_pred, target_names=[_CODE_TO_LABEL_NAME[i] for i in range(classes)], digits=4)\n",
        "    return acc, f1, cm, report\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_simple(model, loader, cfg: Config):\n",
        "    \"\"\"Simple evaluation for accuracy only\"\"\"\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(cfg.device), y.to(cfg.device)\n",
        "        logits, _ = model(x)\n",
        "        pred = logits.argmax(dim=-1)\n",
        "\n",
        "        ys.append(y.cpu().numpy())\n",
        "        ps.append(pred.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_pred = np.concatenate(ps)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "# ========================\n",
        "# 6) Extreme Transition Test Set\n",
        "# ========================\n",
        "def create_transitional_test_set(orig_dataset: UCIHARInertial, class_A: str, class_B: str,\n",
        "                                 p: float=0.05, mix: float=0.25, profile: str='abrupt',\n",
        "                                 pos: str='tail', segments: int=1) -> Tuple[UCIHARInertial, dict]:\n",
        "    \"\"\"Create transitional test set\"\"\"\n",
        "    X, y = orig_dataset.X.copy(), orig_dataset.y.copy()\n",
        "    N, C, T = X.shape\n",
        "\n",
        "    code_A, code_B = _LABEL_NAME_TO_CODE[class_A], _LABEL_NAME_TO_CODE[class_B]\n",
        "    idx_A, idx_B = np.where(y == code_A)[0], np.where(y == code_B)[0]\n",
        "    mix_pts = int(T * mix)\n",
        "\n",
        "    modified_indices = []\n",
        "\n",
        "    if segments > 1:\n",
        "        seg_length = mix_pts // segments\n",
        "        remaining = mix_pts % segments\n",
        "    else:\n",
        "        seg_length = mix_pts\n",
        "        remaining = 0\n",
        "\n",
        "    def get_transition_positions(T, mix_pts, pos, segments):\n",
        "        positions = []\n",
        "\n",
        "        if segments == 1:\n",
        "            if pos == 'tail':\n",
        "                positions = [T - mix_pts]\n",
        "            elif pos == 'middle':\n",
        "                positions = [(T - mix_pts) // 2]\n",
        "            elif pos == 'random':\n",
        "                positions = [random.randint(0, max(0, T - mix_pts))]\n",
        "        else:\n",
        "            seg_len = mix_pts // segments\n",
        "            if pos == 'tail':\n",
        "                start = T - mix_pts\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * seg_len)\n",
        "            elif pos == 'middle':\n",
        "                center = T // 2\n",
        "                total_span = mix_pts + (segments - 1) * seg_len\n",
        "                start = center - total_span // 2\n",
        "                for i in range(segments):\n",
        "                    positions.append(start + i * (seg_len * 2))\n",
        "            elif pos == 'random':\n",
        "                available_positions = list(range(0, T - seg_len))\n",
        "                random.shuffle(available_positions)\n",
        "                positions = sorted(available_positions[:segments])\n",
        "\n",
        "        return positions\n",
        "\n",
        "    def apply_transition(target_data, source_data, start_pos, length, profile):\n",
        "        end_pos = start_pos + length\n",
        "\n",
        "        if profile == 'abrupt':\n",
        "            target_data[:, start_pos:end_pos] = source_data[:, start_pos:end_pos].copy()\n",
        "        elif profile == 'fade':\n",
        "            alpha = np.linspace(0, 1, length).reshape(1, -1)\n",
        "            target_segment = target_data[:, start_pos:end_pos]\n",
        "            source_segment = source_data[:, start_pos:end_pos]\n",
        "            target_data[:, start_pos:end_pos] = (\n",
        "                target_segment * (1 - alpha) + source_segment * alpha\n",
        "            )\n",
        "\n",
        "    # Apply transitions for class A\n",
        "    n_targets_A = max(1, int(len(idx_A) * p))\n",
        "    targets_A = np.random.choice(idx_A, n_targets_A, replace=False)\n",
        "    sources_B = np.random.choice(idx_B, len(targets_A), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_A, sources_B):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    # Apply transitions for class B\n",
        "    n_targets_B = max(1, int(len(idx_B) * p))\n",
        "    targets_B = np.random.choice(idx_B, n_targets_B, replace=False)\n",
        "    sources_A = np.random.choice(idx_A, len(targets_B), replace=True)\n",
        "\n",
        "    for t, s in zip(targets_B, sources_A):\n",
        "        positions = get_transition_positions(T, mix_pts, pos, segments)\n",
        "\n",
        "        for i, start in enumerate(positions):\n",
        "            curr_len = seg_length + (remaining if i == len(positions) - 1 else 0)\n",
        "\n",
        "            if start + curr_len > T:\n",
        "                curr_len = T - start\n",
        "\n",
        "            if curr_len > 0:\n",
        "                apply_transition(X[t], orig_dataset.X[s], start, curr_len, profile)\n",
        "\n",
        "        modified_indices.append(t)\n",
        "\n",
        "    if p > 0.5:\n",
        "        mid_start = T // 3\n",
        "        mid_end = 2 * T // 3\n",
        "        mid_length = mid_end - mid_start\n",
        "\n",
        "        extra_A = np.random.choice(idx_A, max(1, int(len(idx_A) * p * 0.3)), replace=False)\n",
        "        extra_B_src = np.random.choice(idx_B, len(extra_A), replace=True)\n",
        "\n",
        "        for t, s in zip(extra_A, extra_B_src):\n",
        "            if t not in modified_indices:\n",
        "                apply_transition(X[t], orig_dataset.X[s], mid_start, mid_length, profile)\n",
        "                modified_indices.append(t)\n",
        "\n",
        "    mod_dataset = UCIHARInertial(\n",
        "        root=\"\", split=\"test\",\n",
        "        mean=orig_dataset.mean, std=orig_dataset.std,\n",
        "        preloaded_data=(X, y)\n",
        "    )\n",
        "\n",
        "    info = {\n",
        "        'total_samples': N,\n",
        "        'modified_samples': len(modified_indices),\n",
        "        'modified_ratio': len(modified_indices) / N,\n",
        "        'mix_frames': mix_pts,\n",
        "        'primary_class_ratio': 1 - mix,\n",
        "        'class_A_modified': len(targets_A),\n",
        "        'class_B_modified': len(targets_B),\n",
        "        'profile': profile,\n",
        "        'position': pos,\n",
        "        'segments': segments\n",
        "    }\n",
        "\n",
        "    return mod_dataset, info\n",
        "\n",
        "def get_transition_scenarios():\n",
        "    \"\"\"Return all transitional test scenarios\"\"\"\n",
        "    scenarios_core = [\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"fade\",\"random\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.70,0.55,\"fade\",\"random\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_stress = [\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_UPSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"SITTING\",\"LAYING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "        (\"STANDING\",\"SITTING\",0.75,0.58,\"abrupt\",\"tail\",1),\n",
        "    ]\n",
        "\n",
        "    scenarios_ctrl = [\n",
        "        (\"WALKING\",\"WALKING_DOWNSTAIRS\",0.70,0.55,\"abrupt\",\"middle\",2),\n",
        "        (\"SITTING\",\"LAYING\",0.70,0.55,\"fade\",\"middle\",2),\n",
        "    ]\n",
        "\n",
        "    return scenarios_core + scenarios_stress + scenarios_ctrl\n",
        "\n",
        "# ========================\n",
        "# 7) GAP vs Pure TPA Comparison\n",
        "# ========================\n",
        "def run_ablation_study(cfg: Config):\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    # Load full train set\n",
        "    train_set_full = UCIHARInertial(cfg.data_dir, \"train\")\n",
        "\n",
        "    # Split train into train + validation (stratified by class)\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   DATA PREPARATION\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    n_samples = len(train_set_full)\n",
        "    indices = np.arange(n_samples)\n",
        "    labels = train_set_full.y\n",
        "\n",
        "    # Stratified split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=cfg.val_split,\n",
        "        random_state=SEED,\n",
        "        stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Total train samples: {n_samples}\")\n",
        "    print(f\"  → Train split: {len(train_indices)} ({(1-cfg.val_split)*100:.0f}%)\")\n",
        "    print(f\"  → Val split:   {len(val_indices)} ({cfg.val_split*100:.0f}%)\")\n",
        "\n",
        "    # Verify class distribution\n",
        "    train_labels = labels[train_indices]\n",
        "    val_labels = labels[val_indices]\n",
        "    print(f\"\\nClass distribution check:\")\n",
        "    for cls in range(6):\n",
        "        train_count = (train_labels == cls).sum()\n",
        "        val_count = (val_labels == cls).sum()\n",
        "        print(f\"  Class {cls} ({_CODE_TO_LABEL_NAME[cls]}): Train={train_count}, Val={val_count}\")\n",
        "\n",
        "    # Create datasets using indices\n",
        "    X_full, y_full = _load_split_raw(cfg.data_dir, \"train\")\n",
        "    mean = X_full.mean(axis=(0,2), keepdims=True)\n",
        "    std = X_full.std(axis=(0,2), keepdims=True) + 1e-6\n",
        "    X_full = ((X_full - mean) / std).astype(np.float32)\n",
        "\n",
        "    train_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=train_indices\n",
        "    )\n",
        "\n",
        "    val_set = UCIHARInertial(\n",
        "        cfg.data_dir, \"train\",\n",
        "        mean=mean, std=std,\n",
        "        preloaded_data=(X_full, y_full),\n",
        "        indices=val_indices\n",
        "    )\n",
        "\n",
        "    test_set_orig = UCIHARInertial(cfg.data_dir, \"test\", mean=mean, std=std)\n",
        "\n",
        "    val_loader = DataLoader(val_set, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "    test_loader_orig = DataLoader(test_set_orig, cfg.batch_size, num_workers=cfg.num_workers)\n",
        "\n",
        "    scenarios = get_transition_scenarios()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"    GAP vs PURE TPA COMPARISON\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Goal: Compare GAP and Pure TPA (no conv preprocessing)\")\n",
        "    print(f\"\\n   Training: Both configs use SAME augmentation\")\n",
        "    print(f\"   Testing: Differ in pooling method only\")\n",
        "    print(f\"   Scenarios: {len(scenarios)} extreme transitions (p=0.70-0.75, mix=0.55-0.58)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "    # Create transitional test sets\n",
        "    transition_sets, transition_infos = [], []\n",
        "    for i, scenario in enumerate(scenarios):\n",
        "        clsA, clsB, p, mix, profile, pos, segments = scenario\n",
        "        print(f\"  [{i+1}/{len(scenarios)}] {clsA} ↔ {clsB} (p={p:.2f}, mix={mix:.2f}, {profile}, {pos}, seg={segments})\")\n",
        "        print(f\"      Primary class: {(1-mix)*100:.0f}% | Transition: {mix*100:.0f}%\")\n",
        "        test_set_mod, info = create_transitional_test_set(\n",
        "            test_set_orig, clsA, clsB, p=p, mix=mix, profile=profile, pos=pos, segments=segments\n",
        "        )\n",
        "        transition_sets.append(test_set_mod)\n",
        "        transition_infos.append(info)\n",
        "        print(f\"      Modified: {info['modified_samples']}/{info['total_samples']} ({info['modified_ratio']*100:.1f}%)\")\n",
        "\n",
        "    transition_loaders = [DataLoader(ts, cfg.batch_size, num_workers=cfg.num_workers) for ts in transition_sets]\n",
        "    print(f\"\\n✓ {len(transition_loaders)} transitional test sets created.\\n\")\n",
        "\n",
        "    # 2-way ablation configurations\n",
        "    ablation_configs = [\n",
        "        {\"name\": \"GAP\", \"use_tpa\": False},\n",
        "        {\"name\": \"Pure_TPA\", \"use_tpa\": True},\n",
        "    ]\n",
        "\n",
        "    results_table = []\n",
        "\n",
        "    # Train and evaluate each configuration\n",
        "    for ab_cfg in ablation_configs:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"   CONFIG: {ab_cfg['name']}\")\n",
        "        print(f\"   Pooling: {ab_cfg['name']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "        cfg.use_tpa = ab_cfg[\"use_tpa\"]\n",
        "\n",
        "        g = torch.Generator(device='cpu').manual_seed(SEED)\n",
        "        train_loader = DataLoader(train_set, cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, generator=g)\n",
        "\n",
        "        model_path = os.path.join(cfg.save_dir, f\"model_{ab_cfg['name']}.pth\")\n",
        "\n",
        "        tpa_config = {\n",
        "            'num_prototypes': cfg.tpa_num_prototypes,\n",
        "            'heads': cfg.tpa_heads,\n",
        "            'dropout': cfg.tpa_dropout,\n",
        "            'temperature': cfg.tpa_temperature,\n",
        "            'topk_ratio': cfg.tpa_topk_ratio\n",
        "        }\n",
        "\n",
        "        model = HAR_Model(\n",
        "            d_model=cfg.d_model,\n",
        "            use_tpa=cfg.use_tpa,\n",
        "            tpa_config=tpa_config\n",
        "        ).to(cfg.device).float()\n",
        "\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "        best_acc, best_wts = 0.0, None\n",
        "        patience_counter = 0\n",
        "        best_epoch = 0\n",
        "\n",
        "        print(f\"\\nTraining {ab_cfg['name']} for up to {cfg.epochs} epochs (patience={cfg.patience})...\")\n",
        "        if cfg.use_tpa:\n",
        "            print(f\"Pure TPA: prototypes={cfg.tpa_num_prototypes}, heads={cfg.tpa_heads}, temp={cfg.tpa_temperature}\")\n",
        "            print(f\"  → NO convolutional preprocessing (lowpass, depthwise, pointwise removed)\")\n",
        "\n",
        "        for epoch in range(1, cfg.epochs + 1):\n",
        "            # Verbose output for first epoch and every 10th epoch\n",
        "            verbose = (epoch == 1 or epoch % 10 == 0)\n",
        "\n",
        "            stats = train_one_epoch(model, train_loader, opt, cfg, verbose_epoch=verbose)\n",
        "\n",
        "            # Evaluate on VALIDATION set for early stopping\n",
        "            val_acc, val_f1, _, _ = evaluate(model, val_loader, cfg)\n",
        "\n",
        "            # Early stopping logic based on VALIDATION accuracy\n",
        "            improved = False\n",
        "            if val_acc > best_acc + cfg.min_delta:\n",
        "                best_acc = val_acc\n",
        "                best_wts = copy.deepcopy(model.state_dict())\n",
        "                patience_counter = 0\n",
        "                best_epoch = epoch\n",
        "                improved = True\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # Only print for verbose epochs\n",
        "            if verbose:\n",
        "                log_str = f\"[{epoch:02d}/{cfg.epochs}] Train L:{stats['loss']:.4f} A:{stats['acc']:.4f}\"\n",
        "                log_str += f\" Aug:{stats['aug_count']}\"\n",
        "                log_str += f\" | Val A:{val_acc:.4f} F1:{val_f1:.4f}\"\n",
        "                if stats['avg_confidence'] is not None:\n",
        "                    log_str += f\" | Conf:{stats['avg_confidence']:.3f}\"\n",
        "                if improved:\n",
        "                    log_str += \" ✓\"\n",
        "                print(log_str)\n",
        "\n",
        "            # Early stopping check\n",
        "            if patience_counter >= cfg.patience:\n",
        "                print(f\"\\n⚠ Early stopping triggered at epoch {epoch}\")\n",
        "                print(f\"  No improvement for {cfg.patience} epochs on validation set\")\n",
        "                print(f\"  Best validation acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "                break\n",
        "\n",
        "        if best_wts:\n",
        "            torch.save(best_wts, model_path)\n",
        "            model.load_state_dict(best_wts)\n",
        "            print(f\"\\n✓ Best Val Acc: {best_acc:.4f} (epoch {best_epoch})\")\n",
        "\n",
        "        # Final evaluation on TEST set\n",
        "        acc_orig, f1_orig, _, _ = evaluate(model, test_loader_orig, cfg)\n",
        "        print(f\"  Final Test Acc: {acc_orig:.4f}, F1: {f1_orig:.4f}\")\n",
        "\n",
        "        print(f\"\\n   Evaluating on {len(transition_loaders)} transitional test sets...\")\n",
        "        transition_accs = []\n",
        "        scenario_details = []\n",
        "\n",
        "        for i, loader in enumerate(transition_loaders):\n",
        "            result = evaluate_simple(model, loader, cfg)\n",
        "            acc_mod = result['accuracy']\n",
        "            transition_accs.append(acc_mod)\n",
        "\n",
        "            scenario = scenarios[i]\n",
        "            clsA, clsB, p, mix = scenario[0], scenario[1], scenario[2], scenario[3]\n",
        "            primary_ratio = (1 - mix) * 100\n",
        "            drop_from_orig = acc_orig - acc_mod\n",
        "\n",
        "            if drop_from_orig < 0.02:\n",
        "                grade = \"Very Robust\"\n",
        "            elif drop_from_orig < 0.05:\n",
        "                grade = \"Slightly Vulnerable\"\n",
        "            else:\n",
        "                grade = \"Vulnerable\"\n",
        "\n",
        "            scenario_details.append({\n",
        "                'scenario': f\"{clsA}↔{clsB}\",\n",
        "                'primary_ratio': primary_ratio,\n",
        "                'acc': acc_mod,\n",
        "                'drop': drop_from_orig,\n",
        "                'grade': grade\n",
        "            })\n",
        "\n",
        "            print(f\"    Scenario {i+1}: Acc={acc_mod:.4f} Drop={drop_from_orig:.4f} [{grade}]\")\n",
        "\n",
        "        avg_trans_acc = np.mean(transition_accs)\n",
        "        avg_drop = acc_orig - avg_trans_acc\n",
        "        retention = (1 - avg_drop/acc_orig) * 100 if acc_orig > 0 else 0\n",
        "\n",
        "        result = {\n",
        "            \"config\": ab_cfg[\"name\"],\n",
        "            \"use_tpa\": ab_cfg[\"use_tpa\"],\n",
        "            \"orig_acc\": acc_orig,\n",
        "            \"avg_trans_acc\": avg_trans_acc,\n",
        "            \"avg_drop\": avg_drop,\n",
        "            \"retention\": retention,\n",
        "            \"scenario_details\": scenario_details\n",
        "        }\n",
        "        results_table.append(result)\n",
        "\n",
        "        print(f\"\\n {ab_cfg['name']} Summary:\")\n",
        "        print(f\"   Original Test:      {acc_orig:.4f}\")\n",
        "        print(f\"   Avg Transition:     {avg_trans_acc:.4f}\")\n",
        "        print(f\"   Avg Drop:           {avg_drop:.4f}\")\n",
        "        print(f\"   Retention:          {retention:.2f}%\")\n",
        "\n",
        "    # ========================\n",
        "    # Analysis and Comparison\n",
        "    # ========================\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   GAP vs PURE TPA RESULTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"{'Config':<15} {'Pooling':<15} {'Orig':<8} {'Trans':<8} {'Drop':<8} {'Retention':<10}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for r in results_table:\n",
        "        pooling = \"Pure TPA\" if r['use_tpa'] else \"GAP\"\n",
        "        print(f\"{r['config']:<15} {pooling:<15} {r['orig_acc']:<8.4f} {r['avg_trans_acc']:<8.4f} {r['avg_drop']:<8.4f} {r['retention']:<10.2f}%\")\n",
        "\n",
        "    # Compute effects\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"   EFFECT ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Find each config\n",
        "    gap = next(r for r in results_table if not r['use_tpa'])\n",
        "    tpa = next(r for r in results_table if r['use_tpa'])\n",
        "\n",
        "    # Pure TPA effect\n",
        "    tpa_effect = gap['avg_drop'] - tpa['avg_drop']\n",
        "    tpa_improve = (tpa_effect / gap['avg_drop'] * 100) if gap['avg_drop'] > 0 else 0\n",
        "\n",
        "    print(f\"PURE TPA EFFECT (vs GAP):\")\n",
        "    print(f\"   GAP  →  Pure TPA\")\n",
        "    print(f\"   Drop: {gap['avg_drop']:.4f} → {tpa['avg_drop']:.4f}\")\n",
        "    print(f\"   Absolute improvement: {tpa_effect:+.4f}\")\n",
        "    print(f\"   Relative improvement: {tpa_improve:+.2f}% drop reduction\")\n",
        "    print(f\"   Retention gain: {tpa['retention'] - gap['retention']:+.2f}pp\")\n",
        "\n",
        "    if tpa_effect > 0:\n",
        "        print(f\"\\n   ✓ Pure TPA helps by reducing performance drop on transitional data\")\n",
        "        print(f\"     This shows that prototype-based attention alone (without conv)\")\n",
        "        print(f\"     provides robustness benefits over simple global average pooling\")\n",
        "    elif tpa_effect < -0.01:\n",
        "        print(f\"\\n   ✗ Pure TPA performs worse than GAP\")\n",
        "        print(f\"     The conv preprocessing layers may be important for TPA performance\")\n",
        "    else:\n",
        "        print(f\"\\n   ≈ Pure TPA and GAP show similar performance\")\n",
        "        print(f\"     Prototype attention alone may not provide significant advantage\")\n",
        "\n",
        "    # Original accuracy comparison\n",
        "    orig_diff = tpa['orig_acc'] - gap['orig_acc']\n",
        "    print(f\"\\n\\nORIGINAL TEST ACCURACY:\")\n",
        "    print(f\"   GAP:      {gap['orig_acc']:.4f}\")\n",
        "    print(f\"   Pure TPA: {tpa['orig_acc']:.4f}\")\n",
        "    print(f\"   Difference: {orig_diff:+.4f}\")\n",
        "\n",
        "    if abs(orig_diff) < 0.01:\n",
        "        print(f\"   → Similar baseline performance\")\n",
        "    elif orig_diff > 0:\n",
        "        print(f\"   → Pure TPA has higher baseline accuracy\")\n",
        "    else:\n",
        "        print(f\"   → GAP has higher baseline accuracy\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(cfg.save_dir, \"pure_tpa_results.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            'results': results_table,\n",
        "            'analysis': {\n",
        "                'tpa_effect': float(tpa_effect),\n",
        "                'tpa_improve_pct': float(tpa_improve),\n",
        "                'orig_acc_diff': float(orig_diff)\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Results saved to '{cfg.save_dir}/pure_tpa_results.json'\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ========================\n",
        "# 8) Main Execution\n",
        "# ========================\n",
        "if __name__ == \"__main__\":\n",
        "    config = Config()\n",
        "    config.mode = \"ablation\"\n",
        "    config.epochs = 100\n",
        "    config.lr = 1e-4\n",
        "\n",
        "    config.train_augment_prob = 0.25\n",
        "    config.train_augment_mix = 0.35\n",
        "\n",
        "    # Early stopping\n",
        "    config.patience = 20\n",
        "    config.min_delta = 0.0001\n",
        "    config.val_split = 0.2\n",
        "\n",
        "    # Pure TPA hyperparameters (no seg_kernel needed)\n",
        "    config.tpa_num_prototypes = 16\n",
        "    config.tpa_heads = 4\n",
        "    config.tpa_dropout = 0.1\n",
        "    config.tpa_temperature = 0.07\n",
        "    config.tpa_topk_ratio = 0.25\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"    UCI-HAR GAP vs PURE TPA COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {config.device}\")\n",
        "    print(f\"Epochs: {config.epochs}\")\n",
        "    print(f\"Learning Rate: {config.lr}\")\n",
        "    print(f\"Train Augmentation: prob={config.train_augment_prob}, mix={config.train_augment_mix}\")\n",
        "    print(f\"\\n2 Configurations to Compare:\")\n",
        "    print(f\"  1) GAP:      Global Average Pooling (baseline)\")\n",
        "    print(f\"  2) Pure TPA: Temporal Prototype Attention (NO conv preprocessing)\")\n",
        "    print(f\"\\nThis allows us to measure:\")\n",
        "    print(f\"  • Effect of pure prototype attention vs GAP\")\n",
        "    print(f\"  • Whether TPA alone (without conv layers) provides robustness\")\n",
        "    print(f\"  • Robustness to transitional noise\")\n",
        "    print(f\"\\nPure TPA Configuration:\")\n",
        "    print(f\"  Prototypes: {config.tpa_num_prototypes}\")\n",
        "    print(f\"  Heads: {config.tpa_heads}\")\n",
        "    print(f\"  Temperature: {config.tpa_temperature}\")\n",
        "    print(f\"  TopK Ratio: {config.tpa_topk_ratio}\")\n",
        "    print(f\"  ⚠ Conv layers REMOVED: No lowpass, depthwise, or pointwise conv\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    if config.mode == \"ablation\":\n",
        "        run_ablation_study(config)\n",
        "    else:\n",
        "        print(\"✗ Invalid mode. Set config.mode = 'ablation'\")"
      ],
      "metadata": {
        "id": "J6TbYUCQi7Du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab07fc9e-5987-4a4c-b6ea-8de91da13930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "    UCI-HAR GAP vs PURE TPA COMPARISON\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Epochs: 100\n",
            "Learning Rate: 0.0001\n",
            "Train Augmentation: prob=0.25, mix=0.35\n",
            "\n",
            "2 Configurations to Compare:\n",
            "  1) GAP:      Global Average Pooling (baseline)\n",
            "  2) Pure TPA: Temporal Prototype Attention (NO conv preprocessing)\n",
            "\n",
            "This allows us to measure:\n",
            "  • Effect of pure prototype attention vs GAP\n",
            "  • Whether TPA alone (without conv layers) provides robustness\n",
            "  • Robustness to transitional noise\n",
            "\n",
            "Pure TPA Configuration:\n",
            "  Prototypes: 16\n",
            "  Heads: 4\n",
            "  Temperature: 0.07\n",
            "  TopK Ratio: 0.25\n",
            "  ⚠ Conv layers REMOVED: No lowpass, depthwise, or pointwise conv\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   DATA PREPARATION\n",
            "======================================================================\n",
            "Total train samples: 7352\n",
            "  → Train split: 5881 (80%)\n",
            "  → Val split:   1471 (20%)\n",
            "\n",
            "Class distribution check:\n",
            "  Class 0 (WALKING): Train=981, Val=245\n",
            "  Class 1 (WALKING_UPSTAIRS): Train=858, Val=215\n",
            "  Class 2 (WALKING_DOWNSTAIRS): Train=789, Val=197\n",
            "  Class 3 (SITTING): Train=1029, Val=257\n",
            "  Class 4 (STANDING): Train=1099, Val=275\n",
            "  Class 5 (LAYING): Train=1125, Val=282\n",
            "\n",
            "======================================================================\n",
            "    GAP vs PURE TPA COMPARISON\n",
            "======================================================================\n",
            "   Goal: Compare GAP and Pure TPA (no conv preprocessing)\n",
            "\n",
            "   Training: Both configs use SAME augmentation\n",
            "   Testing: Differ in pooling method only\n",
            "   Scenarios: 17 extreme transitions (p=0.70-0.75, mix=0.55-0.58)\n",
            "======================================================================\n",
            "\n",
            "  [1/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 743/2947 (25.2%)\n",
            "  [2/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 749/2947 (25.4%)\n",
            "  [3/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 708/2947 (24.0%)\n",
            "  [4/17] WALKING ↔ WALKING_UPSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 713/2947 (24.2%)\n",
            "  [5/17] SITTING ↔ LAYING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [6/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 747/2947 (25.3%)\n",
            "  [7/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 674/2947 (22.9%)\n",
            "  [8/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 670/2947 (22.7%)\n",
            "  [9/17] STANDING ↔ SITTING (p=0.70, mix=0.55, abrupt, tail, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 753/2947 (25.6%)\n",
            "  [10/17] STANDING ↔ SITTING (p=0.70, mix=0.55, fade, random, seg=1)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 752/2947 (25.5%)\n",
            "  [11/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 795/2947 (27.0%)\n",
            "  [12/17] WALKING ↔ WALKING_UPSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 758/2947 (25.7%)\n",
            "  [13/17] SITTING ↔ LAYING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 800/2947 (27.1%)\n",
            "  [14/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 718/2947 (24.4%)\n",
            "  [15/17] STANDING ↔ SITTING (p=0.75, mix=0.58, abrupt, tail, seg=1)\n",
            "      Primary class: 42% | Transition: 58%\n",
            "      Modified: 803/2947 (27.2%)\n",
            "  [16/17] WALKING ↔ WALKING_DOWNSTAIRS (p=0.70, mix=0.55, abrupt, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 672/2947 (22.8%)\n",
            "  [17/17] SITTING ↔ LAYING (p=0.70, mix=0.55, fade, middle, seg=2)\n",
            "      Primary class: 45% | Transition: 55%\n",
            "      Modified: 745/2947 (25.3%)\n",
            "\n",
            "✓ 17 transitional test sets created.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: GAP\n",
            "   Pooling: GAP\n",
            "======================================================================\n",
            "\n",
            "Training GAP for up to 100 epochs (patience=20)...\n",
            "[01/100] Train L:1.4672 A:0.5246 Aug:16 | Val A:0.8226 F1:0.8098 ✓\n",
            "[10/100] Train L:0.4497 A:0.9342 Aug:16 | Val A:0.9381 F1:0.9419\n",
            "[20/100] Train L:0.3894 A:0.9442 Aug:10 | Val A:0.9565 F1:0.9598\n",
            "[30/100] Train L:0.3731 A:0.9497 Aug:9 | Val A:0.9517 F1:0.9553\n",
            "[40/100] Train L:0.3755 A:0.9510 Aug:13 | Val A:0.9545 F1:0.9579\n",
            "\n",
            "⚠ Early stopping triggered at epoch 47\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9626 (epoch 27)\n",
            "\n",
            "✓ Best Val Acc: 0.9626 (epoch 27)\n",
            "  Final Test Acc: 0.9084, F1: 0.9097\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.8242 Drop=0.0842 [Vulnerable]\n",
            "    Scenario 2: Acc=0.8812 Drop=0.0271 [Slightly Vulnerable]\n",
            "    Scenario 3: Acc=0.7696 Drop=0.1388 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9016 Drop=0.0068 [Very Robust]\n",
            "    Scenario 5: Acc=0.7645 Drop=0.1439 [Vulnerable]\n",
            "    Scenario 6: Acc=0.9209 Drop=-0.0126 [Very Robust]\n",
            "    Scenario 7: Acc=0.7665 Drop=0.1418 [Vulnerable]\n",
            "    Scenario 8: Acc=0.8955 Drop=0.0129 [Very Robust]\n",
            "    Scenario 9: Acc=0.8300 Drop=0.0784 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8836 Drop=0.0248 [Slightly Vulnerable]\n",
            "    Scenario 11: Acc=0.8083 Drop=0.1001 [Vulnerable]\n",
            "    Scenario 12: Acc=0.7340 Drop=0.1744 [Vulnerable]\n",
            "    Scenario 13: Acc=0.7133 Drop=0.1951 [Vulnerable]\n",
            "    Scenario 14: Acc=0.7329 Drop=0.1754 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8073 Drop=0.1011 [Vulnerable]\n",
            "    Scenario 16: Acc=0.7720 Drop=0.1364 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9199 Drop=-0.0115 [Very Robust]\n",
            "\n",
            " GAP Summary:\n",
            "   Original Test:      0.9084\n",
            "   Avg Transition:     0.8191\n",
            "   Avg Drop:           0.0892\n",
            "   Retention:          90.18%\n",
            "\n",
            "======================================================================\n",
            "   CONFIG: Pure_TPA\n",
            "   Pooling: Pure_TPA\n",
            "======================================================================\n",
            "\n",
            "Training Pure_TPA for up to 100 epochs (patience=20)...\n",
            "Pure TPA: prototypes=16, heads=4, temp=0.07\n",
            "  → NO convolutional preprocessing (lowpass, depthwise, pointwise removed)\n",
            "[01/100] Train L:1.4997 A:0.6385 Aug:16 | Val A:0.7648 F1:0.7295 | Conf:0.507 ✓\n",
            "[10/100] Train L:0.4296 A:0.9381 Aug:16 | Val A:0.9483 F1:0.9515 | Conf:0.331\n",
            "[20/100] Train L:0.3634 A:0.9498 Aug:10 | Val A:0.9545 F1:0.9580 | Conf:0.341\n",
            "[30/100] Train L:0.3453 A:0.9534 Aug:9 | Val A:0.9613 F1:0.9643 | Conf:0.353\n",
            "[40/100] Train L:0.3378 A:0.9573 Aug:13 | Val A:0.9633 F1:0.9662 | Conf:0.387\n",
            "[50/100] Train L:0.3151 A:0.9667 Aug:5 | Val A:0.9708 F1:0.9730 | Conf:0.406\n",
            "[60/100] Train L:0.3066 A:0.9748 Aug:13 | Val A:0.9782 F1:0.9799 | Conf:0.451\n",
            "[70/100] Train L:0.2963 A:0.9808 Aug:17 | Val A:0.9857 F1:0.9868 | Conf:0.473\n",
            "[80/100] Train L:0.2875 A:0.9813 Aug:14 | Val A:0.9857 F1:0.9868 | Conf:0.459\n",
            "[90/100] Train L:0.2760 A:0.9867 Aug:7 | Val A:0.9878 F1:0.9887 | Conf:0.435\n",
            "\n",
            "⚠ Early stopping triggered at epoch 98\n",
            "  No improvement for 20 epochs on validation set\n",
            "  Best validation acc: 0.9912 (epoch 78)\n",
            "\n",
            "✓ Best Val Acc: 0.9912 (epoch 78)\n",
            "  Final Test Acc: 0.9542, F1: 0.9539\n",
            "\n",
            "   Evaluating on 17 transitional test sets...\n",
            "    Scenario 1: Acc=0.9057 Drop=0.0485 [Slightly Vulnerable]\n",
            "    Scenario 2: Acc=0.8945 Drop=0.0597 [Vulnerable]\n",
            "    Scenario 3: Acc=0.8327 Drop=0.1215 [Vulnerable]\n",
            "    Scenario 4: Acc=0.9348 Drop=0.0193 [Very Robust]\n",
            "    Scenario 5: Acc=0.9423 Drop=0.0119 [Very Robust]\n",
            "    Scenario 6: Acc=0.8839 Drop=0.0702 [Vulnerable]\n",
            "    Scenario 7: Acc=0.8164 Drop=0.1378 [Vulnerable]\n",
            "    Scenario 8: Acc=0.9410 Drop=0.0132 [Very Robust]\n",
            "    Scenario 9: Acc=0.9013 Drop=0.0529 [Vulnerable]\n",
            "    Scenario 10: Acc=0.8962 Drop=0.0580 [Vulnerable]\n",
            "    Scenario 11: Acc=0.8962 Drop=0.0580 [Vulnerable]\n",
            "    Scenario 12: Acc=0.8001 Drop=0.1541 [Vulnerable]\n",
            "    Scenario 13: Acc=0.9440 Drop=0.0102 [Very Robust]\n",
            "    Scenario 14: Acc=0.7876 Drop=0.1666 [Vulnerable]\n",
            "    Scenario 15: Acc=0.8921 Drop=0.0621 [Vulnerable]\n",
            "    Scenario 16: Acc=0.8093 Drop=0.1449 [Vulnerable]\n",
            "    Scenario 17: Acc=0.9125 Drop=0.0417 [Slightly Vulnerable]\n",
            "\n",
            " Pure_TPA Summary:\n",
            "   Original Test:      0.9542\n",
            "   Avg Transition:     0.8818\n",
            "   Avg Drop:           0.0724\n",
            "   Retention:          92.41%\n",
            "\n",
            "======================================================================\n",
            "   GAP vs PURE TPA RESULTS\n",
            "======================================================================\n",
            "\n",
            "Config          Pooling         Orig     Trans    Drop     Retention \n",
            "----------------------------------------------------------------------\n",
            "GAP             GAP             0.9084   0.8191   0.0892   90.18     %\n",
            "Pure_TPA        Pure TPA        0.9542   0.8818   0.0724   92.41     %\n",
            "\n",
            "======================================================================\n",
            "   EFFECT ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "PURE TPA EFFECT (vs GAP):\n",
            "   GAP  →  Pure TPA\n",
            "   Drop: 0.0892 → 0.0724\n",
            "   Absolute improvement: +0.0168\n",
            "   Relative improvement: +18.88% drop reduction\n",
            "   Retention gain: +2.24pp\n",
            "\n",
            "   ✓ Pure TPA helps by reducing performance drop on transitional data\n",
            "     This shows that prototype-based attention alone (without conv)\n",
            "     provides robustness benefits over simple global average pooling\n",
            "\n",
            "\n",
            "ORIGINAL TEST ACCURACY:\n",
            "   GAP:      0.9084\n",
            "   Pure TPA: 0.9542\n",
            "   Difference: +0.0458\n",
            "   → Pure TPA has higher baseline accuracy\n",
            "\n",
            "✓ Results saved to '/content/drive/MyDrive/AI_data/ablation_pure_tpa/pure_tpa_results.json'\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}