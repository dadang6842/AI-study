{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "`__all__`\n",
        "- 이 모듈(ModernTCN_Layer.py)을 `from ModernTCN_Layer import *` 형태로 import할 때,\n",
        "외부에 공식적으로 공개되는 심볼들을 지정\n",
        "- `__all__`이 있으면 지정된 클래스만 불러옴, 없으면 파일 안의 모든 클래스가 import될 수도 있"
      ],
      "metadata": {
        "id": "R_VW8IJ_9tp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb37-vJp9A_V"
      },
      "outputs": [],
      "source": [
        "#__all__ = ['Transpose', 'get_activation_fn', 'moving_avg', 'series_decomp', 'PositionalEncoding', 'SinCosPosEncoding', 'Coord2dPosEncoding', 'Coord1dPosEncoding', 'positional_encoding']\n",
        "__all__ = ['moving_avg', 'series_decomp',  'Flatten_Head']\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "# decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## moving_avg: 이동 평균 시계열을 만드는 코드\n",
        "- 이동 평균 시계열: 원래 시계열에서 일정 구간(윈도우)의 값들을 평균 내서 만든 새로운 시계열\n",
        "- noise를 줄이고 장기 추세(trend)를 뚜렷하게 보이게 함\n",
        "\n",
        "입력 x의 모양: (batch, seq_len, channels)\n",
        "\n",
        "양끝에 패딩을 주고 cat하는 이유\n",
        "- 시계열 양 끝단에서는 윈도우가 잘려서 평균을 제대로 못 냄\n",
        "- e.g., 시퀀스가 [x0, x1, x2]와 같고 윈도우 크기가 3일 때, x0 기준 [?, x0, x1] 값을 보고 평균을 구해야 하는데 x0 앞의 값이 없음\n",
        "- 따라서 양끝을 패딩으로 채워줌\n",
        "\n",
        "repeat(): 텐서를 복제해서 확장하는 메소드\n",
        "- x[:, 0:1, :]으로 시계열 맨 앞 timestep만 뽑으면 shape이 (batch, 1, features)가 됨\n",
        "- .repeat(1, (kernel_size-1)//2, 1)\n",
        "- 첫 번째 1 → 배치 차원은 반복하지 않음\n",
        "- 두 번째 (kernel_size-1)//2 → time 차원(timestep)을 이만큼 복제\n",
        "- 세 번째 1 → feature 차원도 반복하지 않음\n",
        "- 즉, 맨 앞 timestep을 여러 번 복제해서 새로운 앞쪽 시퀀스를 만듦 (뒷쪽도 마찬가지)"
      ],
      "metadata": {
        "id": "ZmsJgHuP-Vfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class moving_avg(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block to highlight the trend of time series\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(moving_avg, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # padding on the both ends of time series\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7hOB5ZDN9I4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## series-decomp: 시계열 분해\n",
        "- moving_avg 블록을 이용해 추세를 계산\n",
        "- 원본에서 추세를 뺀 나머지 -> res (잔차)\n",
        "- 잔차와 추세를 반환\n",
        "\n",
        "잔차-추세 분해를 왜 하는가?\n",
        "- 추세는 장기적인 변화 양상, 비교적 부드럽고 저주파 성분\n",
        "- 잔차는 추세에서 벗어난 변동, 노이즈, 비교적 불규칙하고 고주파\n",
        "- 모델이 한번에 둘을 학습하면 어렵기 때문에, 잔차-추세로 분해해서 문제를 단순화\n",
        "- 노이즈가 잔차 쪽으로 몰리므로 중요한 패턴(추세)을 더 뚜렷하게 모델에 제공할 수 있음\n",
        "- 추세는 장기 패턴으로, 잔차는 이상 탐지에 활용 가능"
      ],
      "metadata": {
        "id": "NrYccDrpGXYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean"
      ],
      "metadata": {
        "id": "dXz-UjdV9OSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten_Head: ModernTCN의 출력 feature map을 task-specific output으로 변환하는 Head\n",
        "\n",
        "인자\n",
        "- individual: True면 변수별로 독립적인 head, False면 공통 head\n",
        "- n_vars: 입력 변수 개수\n",
        "- nf: 입력 feature 수 (보통 d_model * patch_num)\n",
        "- target_window: 최종 출력 시퀀스 길이\n",
        "- head_dropout: head에서 dropout 비율\n",
        "\n",
        "입력 x의 모양: [batch, nvars, d_model, patch_num]\n",
        "\n",
        "- flattens: 각 변수마다 feature map을 1D로 펼치는 역할\n",
        "- linears: 펼친 벡터를 최종 출력(target_window)로 변환\n",
        "- individual=True면 각 변수별로 flatten, linear, dropout 레이어가 있음"
      ],
      "metadata": {
        "id": "-QyrJuHvHZDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forecast task head\n",
        "class Flatten_Head(nn.Module):\n",
        "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
        "        super(Flatten_Head, self).__init__()\n",
        "\n",
        "        self.individual = individual\n",
        "        self.n_vars = n_vars\n",
        "\n",
        "        if self.individual:\n",
        "            self.linears = nn.ModuleList()\n",
        "            self.dropouts = nn.ModuleList()\n",
        "            self.flattens = nn.ModuleList()\n",
        "            for i in range(self.n_vars):\n",
        "                self.flattens.append(nn.Flatten(start_dim=-2))\n",
        "                self.linears.append(nn.Linear(nf, target_window))\n",
        "                self.dropouts.append(nn.Dropout(head_dropout))\n",
        "        else:\n",
        "            self.flatten = nn.Flatten(start_dim=-2)\n",
        "            self.linear = nn.Linear(nf, target_window)\n",
        "            self.dropout = nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
        "        if self.individual:\n",
        "            x_out = []\n",
        "            for i in range(self.n_vars):\n",
        "                z = self.flattens[i](x[:, i, :, :])  # z: [bs x d_model * patch_num]\n",
        "                z = self.linears[i](z)  # z: [bs x target_window]\n",
        "                z = self.dropouts[i](z)\n",
        "                x_out.append(z)\n",
        "            x = torch.stack(x_out, dim=1)  # x: [bs x nvars x target_window]\n",
        "        else:\n",
        "            x = self.flatten(x)\n",
        "            x = self.linear(x)\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Qut4aOBW9Ux6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}