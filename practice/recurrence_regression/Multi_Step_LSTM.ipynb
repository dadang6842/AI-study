{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPr6AqOmXwNbxpKqbxmg7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadang6842/AI-study/blob/main/practice/recurrence_regression/Multi_Step_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Step LSTM Models\n",
        "- 미래의 여러 시점을 예측, 출력층(Dense) 유닛 수 = M\n",
        "- Single-Step은 한 시점만 예측, 출력층(Dense) 유닛 수 = 1\n",
        "\n",
        "```\n",
        "e.g.\n",
        "\n",
        "Input\n",
        "[10, 20, 30]\n",
        "\n",
        "Output\n",
        "[40, 50]\n",
        "```\n",
        "\n",
        "\n",
        "- 참고 자료: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n"
      ],
      "metadata": {
        "id": "-vxMDjPIuSy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "- split_sequence() 함수에서 n_steps_in, n_steps_out 인자를 받음"
      ],
      "metadata": {
        "id": "F0Ns8WT4vEyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92F6ZowGuRT3"
      },
      "outputs": [],
      "source": [
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif out_end_ix > len(sequence):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Output Model\n",
        "- 벡터를 출력"
      ],
      "metadata": {
        "id": "PLJkEpObv8kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# univariate multi-step vector-output stacked lstm example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "\n",
        "# define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps_in, n_steps_out = 3, 2\n",
        "\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
        "\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "# define model -> use a Stacked LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
        "model.add(LSTM(100, activation='relu'))\n",
        "model.add(Dense(n_steps_out)) # 출력 개수\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=50, verbose=0)\n",
        "\n",
        "# demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1eloSEhw3V9",
        "outputId": "688c6a56-ddc9-4b45-f132-37c18399ec2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[109.1352   123.342094]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder Model\n",
        "- = sequence-to-sequence 모델\n",
        "- 입력, 출력 시퀀스의 길이가 가변\n",
        "- 모델이 두 개 -> encoder와 decoder\n",
        "\n",
        "Encoder\n",
        "- 입력 시퀀스를 읽어들여 하나의 고정 길이 벡터로 요약\n",
        "- (batch_size, n_unit) 벡터를 출력\n",
        "\n",
        "RepeatVector\n",
        "- 출력 시퀀스의 길이만큼 인코더 출력을 반복\n",
        "- 출력이 여러 타임스텝으로 구성되어야 하므로 같은 벡터를 여러 번 복제해서 (batch_size, n_steps_out, n_unit) 형태를 만듦\n",
        "\n",
        "Decoder\n",
        "- (batch_size, n_steps_out, n_unit) 벡터를 입력받아 LSTM으로 time step마다 출력 시퀀스를 생성 -> (batch_size, time_steps, n_unit)\n",
        "- return_sequences=True → 전체 시퀀스를 출력하겠다는 뜻\n",
        "\n",
        "TimeDistributed(Dense)\n",
        "- 디코더가 출력하는 각 시점에 대해 Dense(1)를 적용하여 실제 예측값을 산출\n",
        "- 최종 출력: (batch_size, n_steps_out, 1)\n",
        "- n_steps_out개의 예측값이 1차원으로 나오는 구조"
      ],
      "metadata": {
        "id": "KykpTz2yxwQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# univariate multi-step encoder-decoder lstm example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "# define input sequence\n",
        "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "\n",
        "# choose a number of time steps\n",
        "n_steps_in, n_steps_out = 3, 2\n",
        "\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
        "\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "n_features = 1\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "y = y.reshape((y.shape[0], y.shape[1], n_features))\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(RepeatVector(n_steps_out))\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(1)))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# demonstrate prediction\n",
        "x_input = array([70, 80, 90])\n",
        "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
        "yhat = model.predict(x_input, verbose=0)\n",
        "print(yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7gUt5KNyypV",
        "outputId": "cfe1a755-85fd-43c9-e6d7-219da0f657fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[104.17813]\n",
            "  [117.17174]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Output Model vs. Encoder-Decoder Model\n",
        "- Encoder-Decoder는 추론 시 출력 길이 조절 가능 (auto-regressive(디코더의 출력을 다음 입력으로 반복) 사용 시)\n",
        "- Encoder-Decoder는 출력 시퀀스 간 시간적 의존성을 잘 반영, 디코더(LSTM)에서 시점 간 연관성이 있기 때문\n",
        "-  Vector Output은 한 번에 모든 예측값을 독립적으로 계산, 출력 시점 간 의존성 없음\n"
      ],
      "metadata": {
        "id": "EOMEQlwVJ7ab"
      }
    }
  ]
}